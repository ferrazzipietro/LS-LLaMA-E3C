{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from utils import DataPreprocessor, DatasetFormatConverter\n",
    "from src.billm.modeling_mistral import MistralForTokenClassification\n",
    "\n",
    "WANDB_KEY = dotenv_values(\".env.base\")['WANDB_KEY']\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "adapters = \"ferrazzipietro/LS_Mistral-7B-v0.1_adapters_en.layer1_NoQuant_16_32_0.01_2_0.0002\"\n",
    "peft_config = PeftConfig.from_pretrained(adapters)\n",
    "BASE_MODEL_CHECKPOINT = peft_config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1520/1520 [00:00<00:00, 3977.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT,\n",
    "                                          token =HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# seqeval = evaluate.load(\"seqeval\")\n",
    "DATASET_CHEKPOINT=\"ferrazzipietro/e3c-sentences\" \n",
    "TRAIN_LAYER=\"en.layer1\"\n",
    "preprocessor = DataPreprocessor(BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer)\n",
    "dataset = load_dataset(DATASET_CHEKPOINT) #download_mode=\"force_redownload\"\n",
    "dataset = dataset[TRAIN_LAYER]\n",
    "dataset = dataset.shuffle(seed=1234)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class DatasetFormatConverter():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.label2id = { \"O\": 0, \"B\": 1, \"I\": 2}\n",
    "\n",
    "    def get_id2label(self):\n",
    "        id2label = {v: k for k, v in self.label2id.items()}\n",
    "        return id2label\n",
    "    \n",
    "    def get_label2id(self):\n",
    "        return self.label2id\n",
    "    \n",
    "    def get_label_list(self):\n",
    "        return list(self.label2id.keys())\n",
    "    \n",
    "    def _reformat_entities_dict(self, enitities_dicts_list):\n",
    "        return [{item.get('text') : item.get('offsets')} for item in enitities_dicts_list]\n",
    "    \n",
    "    def _generate_char_based_labels_list(self, example):\n",
    "        labels = [\"O\"] * len(example[\"sentence\"])\n",
    "        for entity in example['entities']:\n",
    "            # print('entity: ', entity)\n",
    "            start = entity[\"offsets\"][0]\n",
    "            end = entity[\"offsets\"][1]\n",
    "            type = entity[\"type\"]\n",
    "            labels[start] = f\"B-{type}\"\n",
    "            for i in range(start+1, end):\n",
    "                # print('char: ', example[\"sentence\"][i])\n",
    "                labels[i] = f\"I-{type}\"\n",
    "        return labels\n",
    "    \n",
    "    def _contains_punctuation(self, word):\n",
    "        return any(char in string.punctuation for char in word)\n",
    "\n",
    "    def _is_only_punctuation(self, word):\n",
    "        return all(char in string.punctuation for char in word)\n",
    "    \n",
    "    def _remove_punctuation_and_count(self, text, punctuation_to_remove = '!\"#&\\'(),-./:;<=>?@[\\\\]^_`|'):\n",
    "        \"\"\"\n",
    "        Remove punctuation from the beginning and end of the text and count how many characters were removed.\n",
    "        \"\"\"\n",
    "        count_beginning = len(text) - len(text.lstrip(punctuation_to_remove))\n",
    "        count_end = len(text) - len(text.rstrip(punctuation_to_remove))\n",
    "        word_no_punct = text.strip(punctuation_to_remove)\n",
    "        return word_no_punct, count_beginning, count_end\n",
    "\n",
    "    def _entities_from_dict_to_labels_list(self, example, word_level=True, token_level=False, tokenizer=None):\n",
    "        if word_level and token_level:\n",
    "            raise ValueError(\"Only one of word_level and token_level can be True\")\n",
    "        if not word_level and not token_level:\n",
    "            raise ValueError(\"One of word_level and token_level must be True\")\n",
    "        if token_level and tokenizer is None:\n",
    "            raise ValueError(\"tokenizer must be provided if token_level is True\")\n",
    "        if word_level:\n",
    "            words = example[\"sentence\"].split()\n",
    "        elif token_level:\n",
    "            raise NotImplementedError\n",
    "        labels = [0] * len(words)\n",
    "        # print(example[\"entities\"])\n",
    "        chars_based_labels = self._generate_char_based_labels_list(example)\n",
    "        word_starting_position = 0\n",
    "        for i, word in enumerate(words):\n",
    "            # print(f'processing word: {word}\\n starting position: {word_starting_position}\\n encompassing labels {chars_based_labels[word_starting_position:word_starting_position+len(word)]}')\n",
    "            if self._is_only_punctuation(word):\n",
    "                word_starting_position = word_starting_position + len(word) + 1\n",
    "                continue\n",
    "            if self._contains_punctuation(word):\n",
    "                _, count_beginning, count_end = self._remove_punctuation_and_count(word)\n",
    "                # print(f'remove punctuation from word: {word}\\n count beginning: {count_beginning}\\n count end: {count_end}')\n",
    "            else:\n",
    "                count_beginning, count_end = 0, 0\n",
    "            word_length = len(word)\n",
    "            start_word = word_starting_position + count_beginning\n",
    "            end_word = word_starting_position + word_length - count_end\n",
    "            chars_labels_of_this_word = chars_based_labels[start_word : end_word]\n",
    "            if (chars_labels_of_this_word[0].startswith(\"B-\") or chars_labels_of_this_word[0].startswith(\"I-\")) \\\n",
    "                and all([label.startswith(\"I-\") for label in chars_labels_of_this_word[1:]]):\n",
    "                labels[i] = self.label2id.get(chars_labels_of_this_word[0][0], -1)\n",
    "            word_starting_position = word_starting_position + word_length + 1\n",
    "        # print(labels)\n",
    "        example['words'] = words\n",
    "        example['word_level_labels'] = labels\n",
    "        return example\n",
    "\n",
    "    def apply(self):\n",
    "        self.dataset = self.dataset.map(self._entities_from_dict_to_labels_list)\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def set_max_seq_length(self, max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def tokenize_and_align_labels(self, examples):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        tokenized_inputs = self.tokenizer(examples[\"tokens\"], is_split_into_words=True, padding='longest', max_length=self.max_seq_length, truncation=True)\n",
    "\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "            print('label: ', label)\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                print('word_idx: ', word_idx)\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_format_converter = DatasetFormatConverter(dataset)\n",
    "dataset_format_converter.apply()\n",
    "ds = dataset_format_converter.dataset\n",
    "ds = ds.rename_column(\"word_level_labels\", \"ner_tags\")\n",
    "ds = ds.rename_column(\"words\", \"tokens\")\n",
    "label2id = dataset_format_converter.label2id\n",
    "id2label = dataset_format_converter.get_id2label()\n",
    "label_list = dataset_format_converter.get_label_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1520/1520 [00:00<00:00, 6562.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(self, examples):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, padding='longest', max_length=100, truncation=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        print('label: ', label)\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            print('word_idx: ', word_idx)\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)# dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1520 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  0\n",
      "word_idx:  None\n",
      "word_idx:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_format_converter\u001b[38;5;241m.\u001b[39mset_tokenizer(tokenizer)\n\u001b[1;32m      2\u001b[0m dataset_format_converter\u001b[38;5;241m.\u001b[39mset_max_seq_length(\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m tokenized_ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_format_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_data, val_data, test_data \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39msplit_layer_into_train_val_test_(tokenized_ds, TRAIN_LAYER)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3106\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3456\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3458\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_format_converter\u001b[38;5;241m.\u001b[39mset_tokenizer(tokenizer)\n\u001b[1;32m      2\u001b[0m dataset_format_converter\u001b[38;5;241m.\u001b[39mset_max_seq_length(\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m tokenized_ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mdataset_format_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m# dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_data, val_data, test_data \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39msplit_layer_into_train_val_test_(tokenized_ds, TRAIN_LAYER)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 114\u001b[0m, in \u001b[0;36mDatasetFormatConverter.tokenize_and_align_labels\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    112\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m word_idx \u001b[38;5;241m!=\u001b[39m previous_word_idx:  \u001b[38;5;66;03m# Only label the first token of a given word.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlabel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     label_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "dataset_format_converter.set_tokenizer(tokenizer)\n",
    "dataset_format_converter.set_max_seq_length(256)\n",
    "tokenized_ds = ds.map(lambda x: dataset_format_converter.tokenize_and_align_labels(x))# dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)\n",
    "train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(tokenized_ds, TRAIN_LAYER)\n",
    "print(train_data['sentence'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MistralForTokenClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    num_labels=len(label2id), id2label=id2label, label2id=label2id,\n",
    "    token = HF_TOKEN,\n",
    "    # cache_dir='/data/disk1/share/pferrazzi/.cache'\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapters, token = HF_TOKEN)\n",
    "# merge_and_unload is necessary for inference\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# token_classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "# sentence = train_data['sentence'][0]\n",
    "# tokens = token_classifier(sentence)\n",
    "# print(tokens)\n",
    "\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", model=model, \n",
    "                            tokenizer=tokenizer, \n",
    "                            aggregation_strategy=\"simple\",\n",
    "                            batch_size=12)\n",
    "for out in tqdm(token_classifier(KeyDataset(train_data, \"sentence\"))):\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
