{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from utils import DataPreprocessor, DatasetFormatConverter\n",
    "from modeling_llama import LlamaForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esempio con LlamaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token </s>\n",
      "tokens {'input_ids': tensor([[    1,   306, 29915,   345,  1063, 10534,   363,   263],\n",
      "        [    1,  1105,   505,   306, 29991,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions tensor([[0.0854, 0.9146],\n",
      "        [0.9874, 0.0126]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.llama import LlamaForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=LLAMA_TOKEN,\n",
    "                                          cache_dir='/data/disk1/share/pferrazzi/.cache')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"tokenizer.eos_token\", tokenizer.eos_token)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "tokens = tokenizer(sequences, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=8)\n",
    "print(\"tokens\", tokens)\n",
    "model = LlamaForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=LLAMA_TOKEN,\n",
    "                                                       cache_dir='/data/disk1/share/pferrazzi/.cache')\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "outputs = model(**tokens)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(\"predictions\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_CHECKPOINT = 'meta-llama/Llama-2-7b-hf'\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT,\n",
    "                                          token =LLAMA_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CHEKPOINT=\"ferrazzipietro/e3c-sentences\" \n",
    "TRAIN_LAYER=\"en.layer1\"\n",
    "offset=False\n",
    "instruction_on_response_format='Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}].'# 'Return the result in a json format.'\n",
    "simplest_prompt=False\n",
    "dataset_text_field=\"prompt\"\n",
    "preprocessor = DataPreprocessor(BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer)\n",
    "dataset = load_dataset(DATASET_CHEKPOINT) #download_mode=\"force_redownload\"\n",
    "dataset = dataset[TRAIN_LAYER]\n",
    "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset, \n",
    "                                                 instruction_on_response_format=instruction_on_response_format,\n",
    "                                                 simplest_prompt=simplest_prompt)\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[dataset_text_field]), batched=True)\n",
    "dataset_format_converter = DatasetFormatConverter(dataset)\n",
    "dataset_format_converter.apply()\n",
    "ds = dataset_format_converter.dataset\n",
    "ds = ds.rename_column(\"word_level_labels\", \"ner_tags\")\n",
    "ds = ds.rename_column(\"words\", \"tokens\")\n",
    "label2id = dataset_format_converter.label2id\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "label_list = list(label2id.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n",
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,303,747 || all params: 6,613,659,654 || trainable%: 0.09531405197404492\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,# model_loading_params.load_in_4bit,\n",
    "    load_in_8bit = False,#  model_loading_params.load_in_8bit,\n",
    "\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "\n",
    "    # llm_int8_threshold= 6.0,# model_loading_params.llm_int8_threshold,\n",
    "    # llm_int8_skip_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],# model_loading_params.llm_int8_skip_modules,\n",
    "    # llm_int8_has_fp16_weight= True# model_loading_params.llm_int8_has_fp16_weight\n",
    ")\n",
    "\n",
    "\n",
    "model = LlamaForTokenClassification.from_pretrained(\n",
    "    BASE_MODEL_CHECKPOINT, \n",
    "    num_labels=len(label2id), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    token = LLAMA_TOKEN,\n",
    "    load_in_4bit=True,\n",
    "    quantization_config = bnb_config,\n",
    "    # device_map = 'auto',\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache')\n",
    "peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, inference_mode=False, r=12, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, max_length=28, word_column_name='words', labels_column_name='word_level_labels'):# , word_column_name='tokens', labels_column_name='ner_tags'):#\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples[word_column_name], is_split_into_words=True, padding='longest', max_length=max_length, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[labels_column_name]):\n",
    "        # print('label: ', label)\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-99)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_ds = dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)# dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)\n",
    "train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(tokenized_ds, TRAIN_LAYER)\n",
    "# tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(tokenized_ds, TRAIN_LAYER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate metrics for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mferrazzipietro\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pferrazzi/LS-LLaMA-E3C/wandb/run-20240429_100555-epntbyhj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ferrazzipietro/huggingface/runs/epntbyhj' target=\"_blank\">dutiful-wood-9</a></strong> to <a href='https://wandb.ai/ferrazzipietro/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ferrazzipietro/huggingface' target=\"_blank\">https://wandb.ai/ferrazzipietro/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ferrazzipietro/huggingface/runs/epntbyhj' target=\"_blank\">https://wandb.ai/ferrazzipietro/huggingface/runs/epntbyhj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/peft_model.py\", line 1590, in forward\n    return self.base_model(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py\", line 161, in forward\n    return self.model.forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/hooks.py\", line 167, in new_forward\n    return module._hf_hook.post_forward(module, output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/hooks.py\", line 380, in post_forward\n    output = send_to_device(output, self.input_device, skip_keys=self.skip_keys)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 189, in send_to_device\n    {\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 190, in <dictcomp>\n    k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 174, in send_to_device\n    raise error\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 158, in send_to_device\n    return tensor.to(device, non_blocking=non_blocking)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_awesome_ds_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# report_to=\"wandb\",\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2706\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2707\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/peft_model.py\", line 1590, in forward\n    return self.base_model(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py\", line 161, in forward\n    return self.model.forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/hooks.py\", line 167, in new_forward\n    return module._hf_hook.post_forward(module, output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/hooks.py\", line 380, in post_forward\n    output = send_to_device(output, self.input_device, skip_keys=self.skip_keys)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 189, in send_to_device\n    {\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 190, in <dictcomp>\n    k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 174, in send_to_device\n    raise error\n  File \"/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 158, in send_to_device\n    return tensor.to(device, non_blocking=non_blocking)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_ds_model\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    "    # hub_token=HF_TOKEN,\n",
    "    # hub_model_id='ls_llama_e3c',\n",
    "    # report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,303,747 || all params: 6,613,659,654 || trainable%: 0.09531405197404492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mferrazzipietro\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/pferrazzi/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pferrazzi/LS-LLaMA-E3C/wandb/run-20240429_101742-kbki9nrq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ferrazzipietro/ls_llama_e3c/runs/kbki9nrq' target=\"_blank\">2024-04-29 10:17:42</a></strong> to <a href='https://wandb.ai/ferrazzipietro/ls_llama_e3c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ferrazzipietro/ls_llama_e3c' target=\"_blank\">https://wandb.ai/ferrazzipietro/ls_llama_e3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ferrazzipietro/ls_llama_e3c/runs/kbki9nrq' target=\"_blank\">https://wandb.ai/ferrazzipietro/ls_llama_e3c/runs/kbki9nrq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:39, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f57b8-7ed6a37a21e6eadd19a2d2e3;b8a9c781-47e5-459d-9920-b29eefc6cbb4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f57c8-46baa4c32dd4511b03162d1b;2a06f6a8-a29c-4b89-a052-d7d6eca134fa)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f57d8-032debdb56b83219592d03e2;56698a48-28de-431b-89e4-b990a4d5d7c9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f57e8-32f5c7d9550a129a71d41566;71f52b5e-9e87-4484-abcd-b023631b7153)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f57f8-243ed2557045e5111da82eb9;3ac08946-0946-4174-ba98-fbe197feb4d2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5808-3dd6139e1efd7c49166cd56f;ba0ec53e-37b5-431e-b163-dfecc0eab272)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5815-7fdd24dc7191afa365ae28ec;4508ed0d-4b8c-481e-89cc-30c9098ed3c1)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=3.366946792602539, metrics={'train_runtime': 109.9806, 'train_samples_per_second': 0.727, 'train_steps_per_second': 0.091, 'total_flos': 131600366220960.0, 'train_loss': 3.366946792602539, 'epoch': 6.67})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from dotenv import dotenv_values\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from utils import DataPreprocessor, DatasetFormatConverter\n",
    "from modeling_llama import LlamaForTokenClassification\n",
    "\n",
    "\n",
    "\n",
    "WANDB_KEY = dotenv_values(\".env.base\")['WANDB_KEY']\n",
    "BASE_MODEL_CHECKPOINT = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "use_e3c = True\n",
    "\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT,\n",
    "                                          token =LLAMA_TOKEN,\n",
    "                                          cache_dir='/data/disk1/share/pferrazzi/.cache')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# if not use_e3c:\n",
    "#     ds = load_dataset(\"wnut_17\")\n",
    "#     label2id_ds = { \"O\": 0, \"B-corporation\": 1, \"I-corporation\": 2, \"B-creative-work\": 3, \"I-creative-work\": 4, \"B-group\": 5, \"I-group\": 6, \"B-location\": 7, \"I-location\": 8, \"B-person\": 9, \"I-person\": 10, \"B-product\": 11, \"I-product\": 12, }\n",
    "#     id2label_ds = {v: k for k, v in label2id_ds.items()}\n",
    "#     label_list_ds = list(label2id_ds.keys()) # ds[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "#     id2label = id2label_ds\n",
    "#     label2id = label2id_ds\n",
    "#     label_list = label_list_ds\n",
    "#     ds = ds.rename_column(\"ner_tags\", \"word_level_labels\")\n",
    "#     ds = ds.rename_column(\"tokens\", \"words\")\n",
    "if use_e3c:\n",
    "    DATASET_CHEKPOINT=\"ferrazzipietro/e3c-sentences\" \n",
    "    TRAIN_LAYER=\"en.layer1\"\n",
    "    offset=False\n",
    "    instruction_on_response_format='Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}].'# 'Return the result in a json format.'\n",
    "    simplest_prompt=False\n",
    "    dataset_text_field=\"prompt\"\n",
    "    preprocessor = DataPreprocessor(BASE_MODEL_CHECKPOINT, \n",
    "                                    tokenizer)\n",
    "    dataset = load_dataset(DATASET_CHEKPOINT) #download_mode=\"force_redownload\"\n",
    "    dataset = dataset[TRAIN_LAYER]\n",
    "    dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "    dataset = preprocessor.preprocess_data_one_layer(dataset, \n",
    "                                                    instruction_on_response_format=instruction_on_response_format,\n",
    "                                                    simplest_prompt=simplest_prompt)\n",
    "    dataset = dataset.map(lambda samples: tokenizer(samples[dataset_text_field]), batched=True)\n",
    "    dataset_format_converter = DatasetFormatConverter(dataset)\n",
    "    dataset_format_converter.apply()\n",
    "\n",
    "    ds = dataset_format_converter.dataset\n",
    "    ds = ds.rename_column(\"word_level_labels\", \"ner_tags\")\n",
    "    ds = ds.rename_column(\"words\", \"tokens\")\n",
    "    label2id = dataset_format_converter.label2id\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    label_list = list(label2id.keys())\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, padding='longest', max_length=256, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,# model_loading_params.load_in_4bit,\n",
    "    load_in_8bit = False,#  model_loading_params.load_in_8bit,\n",
    "\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "\n",
    "    # llm_int8_threshold= 6.0,# model_loading_params.llm_int8_threshold,\n",
    "    # llm_int8_skip_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],# model_loading_params.llm_int8_skip_modules,\n",
    "    # llm_int8_has_fp16_weight= True# model_loading_params.llm_int8_has_fp16_weight\n",
    ")\n",
    "\n",
    "model = LlamaForTokenClassification.from_pretrained(\n",
    "    BASE_MODEL_CHECKPOINT, \n",
    "    num_labels=len(label2id), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    token = LLAMA_TOKEN,\n",
    "    quantization_config=bnb_config,    \n",
    "    device_map = 'cuda:0',\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache'\n",
    "    )\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, inference_mode=False, r=12, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)# dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "if use_e3c:\n",
    "    train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(tokenized_ds, TRAIN_LAYER)\n",
    "\n",
    "\n",
    "\n",
    "wandb.login(key = WANDB_KEY)\n",
    "run = wandb.init(project='ls_llama_e3c', job_type=\"training\", anonymous=\"allow\",\n",
    "                  name=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                  config={'model': BASE_MODEL_CHECKPOINT, \n",
    "                          'time': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"soverall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_ds_model\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    hub_token=HF_TOKEN,\n",
    "    hub_model_id='ls_llama_e3c',\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data.select(range(6)),\n",
    "    eval_dataset=val_data.select(range(6)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('model.model.embed_tokens.weight', tensor([[ 1.1921e-06, -1.7881e-06, -4.2915e-06,  ...,  8.3447e-07,\n",
      "         -6.4373e-06,  8.9407e-07],\n",
      "        [ 1.8387e-03, -3.8147e-03,  9.6130e-04,  ..., -9.0332e-03,\n",
      "          2.6550e-03, -3.7537e-03],\n",
      "        [ 1.0193e-02,  9.7656e-03, -5.2795e-03,  ...,  2.9297e-03,\n",
      "          4.0817e-04, -5.0964e-03],\n",
      "        ...,\n",
      "        [-1.3550e-02, -3.5095e-03, -1.8921e-02,  ..., -9.3384e-03,\n",
      "          8.7891e-03, -1.2741e-03],\n",
      "        [-1.0681e-02,  8.9722e-03,  1.2573e-02,  ..., -3.3691e-02,\n",
      "         -1.6235e-02,  3.0212e-03],\n",
      "        [-9.0942e-03, -1.8082e-03, -6.9809e-04,  ...,  3.8452e-03,\n",
      "         -1.2085e-02,  7.2861e-04]], device='cuda:0', dtype=torch.float16)), ('model.model.layers.0.self_attn.q_proj.base_layer.weight', tensor([[ 83],\n",
      "        [103],\n",
      "        [ 74],\n",
      "        ...,\n",
      "        [114],\n",
      "        [108],\n",
      "        [197]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax', tensor([ 78,  69,  58,  ..., 199,  84,  82], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.1974, 0.1945, 0.1886,  ..., 0.1418, 0.1906, 0.1818], device='cuda:0')), ('model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  48,  54,  49,  50,  48,  53,  52,  55,  54,  53,  50,  50,  52,\n",
      "         52,  54, 125], dtype=torch.uint8)), ('model.model.layers.0.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.0.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.0.self_attn.k_proj.weight', tensor([[ 73],\n",
      "        [120],\n",
      "        [136],\n",
      "        ...,\n",
      "        [101],\n",
      "        [104],\n",
      "        [104]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.self_attn.k_proj.weight.absmax', tensor([ 96, 160,  74,  ..., 199, 181, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.0.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.0.self_attn.k_proj.weight.nested_absmax', tensor([0.3501, 0.2681, 0.2993,  ..., 0.1489, 0.1919, 0.2466], device='cuda:0')), ('model.model.layers.0.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.0.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  52,  50,  49,  50,  50,  51,  52,  57,  49,  52,  51,  48,  50,\n",
      "         56,  50,  54, 125], dtype=torch.uint8)), ('model.model.layers.0.self_attn.v_proj.base_layer.weight', tensor([[118],\n",
      "        [138],\n",
      "        [153],\n",
      "        ...,\n",
      "        [205],\n",
      "        [ 23],\n",
      "        [120]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax', tensor([54, 51, 17,  ..., 31, 13, 30], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0140, 0.0152, 0.0144,  ..., 0.0206, 0.0141, 0.0136], device='cuda:0')), ('model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         50,  55,  50,  51,  52,  54,  55,  49,  54,  51,  55,  52,  49,  53,\n",
      "         56,  56,  54, 125], dtype=torch.uint8)), ('model.model.layers.0.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.0.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.0.self_attn.o_proj.weight', tensor([[117],\n",
      "        [179],\n",
      "        [ 71],\n",
      "        ...,\n",
      "        [126],\n",
      "        [202],\n",
      "        [ 33]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.self_attn.o_proj.weight.absmax', tensor([ 46, 191,  44,  ..., 193,  76,  61], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.0.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.0.self_attn.o_proj.weight.nested_absmax', tensor([0.0244, 0.0310, 0.0466,  ..., 0.0317, 0.0368, 0.0593], device='cuda:0')), ('model.model.layers.0.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.0.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         50,  48,  56,  48,  53,  50,  51,  53,  57,  53,  50,  49,  51,  56,\n",
      "         57, 125], dtype=torch.uint8)), ('model.model.layers.0.mlp.gate_proj.weight', tensor([[204],\n",
      "        [228],\n",
      "        [226],\n",
      "        ...,\n",
      "        [ 26],\n",
      "        [ 23],\n",
      "        [146]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.mlp.gate_proj.weight.absmax', tensor([ 61,  69,  77,  ...,  95, 196,  93], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.0.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.0.mlp.gate_proj.weight.nested_absmax', tensor([0.0223, 0.0257, 0.0438,  ..., 0.0555, 0.1102, 0.0721], device='cuda:0')), ('model.model.layers.0.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  50,  49,  53,  56,  54,  50,  50,  50,  57,  52,  54,  54,\n",
      "         52,  51,  56, 125], dtype=torch.uint8)), ('model.model.layers.0.mlp.up_proj.weight', tensor([[113],\n",
      "        [196],\n",
      "        [ 20],\n",
      "        ...,\n",
      "        [ 57],\n",
      "        [154],\n",
      "        [ 64]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.mlp.up_proj.weight.absmax', tensor([ 72,  90,  57,  ..., 191,  55,  82], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.0.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.0.mlp.up_proj.weight.nested_absmax', tensor([0.0247, 0.0350, 0.0428,  ..., 0.0277, 0.0418, 0.0291], device='cuda:0')), ('model.model.layers.0.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  49,  54,  55,  48,  51,  56,  57,  52,  55,  51,  52,  51,\n",
      "         56,  50,  54, 125], dtype=torch.uint8)), ('model.model.layers.0.mlp.down_proj.weight', tensor([[132],\n",
      "        [165],\n",
      "        [204],\n",
      "        ...,\n",
      "        [ 73],\n",
      "        [103],\n",
      "        [ 98]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.0.mlp.down_proj.weight.absmax', tensor([163,  57,  95,  ..., 160,  62, 217], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.0.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.0.mlp.down_proj.weight.nested_absmax', tensor([0.0432, 0.0227, 0.0227,  ..., 0.0349, 0.0193, 0.0198], device='cuda:0')), ('model.model.layers.0.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  51,  54,  56,  50,  56,  53,  48,  56,  57,  55,  51,  49,\n",
      "         50,  49,  54,  52, 125], dtype=torch.uint8)), ('model.model.layers.0.input_layernorm.weight', tensor([0.0298, 0.0140, 0.0031,  ..., 0.0099, 0.0114, 0.0073], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.0.post_attention_layernorm.weight', tensor([0.0508, 0.0525, 0.0498,  ..., 0.0522, 0.0542, 0.0479], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.1.self_attn.q_proj.base_layer.weight', tensor([[ 88],\n",
      "        [ 41],\n",
      "        [155],\n",
      "        ...,\n",
      "        [ 71],\n",
      "        [147],\n",
      "        [114]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.1.self_attn.q_proj.base_layer.weight.absmax', tensor([184, 183, 181,  ...,  33,  25,  35], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.1.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.1880, 0.1323, 0.1040,  ..., 0.1206, 0.1137, 0.0815], device='cuda:0')), ('model.model.layers.1.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  51,  57,  57,  53,  48,  52,  56,  52,  48,  51,  55,  51,  57,\n",
      "         57,  51, 125], dtype=torch.uint8)), ('model.model.layers.1.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.1.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.1.self_attn.k_proj.weight', tensor([[ 38],\n",
      "        [228],\n",
      "        [ 70],\n",
      "        ...,\n",
      "        [212],\n",
      "        [ 25],\n",
      "        [ 91]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.1.self_attn.k_proj.weight.absmax', tensor([ 71, 194, 193,  ...,  44,  37,  35], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.1.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.1.self_attn.k_proj.weight.nested_absmax', tensor([0.1423, 0.1335, 0.1335,  ..., 0.1150, 0.0993, 0.0915], device='cuda:0')), ('model.model.layers.1.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.1.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  52,  55,  49,  57,  56,  49,  49,  48,  56,  49,  56,  56,  54,\n",
      "         50,  57, 125], dtype=torch.uint8)), ('model.model.layers.1.self_attn.v_proj.base_layer.weight', tensor([[150],\n",
      "        [104],\n",
      "        [154],\n",
      "        ...,\n",
      "        [245],\n",
      "        [ 89],\n",
      "        [ 45]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.1.self_attn.v_proj.base_layer.weight.absmax', tensor([239, 205, 202,  ...,  34,  37,  18], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.1.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0424, 0.0292, 0.0285,  ..., 0.0160, 0.0348, 0.0159], device='cuda:0')), ('model.model.layers.1.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         50,  53,  53,  48,  49,  52,  48,  53,  56,  50,  48,  50,  53,  48,\n",
      "         53,  49, 125], dtype=torch.uint8)), ('model.model.layers.1.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.1.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.1.self_attn.o_proj.weight', tensor([[145],\n",
      "        [213],\n",
      "        [182],\n",
      "        ...,\n",
      "        [ 71],\n",
      "        [202],\n",
      "        [ 24]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.1.self_attn.o_proj.weight.absmax', tensor([189, 209,  64,  ...,  58,  55,  34], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.1.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.1.self_attn.o_proj.weight.nested_absmax', tensor([0.0199, 0.0206, 0.0323,  ..., 0.0203, 0.0216, 0.0267], device='cuda:0')), ('model.model.layers.1.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.1.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         50,  50,  49,  51,  56,  57,  52,  53,  55,  53,  56,  51,  52,  50,\n",
      "         55,  52,  51, 125], dtype=torch.uint8)), ('model.model.layers.1.mlp.gate_proj.weight', tensor([[229],\n",
      "        [246],\n",
      "        [163],\n",
      "        ...,\n",
      "        [168],\n",
      "        [ 94],\n",
      "        [161]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.1.mlp.gate_proj.weight.absmax', tensor([ 50,  96,  56,  ..., 220, 195,  56], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.1.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.1.mlp.gate_proj.weight.nested_absmax', tensor([0.0181, 0.0284, 0.0411,  ..., 0.0245, 0.0211, 0.0154], device='cuda:0')), ('model.model.layers.1.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  54,  56,  49,  53,  50,  54,  56,  54,  57,  53,  51,  53,\n",
      "         52,  52,  54, 125], dtype=torch.uint8)), ('model.model.layers.1.mlp.up_proj.weight', tensor([[143],\n",
      "        [ 61],\n",
      "        [149],\n",
      "        ...,\n",
      "        [122],\n",
      "        [ 18],\n",
      "        [ 92]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.1.mlp.up_proj.weight.absmax', tensor([ 51, 190, 215,  ...,  50, 159, 179], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.1.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.1.mlp.up_proj.weight.nested_absmax', tensor([0.0230, 0.0235, 0.0221,  ..., 0.0245, 0.0304, 0.0269], device='cuda:0')), ('model.model.layers.1.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  52,  51,  53,  51,  52,  55,  48,  50,  48,  54,  50,  54,\n",
      "         48,  54,  56, 125], dtype=torch.uint8)), ('model.model.layers.1.mlp.down_proj.weight', tensor([[153],\n",
      "        [249],\n",
      "        [186],\n",
      "        ...,\n",
      "        [ 54],\n",
      "        [131],\n",
      "        [ 62]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.1.mlp.down_proj.weight.absmax', tensor([206,  55,  72,  ...,  90,  82,  49], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.1.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.1.mlp.down_proj.weight.nested_absmax', tensor([0.0272, 0.0311, 0.0340,  ..., 0.0272, 0.0257, 0.0340], device='cuda:0')), ('model.model.layers.1.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  48,  53,  52,  53,  53,  56,  54,  54,  52,  53,  54,\n",
      "         48,  51,  50, 125], dtype=torch.uint8)), ('model.model.layers.1.input_layernorm.weight', tensor([0.1123, 0.1094, 0.0986,  ..., 0.0640, 0.0918, 0.0723], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.1.post_attention_layernorm.weight', tensor([0.0996, 0.0996, 0.0962,  ..., 0.1050, 0.0991, 0.1021], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.2.self_attn.q_proj.base_layer.weight', tensor([[ 20],\n",
      "        [173],\n",
      "        [210],\n",
      "        ...,\n",
      "        [199],\n",
      "        [ 62],\n",
      "        [ 39]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.2.self_attn.q_proj.base_layer.weight.absmax', tensor([ 25,  59,  42,  ..., 195, 205, 174], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.2.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0564, 0.0749, 0.0847,  ..., 0.2039, 0.2097, 0.1882], device='cuda:0')), ('model.model.layers.2.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  53,  54,  55,  55,  51,  56,  57,  53,  48,  50,  53,  50,  53,\n",
      "         51,  51, 125], dtype=torch.uint8)), ('model.model.layers.2.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.2.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.2.self_attn.k_proj.weight', tensor([[108],\n",
      "        [ 77],\n",
      "        [ 90],\n",
      "        ...,\n",
      "        [125],\n",
      "        [192],\n",
      "        [ 79]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.2.self_attn.k_proj.weight.absmax', tensor([ 46,  44,  43,  ..., 191, 199, 198], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.2.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.2.self_attn.k_proj.weight.nested_absmax', tensor([0.1227, 0.0671, 0.1227,  ..., 0.1257, 0.1003, 0.1042], device='cuda:0')), ('model.model.layers.2.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.2.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  55,  54,  57,  52,  54,  53,  54,  53,  53,  48,  56,  56,  52,\n",
      "         50,  53, 125], dtype=torch.uint8)), ('model.model.layers.2.self_attn.v_proj.base_layer.weight', tensor([[ 87],\n",
      "        [215],\n",
      "        [148],\n",
      "        ...,\n",
      "        [153],\n",
      "        [165],\n",
      "        [196]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.2.self_attn.v_proj.base_layer.weight.absmax', tensor([ 63,  62,  80,  ...,  91, 204,  67], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.2.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0259, 0.0218, 0.0259,  ..., 0.0269, 0.0303, 0.0289], device='cuda:0')), ('model.model.layers.2.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  55,  48,  52,  55,  57,  50,  50,  54,  49,  49,  50,  51,  54,\n",
      "         53,  55, 125], dtype=torch.uint8)), ('model.model.layers.2.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.2.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.2.self_attn.o_proj.weight', tensor([[123],\n",
      "        [155],\n",
      "        [ 38],\n",
      "        ...,\n",
      "        [120],\n",
      "        [116],\n",
      "        [179]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.2.self_attn.o_proj.weight.absmax', tensor([223, 204, 191,  ..., 221, 213,  66], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.2.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.2.self_attn.o_proj.weight.nested_absmax', tensor([0.0179, 0.0191, 0.0208,  ..., 0.0296, 0.0257, 0.0230], device='cuda:0')), ('model.model.layers.2.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.2.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  53,  56,  48,  54,  57,  51,  53,  50,  56,  48,  53,  54,  49,\n",
      "         52,  53, 125], dtype=torch.uint8)), ('model.model.layers.2.mlp.gate_proj.weight', tensor([[139],\n",
      "        [ 88],\n",
      "        [105],\n",
      "        ...,\n",
      "        [123],\n",
      "        [244],\n",
      "        [ 25]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.2.mlp.gate_proj.weight.absmax', tensor([255,  80, 165,  ...,  44, 186,  48], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.2.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.2.mlp.gate_proj.weight.nested_absmax', tensor([0.0264, 0.0269, 0.0269,  ..., 0.0215, 0.0259, 0.0284], device='cuda:0')), ('model.model.layers.2.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  51,  48,  57,  56,  51,  54,  53,  51,  54,  54,  52,\n",
      "         53,  56,  57, 125], dtype=torch.uint8)), ('model.model.layers.2.mlp.up_proj.weight', tensor([[119],\n",
      "        [170],\n",
      "        [123],\n",
      "        ...,\n",
      "        [168],\n",
      "        [ 69],\n",
      "        [118]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.2.mlp.up_proj.weight.absmax', tensor([255,  65,  48,  ...,  55,  59,  55], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.2.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.2.mlp.up_proj.weight.nested_absmax', tensor([0.0227, 0.0354, 0.0237,  ..., 0.0276, 0.0276, 0.0315], device='cuda:0')), ('model.model.layers.2.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  49,  52,  53,  49,  51,  49,  54,  52,  55,  53,  56,\n",
      "         54,  56,  50, 125], dtype=torch.uint8)), ('model.model.layers.2.mlp.down_proj.weight', tensor([[154],\n",
      "        [212],\n",
      "        [108],\n",
      "        ...,\n",
      "        [208],\n",
      "        [109],\n",
      "        [  5]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.2.mlp.down_proj.weight.absmax', tensor([213, 227,  26,  ..., 197,  72,  84], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.2.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.2.mlp.down_proj.weight.nested_absmax', tensor([0.0235, 0.0332, 0.0293,  ..., 0.0249, 0.0210, 0.0449], device='cuda:0')), ('model.model.layers.2.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  51,  56,  56,  53,  56,  51,  48,  57,  51,  56,  56,\n",
      "         49,  54,  49, 125], dtype=torch.uint8)), ('model.model.layers.2.input_layernorm.weight', tensor([0.1719, 0.1758, 0.1729,  ..., 0.1768, 0.1699, 0.1729], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.2.post_attention_layernorm.weight', tensor([0.1338, 0.1377, 0.1348,  ..., 0.1357, 0.1377, 0.1367], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.3.self_attn.q_proj.base_layer.weight', tensor([[172],\n",
      "        [115],\n",
      "        [245],\n",
      "        ...,\n",
      "        [ 89],\n",
      "        [181],\n",
      "        [102]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.self_attn.q_proj.base_layer.weight.absmax', tensor([  0,  30,  33,  ..., 209, 213, 216], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.3.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0318, 0.0328, 0.0335,  ..., 0.1715, 0.1461, 0.1568], device='cuda:0')), ('model.model.layers.3.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  50,  56,  57,  54,  49,  54,  50,  50,  55,  49,  52,  57,  57,\n",
      "         54,  51, 125], dtype=torch.uint8)), ('model.model.layers.3.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.3.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.3.self_attn.k_proj.weight', tensor([[149],\n",
      "        [ 98],\n",
      "        [173],\n",
      "        ...,\n",
      "        [ 89],\n",
      "        [134],\n",
      "        [134]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.self_attn.k_proj.weight.absmax', tensor([ 38,  40,   0,  ..., 206, 220, 240], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.3.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.3.self_attn.k_proj.weight.nested_absmax', tensor([0.0310, 0.0928, 0.0360,  ..., 0.1709, 0.2071, 0.1026], device='cuda:0')), ('model.model.layers.3.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.3.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  52,  52,  49,  52,  54,  48,  53,  52,  57,  56,  51,  49,  51,\n",
      "         57, 125], dtype=torch.uint8)), ('model.model.layers.3.self_attn.v_proj.base_layer.weight', tensor([[123],\n",
      "        [ 88],\n",
      "        [ 77],\n",
      "        ...,\n",
      "        [103],\n",
      "        [ 54],\n",
      "        [141]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.self_attn.v_proj.base_layer.weight.absmax', tensor([46, 93, 63,  ..., 50, 56, 53], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.3.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0190, 0.0192, 0.0219,  ..., 0.0236, 0.0234, 0.0763], device='cuda:0')), ('model.model.layers.3.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  53,  48,  48,  50,  50,  54,  56,  56,  53,  48,  56,  48,  51,\n",
      "         51,  55,  53, 125], dtype=torch.uint8)), ('model.model.layers.3.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.3.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.3.self_attn.o_proj.weight', tensor([[ 26],\n",
      "        [ 38],\n",
      "        [101],\n",
      "        ...,\n",
      "        [132],\n",
      "        [202],\n",
      "        [156]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.self_attn.o_proj.weight.absmax', tensor([191, 199,  46,  ..., 216,  23,  28], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.3.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.3.self_attn.o_proj.weight.nested_absmax', tensor([0.0208, 0.0234, 0.0239,  ..., 0.0208, 0.0234, 0.0232], device='cuda:0')), ('model.model.layers.3.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.3.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  52,  49,  55,  48,  53,  51,  56,  49,  56,  55,  48,  50,  54,\n",
      "         57,  56, 125], dtype=torch.uint8)), ('model.model.layers.3.mlp.gate_proj.weight', tensor([[101],\n",
      "        [ 68],\n",
      "        [139],\n",
      "        ...,\n",
      "        [239],\n",
      "        [173],\n",
      "        [ 26]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.mlp.gate_proj.weight.absmax', tensor([222, 195, 197,  ..., 188, 163,  83], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.3.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.3.mlp.gate_proj.weight.nested_absmax', tensor([0.0282, 0.0282, 0.0223,  ..., 0.1669, 0.0233, 0.2030], device='cuda:0')), ('model.model.layers.3.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  57,  54,  53,  55,  49,  56,  53,  57,  55,  49,  55,\n",
      "         51,  54,  57, 125], dtype=torch.uint8)), ('model.model.layers.3.mlp.up_proj.weight', tensor([[ 91],\n",
      "        [157],\n",
      "        [108],\n",
      "        ...,\n",
      "        [ 60],\n",
      "        [158],\n",
      "        [135]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.mlp.up_proj.weight.absmax', tensor([ 56,  58, 208,  ..., 197,  63, 195], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.3.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.3.mlp.up_proj.weight.nested_absmax', tensor([0.0209, 0.0380, 0.0244,  ..., 0.1254, 0.0283, 0.0459], device='cuda:0')), ('model.model.layers.3.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  52,  53,  56,  48,  54,  55,  50,  48,  56,  53,  50,\n",
      "         56,  53,  50, 125], dtype=torch.uint8)), ('model.model.layers.3.mlp.down_proj.weight', tensor([[117],\n",
      "        [150],\n",
      "        [ 43],\n",
      "        ...,\n",
      "        [ 88],\n",
      "        [167],\n",
      "        [154]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.3.mlp.down_proj.weight.absmax', tensor([ 53,  31,  89,  ...,  49, 172, 193], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.3.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.3.mlp.down_proj.weight.nested_absmax', tensor([0.0228, 0.0194, 0.0233,  ..., 0.0292, 0.0316, 0.0282], device='cuda:0')), ('model.model.layers.3.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  53,  52,  55,  53,  56,  53,  57,  51,  52,  52,  48,\n",
      "         48,  53,  54, 125], dtype=torch.uint8)), ('model.model.layers.3.input_layernorm.weight', tensor([0.2793, 0.2793, 0.2793,  ..., 0.2773, 0.2852, 0.2852], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.3.post_attention_layernorm.weight', tensor([0.1738, 0.1719, 0.1680,  ..., 0.1719, 0.1699, 0.1719], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.4.self_attn.q_proj.base_layer.weight', tensor([[ 40],\n",
      "        [101],\n",
      "        [101],\n",
      "        ...,\n",
      "        [179],\n",
      "        [158],\n",
      "        [210]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.4.self_attn.q_proj.base_layer.weight.absmax', tensor([ 13,  25,   0,  ..., 221, 226, 188], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.4.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0415, 0.0365, 0.0559,  ..., 0.3421, 0.1223, 0.0813], device='cuda:0')), ('model.model.layers.4.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  54,  49,  52,  53,  55,  57,  50,  54,  48,  51,  52,  57,  50,\n",
      "         55,  52, 125], dtype=torch.uint8)), ('model.model.layers.4.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.4.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.4.self_attn.k_proj.weight', tensor([[ 75],\n",
      "        [ 57],\n",
      "        [170],\n",
      "        ...,\n",
      "        [135],\n",
      "        [151],\n",
      "        [182]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.4.self_attn.k_proj.weight.absmax', tensor([ 28,  39,  39,  ..., 221, 199, 235], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.4.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.4.self_attn.k_proj.weight.nested_absmax', tensor([0.0499, 0.0440, 0.0953,  ..., 0.1236, 0.0894, 0.0923], device='cuda:0')), ('model.model.layers.4.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.4.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  54,  56,  51,  56,  50,  56,  54,  56,  49,  55,  48,  55,  51,\n",
      "         56,  50, 125], dtype=torch.uint8)), ('model.model.layers.4.self_attn.v_proj.base_layer.weight', tensor([[119],\n",
      "        [ 73],\n",
      "        [148],\n",
      "        ...,\n",
      "        [ 99],\n",
      "        [ 57],\n",
      "        [123]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.4.self_attn.v_proj.base_layer.weight.absmax', tensor([ 45, 197, 190,  ..., 232,  55, 212], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.4.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0196, 0.0269, 0.0181,  ..., 0.0230, 0.0186, 0.0200], device='cuda:0')), ('model.model.layers.4.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  55,  48,  56,  50,  57,  53,  49,  53,  49,  53,  57,  49,  51,\n",
      "         48,  49, 125], dtype=torch.uint8)), ('model.model.layers.4.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.4.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.4.self_attn.o_proj.weight', tensor([[250],\n",
      "        [104],\n",
      "        [ 52],\n",
      "        ...,\n",
      "        [182],\n",
      "        [118],\n",
      "        [200]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.4.self_attn.o_proj.weight.absmax', tensor([ 85, 189,  74,  ..., 195, 207,  73], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.4.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.4.self_attn.o_proj.weight.nested_absmax', tensor([0.0235, 0.0218, 0.0252,  ..., 0.0303, 0.0220, 0.0293], device='cuda:0')), ('model.model.layers.4.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.4.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  54,  48,  56,  50,  57,  54,  56,  49,  49,  53,  56,  48,  54,\n",
      "         53,  56, 125], dtype=torch.uint8)), ('model.model.layers.4.mlp.gate_proj.weight', tensor([[105],\n",
      "        [ 89],\n",
      "        [123],\n",
      "        ...,\n",
      "        [ 67],\n",
      "        [ 85],\n",
      "        [186]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.4.mlp.gate_proj.weight.absmax', tensor([181,  89,  47,  ...,  60,  92, 181], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.4.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.4.mlp.gate_proj.weight.nested_absmax', tensor([0.0292, 0.0321, 0.0194,  ..., 0.0321, 0.0336, 0.0385], device='cuda:0')), ('model.model.layers.4.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  56,  56,  52,  48,  49,  48,  49,  48,  54,  51,  50,\n",
      "         53,  49,  53, 125], dtype=torch.uint8)), ('model.model.layers.4.mlp.up_proj.weight', tensor([[ 69],\n",
      "        [  5],\n",
      "        [ 76],\n",
      "        ...,\n",
      "        [ 54],\n",
      "        [183],\n",
      "        [118]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.4.mlp.up_proj.weight.absmax', tensor([190,  51, 231,  ..., 192,  70,  47], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.4.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.4.mlp.up_proj.weight.nested_absmax', tensor([0.0271, 0.0315, 0.0354,  ..., 0.0262, 0.0208, 0.0208], device='cuda:0')), ('model.model.layers.4.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  49,  51,  53,  56,  54,  51,  49,  50,  53,  51,  50,\n",
      "         52,  50,  53, 125], dtype=torch.uint8)), ('model.model.layers.4.mlp.down_proj.weight', tensor([[230],\n",
      "        [226],\n",
      "        [228],\n",
      "        ...,\n",
      "        [187],\n",
      "        [230],\n",
      "        [  2]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.4.mlp.down_proj.weight.absmax', tensor([ 52,  46, 196,  ..., 221,  67, 175], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.4.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.4.mlp.down_proj.weight.nested_absmax', tensor([0.0272, 0.0252, 0.0228,  ..., 0.0208, 0.0228, 0.0233], device='cuda:0')), ('model.model.layers.4.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  48,  57,  54,  48,  50,  56,  53,  57,  54,  49,  54,\n",
      "         50,  55,  57,  54, 125], dtype=torch.uint8)), ('model.model.layers.4.input_layernorm.weight', tensor([0.2617, 0.2559, 0.2598,  ..., 0.2539, 0.2617, 0.2695], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.4.post_attention_layernorm.weight', tensor([0.1875, 0.1836, 0.1807,  ..., 0.1855, 0.1836, 0.1846], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.5.self_attn.q_proj.base_layer.weight', tensor([[ 85],\n",
      "        [ 42],\n",
      "        [165],\n",
      "        ...,\n",
      "        [187],\n",
      "        [195],\n",
      "        [ 71]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.5.self_attn.q_proj.base_layer.weight.absmax', tensor([ 17,  30,  12,  ..., 116, 161, 153], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.5.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0447, 0.0349, 0.0418,  ..., 0.4803, 0.1248, 0.4002], device='cuda:0')), ('model.model.layers.5.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  54,  53,  57,  57,  49,  52,  53,  53,  51,  49,  54,  53,  52,\n",
      "         51,  54, 125], dtype=torch.uint8)), ('model.model.layers.5.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.5.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.5.self_attn.k_proj.weight', tensor([[ 63],\n",
      "        [157],\n",
      "        [ 92],\n",
      "        ...,\n",
      "        [178],\n",
      "        [ 85],\n",
      "        [ 56]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.5.self_attn.k_proj.weight.absmax', tensor([ 23,  34,  35,  ..., 205, 214, 196], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.5.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.5.self_attn.k_proj.weight.nested_absmax', tensor([0.0553, 0.0383, 0.0517,  ..., 0.1276, 0.0915, 0.1344], device='cuda:0')), ('model.model.layers.5.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.5.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  56,  54,  57,  56,  53,  49,  48,  53,  50,  55,  54,  49,  48,\n",
      "         55,  56, 125], dtype=torch.uint8)), ('model.model.layers.5.self_attn.v_proj.base_layer.weight', tensor([[  9],\n",
      "        [199],\n",
      "        [ 89],\n",
      "        ...,\n",
      "        [215],\n",
      "        [212],\n",
      "        [ 81]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.5.self_attn.v_proj.base_layer.weight.absmax', tensor([ 63, 222, 198,  ...,  58,  45,  37], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.5.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0227, 0.0315, 0.0295,  ..., 0.0200, 0.0246, 0.0232], device='cuda:0')), ('model.model.layers.5.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  55,  56,  56,  52,  50,  52,  54,  53,  53,  55,  57,  53,  48,\n",
      "         57,  55,  52, 125], dtype=torch.uint8)), ('model.model.layers.5.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.5.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.5.self_attn.o_proj.weight', tensor([[ 36],\n",
      "        [136],\n",
      "        [100],\n",
      "        ...,\n",
      "        [101],\n",
      "        [105],\n",
      "        [ 82]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.5.self_attn.o_proj.weight.absmax', tensor([ 37,  94, 192,  ..., 193,  58, 198], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.5.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.5.self_attn.o_proj.weight.nested_absmax', tensor([0.0248, 0.0265, 0.0255,  ..., 0.0241, 0.0226, 0.0245], device='cuda:0')), ('model.model.layers.5.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.5.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  54,  57,  53,  55,  54,  57,  50,  51,  53,  52,  57,  49,  55,\n",
      "         53,  50,  54, 125], dtype=torch.uint8)), ('model.model.layers.5.mlp.gate_proj.weight', tensor([[120],\n",
      "        [ 43],\n",
      "        [ 65],\n",
      "        ...,\n",
      "        [ 26],\n",
      "        [ 86],\n",
      "        [113]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.5.mlp.gate_proj.weight.absmax', tensor([ 76,  35,  42,  ...,  53,  31, 209], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.5.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.5.mlp.gate_proj.weight.nested_absmax', tensor([0.0266, 0.1062, 0.0275,  ..., 0.0227, 0.0691, 0.0222], device='cuda:0')), ('model.model.layers.5.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  48,  48,  54,  54,  57,  56,  53,  49,  56,  57,  57,  49,\n",
      "         52,  55, 125], dtype=torch.uint8)), ('model.model.layers.5.mlp.up_proj.weight', tensor([[100],\n",
      "        [106],\n",
      "        [170],\n",
      "        ...,\n",
      "        [ 33],\n",
      "        [  4],\n",
      "        [ 98]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.5.mlp.up_proj.weight.absmax', tensor([198,  53,  87,  ...,  58,  59,  47], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.5.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.5.mlp.up_proj.weight.nested_absmax', tensor([0.0418, 0.0467, 0.0232,  ..., 0.0257, 0.0598, 0.0242], device='cuda:0')), ('model.model.layers.5.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  49,  52,  49,  56,  53,  51,  51,  57,  50,  49,  50,\n",
      "         52,  49,  55,  54, 125], dtype=torch.uint8)), ('model.model.layers.5.mlp.down_proj.weight', tensor([[ 97],\n",
      "        [106],\n",
      "        [120],\n",
      "        ...,\n",
      "        [ 54],\n",
      "        [142],\n",
      "        [164]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.5.mlp.down_proj.weight.absmax', tensor([143,  94, 179,  ..., 211,  61,  59], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.5.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.5.mlp.down_proj.weight.nested_absmax', tensor([0.0325, 0.0247, 0.0311,  ..., 0.0320, 0.0262, 0.0257], device='cuda:0')), ('model.model.layers.5.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  49,  49,  55,  54,  53,  51,  57,  48,  54,  51,  52,\n",
      "         53,  51,  55, 125], dtype=torch.uint8)), ('model.model.layers.5.input_layernorm.weight', tensor([0.2617, 0.2617, 0.2578,  ..., 0.2500, 0.2656, 0.2676], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.5.post_attention_layernorm.weight', tensor([0.2021, 0.1914, 0.1875,  ..., 0.2041, 0.1973, 0.2012], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.6.self_attn.q_proj.base_layer.weight', tensor([[102],\n",
      "        [142],\n",
      "        [ 81],\n",
      "        ...,\n",
      "        [ 56],\n",
      "        [137],\n",
      "        [120]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.6.self_attn.q_proj.base_layer.weight.absmax', tensor([168,  37,  45,  ..., 226, 206, 223], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.6.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0304, 0.0329, 0.0275,  ..., 0.1256, 0.2624, 0.0758], device='cuda:0')), ('model.model.layers.6.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  49,  56,  53,  54,  54,  49,  50,  53,  54,  51,  49,  51,  51,\n",
      "         50,  52, 125], dtype=torch.uint8)), ('model.model.layers.6.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.6.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.6.self_attn.k_proj.weight', tensor([[165],\n",
      "        [162],\n",
      "        [152],\n",
      "        ...,\n",
      "        [ 39],\n",
      "        [ 74],\n",
      "        [ 35]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.6.self_attn.k_proj.weight.absmax', tensor([201,  20,  26,  ..., 234, 205, 171], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.6.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.6.self_attn.k_proj.weight.nested_absmax', tensor([0.0278, 0.0406, 0.0278,  ..., 0.0860, 0.1641, 0.0772], device='cuda:0')), ('model.model.layers.6.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.6.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  50,  52,  52,  49,  56,  52,  56,  50,  49,  56,  52,  52,  49,\n",
      "         48,  49, 125], dtype=torch.uint8)), ('model.model.layers.6.self_attn.v_proj.base_layer.weight', tensor([[175],\n",
      "        [137],\n",
      "        [191],\n",
      "        ...,\n",
      "        [183],\n",
      "        [242],\n",
      "        [118]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.6.self_attn.v_proj.base_layer.weight.absmax', tensor([191, 191, 189,  ..., 171, 204, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.6.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0303, 0.0312, 0.0271,  ..., 0.0256, 0.0251, 0.0210], device='cuda:0')), ('model.model.layers.6.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  53,  49,  54,  48,  50,  49,  51,  55,  48,  56,  56,  55,  55,\n",
      "         53,  54,  51, 125], dtype=torch.uint8)), ('model.model.layers.6.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.6.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.6.self_attn.o_proj.weight', tensor([[195],\n",
      "        [  9],\n",
      "        [103],\n",
      "        ...,\n",
      "        [ 19],\n",
      "        [195],\n",
      "        [154]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.6.self_attn.o_proj.weight.absmax', tensor([191, 209,  50,  ...,  59, 195, 223], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.6.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.6.self_attn.o_proj.weight.nested_absmax', tensor([0.0219, 0.0207, 0.0202,  ..., 0.0280, 0.0187, 0.0202], device='cuda:0')), ('model.model.layers.6.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.6.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  52,  52,  55,  56,  57,  49,  55,  55,  49,  55,  57,  51,  51,\n",
      "         54,  53,  53, 125], dtype=torch.uint8)), ('model.model.layers.6.mlp.gate_proj.weight', tensor([[221],\n",
      "        [ 55],\n",
      "        [230],\n",
      "        ...,\n",
      "        [161],\n",
      "        [209],\n",
      "        [139]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.6.mlp.gate_proj.weight.absmax', tensor([ 37,  45,  62,  ..., 193, 208, 190], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.6.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.6.mlp.gate_proj.weight.nested_absmax', tensor([0.0241, 0.0488, 0.0395,  ..., 0.0346, 0.0414, 0.0478], device='cuda:0')), ('model.model.layers.6.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  48,  56,  52,  48,  55,  56,  51,  56,  54,  52,  50,  53,\n",
      "         57,  55,  50, 125], dtype=torch.uint8)), ('model.model.layers.6.mlp.up_proj.weight', tensor([[ 85],\n",
      "        [ 73],\n",
      "        [ 88],\n",
      "        ...,\n",
      "        [250],\n",
      "        [185],\n",
      "        [ 64]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.6.mlp.up_proj.weight.absmax', tensor([193,  53,  56,  ...,  48, 222, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.6.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.6.mlp.up_proj.weight.nested_absmax', tensor([0.0326, 0.0375, 0.0258,  ..., 0.0204, 0.0336, 0.0209], device='cuda:0')), ('model.model.layers.6.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  52,  57,  57,  54,  51,  49,  51,  55,  53,  48,  55,  52,\n",
      "         51,  56,  54,  54, 125], dtype=torch.uint8)), ('model.model.layers.6.mlp.down_proj.weight', tensor([[ 91],\n",
      "        [105],\n",
      "        [176],\n",
      "        ...,\n",
      "        [226],\n",
      "        [175],\n",
      "        [ 89]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.6.mlp.down_proj.weight.absmax', tensor([ 73, 199,  49,  ...,  93, 183,  46], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.6.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.6.mlp.down_proj.weight.nested_absmax', tensor([0.0318, 0.0240, 0.0230,  ..., 0.0289, 0.0264, 0.0260], device='cuda:0')), ('model.model.layers.6.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  52,  56,  53,  48,  55,  49,  52,  53,  48,  52,  55,  49,\n",
      "         56,  55,  56, 125], dtype=torch.uint8)), ('model.model.layers.6.input_layernorm.weight', tensor([0.3145, 0.3516, 0.3262,  ..., 0.3125, 0.3340, 0.3184], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.6.post_attention_layernorm.weight', tensor([0.2148, 0.2061, 0.2021,  ..., 0.2168, 0.2109, 0.2100], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.7.self_attn.q_proj.base_layer.weight', tensor([[169],\n",
      "        [115],\n",
      "        [ 33],\n",
      "        ...,\n",
      "        [231],\n",
      "        [158],\n",
      "        [182]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.self_attn.q_proj.base_layer.weight.absmax', tensor([ 13,  20,  15,  ..., 198, 193, 199], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.7.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0418, 0.0431, 0.0427,  ..., 0.0888, 0.1533, 0.2060], device='cuda:0')), ('model.model.layers.7.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  49,  53,  52,  54,  57,  53,  56,  57,  56,  50,  57,  52,  52,\n",
      "         52,  57, 125], dtype=torch.uint8)), ('model.model.layers.7.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.7.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.7.self_attn.k_proj.weight', tensor([[100],\n",
      "        [ 36],\n",
      "        [118],\n",
      "        ...,\n",
      "        [198],\n",
      "        [184],\n",
      "        [132]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.self_attn.k_proj.weight.absmax', tensor([ 18,  10,   4,  ..., 204, 205, 201], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.7.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.7.self_attn.k_proj.weight.nested_absmax', tensor([0.0410, 0.0417, 0.0417,  ..., 0.0878, 0.1015, 0.1249], device='cuda:0')), ('model.model.layers.7.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.7.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  49,  53,  57,  50,  51,  49,  52,  51,  57,  50,  51,  50,  56,\n",
      "         50,  54, 125], dtype=torch.uint8)), ('model.model.layers.7.self_attn.v_proj.base_layer.weight', tensor([[ 56],\n",
      "        [201],\n",
      "        [ 74],\n",
      "        ...,\n",
      "        [124],\n",
      "        [ 46],\n",
      "        [171]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.self_attn.v_proj.base_layer.weight.absmax', tensor([185, 185, 159,  ...,  79, 187,  72], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.7.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0269, 0.0299, 0.0352,  ..., 0.0162, 0.0162, 0.0250], device='cuda:0')), ('model.model.layers.7.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  53,  53,  53,  56,  49,  56,  54,  52,  55,  49,  52,  54,  50,\n",
      "         50,  53, 125], dtype=torch.uint8)), ('model.model.layers.7.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.7.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.7.self_attn.o_proj.weight', tensor([[179],\n",
      "        [102],\n",
      "        [134],\n",
      "        ...,\n",
      "        [ 22],\n",
      "        [ 57],\n",
      "        [ 41]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.self_attn.o_proj.weight.absmax', tensor([207, 208, 200,  ..., 210,  31,  54], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.7.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.7.self_attn.o_proj.weight.nested_absmax', tensor([0.0297, 0.0192, 0.0233,  ..., 0.0231, 0.0221, 0.0209], device='cuda:0')), ('model.model.layers.7.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.7.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  52,  55,  54,  51,  49,  50,  48,  49,  49,  52,  56,  48,  51,\n",
      "         51,  49,  52, 125], dtype=torch.uint8)), ('model.model.layers.7.mlp.gate_proj.weight', tensor([[150],\n",
      "        [ 57],\n",
      "        [177],\n",
      "        ...,\n",
      "        [117],\n",
      "        [ 60],\n",
      "        [199]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.mlp.gate_proj.weight.absmax', tensor([58, 55, 61,  ..., 58, 55, 49], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.7.mlp.gate_proj.weight.nested_absmax', tensor([0.0567, 0.0381, 0.0264,  ..., 0.0298, 0.0367, 0.0279], device='cuda:0')), ('model.model.layers.7.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  48,  55,  51,  49,  55,  57,  54,  55,  55,  49,  50,  56,\n",
      "         55,  57,  50, 125], dtype=torch.uint8)), ('model.model.layers.7.mlp.up_proj.weight', tensor([[215],\n",
      "        [ 97],\n",
      "        [ 34],\n",
      "        ...,\n",
      "        [250],\n",
      "        [188],\n",
      "        [168]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.mlp.up_proj.weight.absmax', tensor([ 45,  38,  52,  ...,  45,  61, 171], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.7.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.7.mlp.up_proj.weight.nested_absmax', tensor([0.0237, 0.0281, 0.0203,  ..., 0.0247, 0.0320, 0.0227], device='cuda:0')), ('model.model.layers.7.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  49,  51,  54,  53,  49,  53,  48,  53,  49,  49,  50,\n",
      "         54,  52,  56, 125], dtype=torch.uint8)), ('model.model.layers.7.mlp.down_proj.weight', tensor([[ 84],\n",
      "        [124],\n",
      "        [ 88],\n",
      "        ...,\n",
      "        [126],\n",
      "        [119],\n",
      "        [ 55]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.7.mlp.down_proj.weight.absmax', tensor([165,  42,  33,  ..., 213, 195, 213], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.7.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.7.mlp.down_proj.weight.nested_absmax', tensor([0.0244, 0.0259, 0.0434,  ..., 0.0263, 0.0215, 0.0439], device='cuda:0')), ('model.model.layers.7.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  52,  57,  52,  55,  52,  55,  49,  52,  54,  57,  54,  52,\n",
      "         48,  55,  51, 125], dtype=torch.uint8)), ('model.model.layers.7.input_layernorm.weight', tensor([0.3203, 0.3613, 0.3359,  ..., 0.3242, 0.3535, 0.3262], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.7.post_attention_layernorm.weight', tensor([0.2305, 0.2158, 0.2139,  ..., 0.2236, 0.2217, 0.2236], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.8.self_attn.q_proj.base_layer.weight', tensor([[164],\n",
      "        [ 18],\n",
      "        [152],\n",
      "        ...,\n",
      "        [ 53],\n",
      "        [146],\n",
      "        [181]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.8.self_attn.q_proj.base_layer.weight.absmax', tensor([ 30,  23,  18,  ..., 200, 196, 191], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.8.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0423, 0.0336, 0.0361,  ..., 0.1214, 0.1722, 0.3011], device='cuda:0')), ('model.model.layers.8.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  50,  49,  56,  49,  53,  49,  51,  55,  53,  54,  53,  49,  51,\n",
      "         53,  57,  54, 125], dtype=torch.uint8)), ('model.model.layers.8.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.8.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.8.self_attn.k_proj.weight', tensor([[ 88],\n",
      "        [ 93],\n",
      "        [182],\n",
      "        ...,\n",
      "        [ 36],\n",
      "        [ 27],\n",
      "        [133]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.8.self_attn.k_proj.weight.absmax', tensor([ 22,  12,   4,  ..., 198, 217, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.8.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.8.self_attn.k_proj.weight.nested_absmax', tensor([0.0394, 0.0349, 0.0392,  ..., 0.0957, 0.1094, 0.1338], device='cuda:0')), ('model.model.layers.8.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.8.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  50,  52,  57,  48,  49,  55,  50,  54,  56,  52,  49,  57,  50,\n",
      "         54,  54, 125], dtype=torch.uint8)), ('model.model.layers.8.self_attn.v_proj.base_layer.weight', tensor([[ 19],\n",
      "        [133],\n",
      "        [114],\n",
      "        ...,\n",
      "        [114],\n",
      "        [ 38],\n",
      "        [149]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.8.self_attn.v_proj.base_layer.weight.absmax', tensor([184,  82, 159,  ...,  46, 209,  31], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.8.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0210, 0.0283, 0.0337,  ..., 0.0166, 0.0229, 0.0180], device='cuda:0')), ('model.model.layers.8.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  54,  54,  52,  49,  48,  55,  57,  57,  51,  50,  52,  53,  49,\n",
      "         50,  53, 125], dtype=torch.uint8)), ('model.model.layers.8.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.8.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.8.self_attn.o_proj.weight', tensor([[204],\n",
      "        [ 60],\n",
      "        [194],\n",
      "        ...,\n",
      "        [120],\n",
      "        [126],\n",
      "        [ 54]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.8.self_attn.o_proj.weight.absmax', tensor([ 41, 234,  37,  ..., 185,  46,  34], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.8.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.8.self_attn.o_proj.weight.nested_absmax', tensor([0.0174, 0.0276, 0.0315,  ..., 0.0217, 0.0244, 0.0203], device='cuda:0')), ('model.model.layers.8.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.8.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  53,  56,  55,  50,  48,  49,  57,  56,  50,  55,  51,  54,  53,\n",
      "         56,  55,  53, 125], dtype=torch.uint8)), ('model.model.layers.8.mlp.gate_proj.weight', tensor([[151],\n",
      "        [ 44],\n",
      "        [153],\n",
      "        ...,\n",
      "        [ 36],\n",
      "        [ 99],\n",
      "        [183]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.8.mlp.gate_proj.weight.absmax', tensor([245, 194,  56,  ...,  49,  28,  25], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.8.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.8.mlp.gate_proj.weight.nested_absmax', tensor([0.0351, 0.0585, 0.0604,  ..., 0.0385, 0.0277, 0.0200], device='cuda:0')), ('model.model.layers.8.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  57,  48,  55,  49,  50,  57,  50,  53,  55,  57,  49,\n",
      "         55,  52,  48,  52, 125], dtype=torch.uint8)), ('model.model.layers.8.mlp.up_proj.weight', tensor([[ 91],\n",
      "        [ 70],\n",
      "        [237],\n",
      "        ...,\n",
      "        [ 53],\n",
      "        [148],\n",
      "        [120]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.8.mlp.up_proj.weight.absmax', tensor([190, 179,  64,  ...,  60,  59,  91], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.8.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.8.mlp.up_proj.weight.nested_absmax', tensor([0.0422, 0.0393, 0.0339,  ..., 0.0325, 0.0237, 0.0344], device='cuda:0')), ('model.model.layers.8.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  54,  55,  52,  52,  54,  57,  51,  50,  49,  57,  54,\n",
      "         54,  49,  55, 125], dtype=torch.uint8)), ('model.model.layers.8.mlp.down_proj.weight', tensor([[ 86],\n",
      "        [183],\n",
      "        [168],\n",
      "        ...,\n",
      "        [132],\n",
      "        [168],\n",
      "        [135]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.8.mlp.down_proj.weight.absmax', tensor([201, 217, 170,  ..., 236,  60, 221], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.8.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.8.mlp.down_proj.weight.nested_absmax', tensor([0.0332, 0.0200, 0.0195,  ..., 0.0346, 0.0229, 0.0205], device='cuda:0')), ('model.model.layers.8.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  52,  52,  56,  56,  54,  57,  52,  54,  54,  55,  56,\n",
      "         49,  54,  49,  54, 125], dtype=torch.uint8)), ('model.model.layers.8.input_layernorm.weight', tensor([0.3281, 0.3398, 0.3281,  ..., 0.3164, 0.3398, 0.3184], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.8.post_attention_layernorm.weight', tensor([0.2354, 0.2207, 0.2129,  ..., 0.2344, 0.2275, 0.2246], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.9.self_attn.q_proj.base_layer.weight', tensor([[ 56],\n",
      "        [186],\n",
      "        [178],\n",
      "        ...,\n",
      "        [ 17],\n",
      "        [146],\n",
      "        [198]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.9.self_attn.q_proj.base_layer.weight.absmax', tensor([ 38,  19,   8,  ..., 170, 199, 205], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.9.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0353, 0.0304, 0.0338,  ..., 0.2631, 0.0560, 0.1019], device='cuda:0')), ('model.model.layers.9.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  51,  48,  57,  54,  49,  56,  56,  48,  48,  56,  55,  56,  53,\n",
      "         50,  53, 125], dtype=torch.uint8)), ('model.model.layers.9.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.9.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.9.self_attn.k_proj.weight', tensor([[ 10],\n",
      "        [ 58],\n",
      "        [ 82],\n",
      "        ...,\n",
      "        [ 58],\n",
      "        [ 41],\n",
      "        [180]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.9.self_attn.k_proj.weight.absmax', tensor([ 23,   4,  44,  ..., 204, 193, 160], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.9.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.9.self_attn.k_proj.weight.nested_absmax', tensor([0.0376, 0.0321, 0.0445,  ..., 0.0841, 0.0704, 0.0724], device='cuda:0')), ('model.model.layers.9.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.9.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  52,  51,  53,  50,  48,  53,  48,  52,  50,  51,  54,  50,  50,\n",
      "         49,  51, 125], dtype=torch.uint8)), ('model.model.layers.9.self_attn.v_proj.base_layer.weight', tensor([[105],\n",
      "        [121],\n",
      "        [ 53],\n",
      "        ...,\n",
      "        [183],\n",
      "        [131],\n",
      "        [102]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.9.self_attn.v_proj.base_layer.weight.absmax', tensor([209, 191, 193,  ...,  24,  36,  58], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.9.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0364, 0.0257, 0.0305,  ..., 0.0215, 0.0177, 0.0166], device='cuda:0')), ('model.model.layers.9.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  55,  51,  51,  48,  48,  50,  48,  50,  49,  57,  48,  56,  55,\n",
      "         54, 125], dtype=torch.uint8)), ('model.model.layers.9.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.9.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.9.self_attn.o_proj.weight', tensor([[170],\n",
      "        [129],\n",
      "        [173],\n",
      "        ...,\n",
      "        [199],\n",
      "        [214],\n",
      "        [170]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.9.self_attn.o_proj.weight.absmax', tensor([194, 208, 175,  ...,  61,  34,  39], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.9.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.9.self_attn.o_proj.weight.nested_absmax', tensor([0.0269, 0.0303, 0.0239,  ..., 0.0257, 0.0308, 0.0174], device='cuda:0')), ('model.model.layers.9.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.9.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  54,  54,  48,  49,  51,  48,  53,  48,  48,  55,  57,  51,  52,\n",
      "         53,  55, 125], dtype=torch.uint8)), ('model.model.layers.9.mlp.gate_proj.weight', tensor([[ 93],\n",
      "        [105],\n",
      "        [245],\n",
      "        ...,\n",
      "        [ 84],\n",
      "        [ 58],\n",
      "        [ 73]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.9.mlp.gate_proj.weight.absmax', tensor([ 62, 158,  57,  ..., 174,  49, 202], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.9.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.9.mlp.gate_proj.weight.nested_absmax', tensor([0.0473, 0.0537, 0.0298,  ..., 0.0298, 0.0204, 0.0268], device='cuda:0')), ('model.model.layers.9.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  51,  53,  48,  51,  51,  57,  57,  49,  57,  51,  50,\n",
      "         56,  54,  57, 125], dtype=torch.uint8)), ('model.model.layers.9.mlp.up_proj.weight', tensor([[ 89],\n",
      "        [168],\n",
      "        [117],\n",
      "        ...,\n",
      "        [ 67],\n",
      "        [179],\n",
      "        [232]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.9.mlp.up_proj.weight.absmax', tensor([197, 200, 213,  ...,  54,  44,  51], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.9.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.9.mlp.up_proj.weight.nested_absmax', tensor([0.0414, 0.0326, 0.0219,  ..., 0.0258, 0.0238, 0.0336], device='cuda:0')), ('model.model.layers.9.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  57,  55,  57,  54,  53,  54,  50,  55,  57,  48,  56,\n",
      "         55,  48,  55, 125], dtype=torch.uint8)), ('model.model.layers.9.mlp.down_proj.weight', tensor([[109],\n",
      "        [  5],\n",
      "        [231],\n",
      "        ...,\n",
      "        [166],\n",
      "        [174],\n",
      "        [187]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.9.mlp.down_proj.weight.absmax', tensor([ 74, 166, 163,  ...,  54,  58, 195], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.9.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.9.mlp.down_proj.weight.nested_absmax', tensor([0.0337, 0.0362, 0.0323,  ..., 0.0211, 0.0391, 0.0250], device='cuda:0')), ('model.model.layers.9.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  53,  56,  52,  51,  50,  51,  57,  56,  55,  51,  54,  52,\n",
      "         55,  54,  57, 125], dtype=torch.uint8)), ('model.model.layers.9.input_layernorm.weight', tensor([0.3496, 0.3535, 0.3203,  ..., 0.3457, 0.3418, 0.3340], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.9.post_attention_layernorm.weight', tensor([0.2402, 0.2305, 0.2197,  ..., 0.2363, 0.2344, 0.2305], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.10.self_attn.q_proj.base_layer.weight', tensor([[122],\n",
      "        [119],\n",
      "        [234],\n",
      "        ...,\n",
      "        [197],\n",
      "        [182],\n",
      "        [ 52]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.10.self_attn.q_proj.base_layer.weight.absmax', tensor([ 10,  11,  31,  ..., 200, 228, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.10.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0407, 0.0408, 0.0393,  ..., 0.2500, 0.0791, 0.0664], device='cuda:0')), ('model.model.layers.10.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  50,  53,  50,  48,  54,  48,  56,  51,  48,  53,  57,  51,  49,\n",
      "         48,  57, 125], dtype=torch.uint8)), ('model.model.layers.10.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.10.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.10.self_attn.k_proj.weight', tensor([[ 13],\n",
      "        [ 91],\n",
      "        [105],\n",
      "        ...,\n",
      "        [ 20],\n",
      "        [ 42],\n",
      "        [229]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.10.self_attn.k_proj.weight.absmax', tensor([  2,  21,  29,  ..., 242, 199, 151], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.10.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.10.self_attn.k_proj.weight.nested_absmax', tensor([0.0422, 0.0434, 0.0418,  ..., 0.1039, 0.0785, 0.0727], device='cuda:0')), ('model.model.layers.10.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.10.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  52,  48,  52,  51,  56,  54,  52,  54,  48,  55,  56,  49,  48,\n",
      "         57,  55, 125], dtype=torch.uint8)), ('model.model.layers.10.self_attn.v_proj.base_layer.weight', tensor([[ 24],\n",
      "        [107],\n",
      "        [216],\n",
      "        ...,\n",
      "        [ 56],\n",
      "        [181],\n",
      "        [ 62]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.10.self_attn.v_proj.base_layer.weight.absmax', tensor([223, 222, 197,  ..., 218,  64,  80], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.10.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0376, 0.0333, 0.0279,  ..., 0.0567, 0.0264, 0.0220], device='cuda:0')), ('model.model.layers.10.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  55,  48,  53,  57,  48,  49,  50,  56,  48,  48,  52,  53,  53,\n",
      "         48,  57, 125], dtype=torch.uint8)), ('model.model.layers.10.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.10.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.10.self_attn.o_proj.weight', tensor([[120],\n",
      "        [ 30],\n",
      "        [ 72],\n",
      "        ...,\n",
      "        [161],\n",
      "        [154],\n",
      "        [120]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.10.self_attn.o_proj.weight.absmax', tensor([210, 218,  46,  ...,  51,  91, 204], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.10.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.10.self_attn.o_proj.weight.nested_absmax', tensor([0.0236, 0.0280, 0.0214,  ..., 0.0290, 0.0236, 0.0260], device='cuda:0')), ('model.model.layers.10.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.10.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  54,  52,  55,  49,  55,  50,  48,  55,  56,  52,  57,  48,  50,\n",
      "         53,  55, 125], dtype=torch.uint8)), ('model.model.layers.10.mlp.gate_proj.weight', tensor([[ 16],\n",
      "        [ 25],\n",
      "        [ 93],\n",
      "        ...,\n",
      "        [147],\n",
      "        [181],\n",
      "        [132]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.10.mlp.gate_proj.weight.absmax', tensor([ 56,  59,  57,  ..., 182,  97, 205], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.10.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.10.mlp.gate_proj.weight.nested_absmax', tensor([0.0300, 0.0710, 0.0520,  ..., 0.0427, 0.0442, 0.0261], device='cuda:0')), ('model.model.layers.10.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  48,  55,  48,  57,  52,  54,  56,  55,  50,  50,  51,\n",
      "         52,  51,  52,  52, 125], dtype=torch.uint8)), ('model.model.layers.10.mlp.up_proj.weight', tensor([[ 22],\n",
      "        [ 21],\n",
      "        [ 87],\n",
      "        ...,\n",
      "        [  4],\n",
      "        [215],\n",
      "        [156]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.10.mlp.up_proj.weight.absmax', tensor([214, 232,  55,  ...,  62,  61,  49], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.10.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.10.mlp.up_proj.weight.nested_absmax', tensor([0.0292, 0.0322, 0.0361,  ..., 0.0351, 0.0331, 0.0207], device='cuda:0')), ('model.model.layers.10.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  54,  52,  54,  50,  57,  56,  54,  54,  49,  56,  50,  56,\n",
      "         48,  52,  49, 125], dtype=torch.uint8)), ('model.model.layers.10.mlp.down_proj.weight', tensor([[ 82],\n",
      "        [109],\n",
      "        [150],\n",
      "        ...,\n",
      "        [ 58],\n",
      "        [179],\n",
      "        [ 73]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.10.mlp.down_proj.weight.absmax', tensor([193,  62, 147,  ..., 187,  44, 202], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.10.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.10.mlp.down_proj.weight.nested_absmax', tensor([0.0289, 0.0284, 0.0323,  ..., 0.0333, 0.0235, 0.0230], device='cuda:0')), ('model.model.layers.10.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  54,  50,  57,  48,  57,  54,  55,  54,  49,  51,  52,  53,\n",
      "         56,  54,  51, 125], dtype=torch.uint8)), ('model.model.layers.10.input_layernorm.weight', tensor([0.3594, 0.3555, 0.3184,  ..., 0.3359, 0.3438, 0.3340], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.10.post_attention_layernorm.weight', tensor([0.2451, 0.2295, 0.2217,  ..., 0.2402, 0.2383, 0.2354], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.11.self_attn.q_proj.base_layer.weight', tensor([[204],\n",
      "        [ 66],\n",
      "        [ 34],\n",
      "        ...,\n",
      "        [156],\n",
      "        [244],\n",
      "        [222]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.11.self_attn.q_proj.base_layer.weight.absmax', tensor([ 21,  46,  14,  ..., 218, 188, 200], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.11.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0359, 0.0375, 0.0363,  ..., 0.0764, 0.2912, 0.0754], device='cuda:0')), ('model.model.layers.11.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  56,  51,  57,  54,  53,  56,  53,  50,  56,  53,  54,  54,  51,\n",
      "         54,  48,  53, 125], dtype=torch.uint8)), ('model.model.layers.11.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.11.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.11.self_attn.k_proj.weight', tensor([[155],\n",
      "        [117],\n",
      "        [139],\n",
      "        ...,\n",
      "        [187],\n",
      "        [150],\n",
      "        [251]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.11.self_attn.k_proj.weight.absmax', tensor([ 51,  19,  24,  ..., 195, 224, 178], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.11.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.11.self_attn.k_proj.weight.nested_absmax', tensor([0.0364, 0.0367, 0.0339,  ..., 0.0695, 0.1574, 0.0622], device='cuda:0')), ('model.model.layers.11.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.11.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  56,  52,  49,  48,  49,  49,  53,  53,  52,  48,  48,  50,  55,\n",
      "         54,  50, 125], dtype=torch.uint8)), ('model.model.layers.11.self_attn.v_proj.base_layer.weight', tensor([[136],\n",
      "        [ 84],\n",
      "        [149],\n",
      "        ...,\n",
      "        [103],\n",
      "        [128],\n",
      "        [100]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.11.self_attn.v_proj.base_layer.weight.absmax', tensor([239, 202,  85,  ..., 195, 100, 193], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.11.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0188, 0.0237, 0.0149,  ..., 0.0425, 0.0454, 0.0351], device='cuda:0')), ('model.model.layers.11.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  56,  53,  57,  49,  51,  52,  48,  49,  56,  52,  50,  49,  49,\n",
      "         55,  51, 125], dtype=torch.uint8)), ('model.model.layers.11.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.11.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.11.self_attn.o_proj.weight', tensor([[ 22],\n",
      "        [160],\n",
      "        [  1],\n",
      "        ...,\n",
      "        [193],\n",
      "        [ 86],\n",
      "        [116]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.11.self_attn.o_proj.weight.absmax', tensor([ 40, 179,  56,  ...,  78, 214, 193], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.11.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.11.self_attn.o_proj.weight.nested_absmax', tensor([0.0320, 0.0286, 0.0325,  ..., 0.0271, 0.0276, 0.0325], device='cuda:0')), ('model.model.layers.11.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.11.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  55,  56,  52,  55,  56,  51,  53,  53,  55,  48,  53,  55,  51,\n",
      "         56,  49, 125], dtype=torch.uint8)), ('model.model.layers.11.mlp.gate_proj.weight', tensor([[  5],\n",
      "        [119],\n",
      "        [169],\n",
      "        ...,\n",
      "        [ 89],\n",
      "        [141],\n",
      "        [100]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.11.mlp.gate_proj.weight.absmax', tensor([181,  52, 210,  ...,  47, 203, 203], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.11.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.11.mlp.gate_proj.weight.nested_absmax', tensor([0.0253, 0.0570, 0.0321,  ..., 0.0345, 0.0282, 0.0331], device='cuda:0')), ('model.model.layers.11.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  57,  53,  53,  50,  55,  50,  56,  56,  51,  49,  55,\n",
      "         54,  56,  48,  52, 125], dtype=torch.uint8)), ('model.model.layers.11.mlp.up_proj.weight', tensor([[ 86],\n",
      "        [ 45],\n",
      "        [ 57],\n",
      "        ...,\n",
      "        [187],\n",
      "        [ 68],\n",
      "        [157]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.11.mlp.up_proj.weight.absmax', tensor([189,  62,  95,  ...,  38,  86,  60], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.11.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.11.mlp.up_proj.weight.nested_absmax', tensor([0.0381, 0.0288, 0.0371,  ..., 0.0386, 0.0200, 0.0313], device='cuda:0')), ('model.model.layers.11.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  54,  56,  52,  52,  54,  53,  48,  48,  53,  57,  57,  51,\n",
      "         56,  52,  51, 125], dtype=torch.uint8)), ('model.model.layers.11.mlp.down_proj.weight', tensor([[ 63],\n",
      "        [183],\n",
      "        [166],\n",
      "        ...,\n",
      "        [198],\n",
      "        [  8],\n",
      "        [ 79]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.11.mlp.down_proj.weight.absmax', tensor([ 71, 168, 192,  ...,  57,  61,  86], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.11.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.11.mlp.down_proj.weight.nested_absmax', tensor([0.0233, 0.0243, 0.0262,  ..., 0.0247, 0.0233, 0.0311], device='cuda:0')), ('model.model.layers.11.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  54,  53,  51,  57,  54,  57,  48,  51,  52,  53,  53,  50,\n",
      "         53,  55,  52, 125], dtype=torch.uint8)), ('model.model.layers.11.input_layernorm.weight', tensor([0.3906, 0.3887, 0.3555,  ..., 0.3770, 0.3730, 0.3652], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.11.post_attention_layernorm.weight', tensor([0.2500, 0.2363, 0.2324,  ..., 0.2480, 0.2471, 0.2441], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.12.self_attn.q_proj.base_layer.weight', tensor([[ 81],\n",
      "        [185],\n",
      "        [211],\n",
      "        ...,\n",
      "        [149],\n",
      "        [116],\n",
      "        [188]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.12.self_attn.q_proj.base_layer.weight.absmax', tensor([  4,   9,  26,  ..., 179, 152,  92], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.12.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0371, 0.0334, 0.0330,  ..., 0.0423, 0.1453, 0.1267], device='cuda:0')), ('model.model.layers.12.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  48,  55,  53,  50,  49,  56,  51,  49,  57,  56,  57,  50,  56,\n",
      "         56,  51, 125], dtype=torch.uint8)), ('model.model.layers.12.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.12.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.12.self_attn.k_proj.weight', tensor([[181],\n",
      "        [119],\n",
      "        [ 80],\n",
      "        ...,\n",
      "        [169],\n",
      "        [105],\n",
      "        [ 19]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.12.self_attn.k_proj.weight.absmax', tensor([ 16,  11,   8,  ...,  87, 192, 203], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.12.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.12.self_attn.k_proj.weight.nested_absmax', tensor([0.0374, 0.0335, 0.0637,  ..., 0.0520, 0.0539, 0.0442], device='cuda:0')), ('model.model.layers.12.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.12.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  50,  50,  57,  50,  53,  55,  53,  56,  51,  54,  49,  56,  49,\n",
      "         54,  52, 125], dtype=torch.uint8)), ('model.model.layers.12.self_attn.v_proj.base_layer.weight', tensor([[153],\n",
      "        [153],\n",
      "        [101],\n",
      "        ...,\n",
      "        [201],\n",
      "        [ 77],\n",
      "        [148]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.12.self_attn.v_proj.base_layer.weight.absmax', tensor([ 79,  94, 217,  ...,  59,  62,  81], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.12.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0171, 0.0181, 0.0308,  ..., 0.0183, 0.0175, 0.0435], device='cuda:0')), ('model.model.layers.12.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  56,  48,  57,  50,  57,  51,  51,  53,  57,  53,  49,  56,  48,\n",
      "         53,  49, 125], dtype=torch.uint8)), ('model.model.layers.12.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.12.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.12.self_attn.o_proj.weight', tensor([[233],\n",
      "        [116],\n",
      "        [233],\n",
      "        ...,\n",
      "        [197],\n",
      "        [222],\n",
      "        [ 50]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.12.self_attn.o_proj.weight.absmax', tensor([172, 183,  54,  ...,  84,  55,  60], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.12.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.12.self_attn.o_proj.weight.nested_absmax', tensor([0.0233, 0.0219, 0.0284,  ..., 0.0260, 0.0299, 0.0314], device='cuda:0')), ('model.model.layers.12.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.12.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  55,  52,  55,  51,  56,  55,  57,  55,  53,  52,  53,  52,  51,\n",
      "         51,  48,  52, 125], dtype=torch.uint8)), ('model.model.layers.12.mlp.gate_proj.weight', tensor([[147],\n",
      "        [147],\n",
      "        [179],\n",
      "        ...,\n",
      "        [100],\n",
      "        [198],\n",
      "        [ 83]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.12.mlp.gate_proj.weight.absmax', tensor([197, 183, 194,  ...,  30,  34,  54], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.12.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.12.mlp.gate_proj.weight.nested_absmax', tensor([0.0456, 0.0353, 0.0543,  ..., 0.0299, 0.0280, 0.0211], device='cuda:0')), ('model.model.layers.12.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  54,  56,  49,  52,  54,  52,  48,  52,  54,  50,  51,\n",
      "         57,  56,  53, 125], dtype=torch.uint8)), ('model.model.layers.12.mlp.up_proj.weight', tensor([[139],\n",
      "        [139],\n",
      "        [153],\n",
      "        ...,\n",
      "        [ 91],\n",
      "        [ 78],\n",
      "        [ 97]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.12.mlp.up_proj.weight.absmax', tensor([213, 255, 198,  ...,  51, 255, 174], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.12.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.12.mlp.up_proj.weight.nested_absmax', tensor([0.0226, 0.0231, 0.0221,  ..., 0.0807, 0.0289, 0.0299], device='cuda:0')), ('model.model.layers.12.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  50,  51,  48,  48,  52,  50,  53,  49,  55,  49,  56,\n",
      "         53,  50,  49, 125], dtype=torch.uint8)), ('model.model.layers.12.mlp.down_proj.weight', tensor([[134],\n",
      "        [ 26],\n",
      "        [139],\n",
      "        ...,\n",
      "        [201],\n",
      "        [ 64],\n",
      "        [215]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.12.mlp.down_proj.weight.absmax', tensor([ 72,  56,  58,  ...,  51, 195,  38], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.12.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.12.mlp.down_proj.weight.nested_absmax', tensor([0.0297, 0.0253, 0.0297,  ..., 0.0292, 0.0356, 0.0253], device='cuda:0')), ('model.model.layers.12.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  54,  57,  51,  56,  49,  48,  50,  54,  57,  50,  51,  54,\n",
      "         53,  54,  52,  54, 125], dtype=torch.uint8)), ('model.model.layers.12.input_layernorm.weight', tensor([0.3984, 0.3926, 0.3613,  ..., 0.3730, 0.3789, 0.3809], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.12.post_attention_layernorm.weight', tensor([0.2578, 0.2432, 0.2363,  ..., 0.2539, 0.2520, 0.2520], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.13.self_attn.q_proj.base_layer.weight', tensor([[100],\n",
      "        [ 63],\n",
      "        [176],\n",
      "        ...,\n",
      "        [166],\n",
      "        [133],\n",
      "        [166]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.13.self_attn.q_proj.base_layer.weight.absmax', tensor([ 55,  55,  68,  ..., 101, 220, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.13.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0988, 0.0318, 0.0368,  ..., 0.0754, 0.1369, 0.0744], device='cuda:0')), ('model.model.layers.13.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  48,  51,  50,  57,  57,  56,  56,  53,  57,  56,  56,  50,  51,\n",
      "         53,  53, 125], dtype=torch.uint8)), ('model.model.layers.13.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.13.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.13.self_attn.k_proj.weight', tensor([[185],\n",
      "        [107],\n",
      "        [106],\n",
      "        ...,\n",
      "        [136],\n",
      "        [ 84],\n",
      "        [ 88]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.13.self_attn.k_proj.weight.absmax', tensor([ 49,  45,  46,  ..., 190, 252, 219], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.13.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.13.self_attn.k_proj.weight.nested_absmax', tensor([0.0873, 0.0341, 0.0687,  ..., 0.1224, 0.1293, 0.1390], device='cuda:0')), ('model.model.layers.13.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.13.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  49,  49,  53,  53,  51,  51,  52,  49,  49,  53,  48,  50,  56,\n",
      "         51,  56, 125], dtype=torch.uint8)), ('model.model.layers.13.self_attn.v_proj.base_layer.weight', tensor([[200],\n",
      "        [ 61],\n",
      "        [101],\n",
      "        ...,\n",
      "        [ 22],\n",
      "        [ 75],\n",
      "        [126]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.13.self_attn.v_proj.base_layer.weight.absmax', tensor([ 85, 172,  71,  ...,  70,  63, 218], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.13.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.1119, 0.0353, 0.0228,  ..., 0.0260, 0.0279, 0.0255], device='cuda:0')), ('model.model.layers.13.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  57,  52,  53,  54,  51,  52,  53,  49,  52,  48,  57,  51,  51,\n",
      "         57,  57, 125], dtype=torch.uint8)), ('model.model.layers.13.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.13.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.13.self_attn.o_proj.weight', tensor([[103],\n",
      "        [ 83],\n",
      "        [ 70],\n",
      "        ...,\n",
      "        [ 73],\n",
      "        [108],\n",
      "        [ 65]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.13.self_attn.o_proj.weight.absmax', tensor([ 63, 212, 159,  ..., 232, 190,  44], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.13.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.13.self_attn.o_proj.weight.nested_absmax', tensor([0.0242, 0.0340, 0.0300,  ..., 0.0235, 0.0276, 0.0335], device='cuda:0')), ('model.model.layers.13.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.13.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  56,  55,  57,  57,  56,  54,  55,  48,  51,  51,  57,  53,  56,\n",
      "         52,  51,  53, 125], dtype=torch.uint8)), ('model.model.layers.13.mlp.gate_proj.weight', tensor([[218],\n",
      "        [ 68],\n",
      "        [ 73],\n",
      "        ...,\n",
      "        [182],\n",
      "        [ 22],\n",
      "        [222]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.13.mlp.gate_proj.weight.absmax', tensor([ 37,  17,  48,  ..., 195,  89,  36], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.13.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.13.mlp.gate_proj.weight.nested_absmax', tensor([0.0301, 0.0540, 0.0316,  ..., 0.0340, 0.0413, 0.0242], device='cuda:0')), ('model.model.layers.13.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  53,  50,  52,  50,  49,  57,  53,  52,  50,  55,  52,\n",
      "         49,  55,  55,  54, 125], dtype=torch.uint8)), ('model.model.layers.13.mlp.up_proj.weight', tensor([[ 29],\n",
      "        [ 86],\n",
      "        [ 38],\n",
      "        ...,\n",
      "        [  3],\n",
      "        [ 99],\n",
      "        [124]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.13.mlp.up_proj.weight.absmax', tensor([ 56,  83,  55,  ...,  46, 193, 199], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.13.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.13.mlp.up_proj.weight.nested_absmax', tensor([0.0231, 0.0241, 0.0319,  ..., 0.0304, 0.0451, 0.0304], device='cuda:0')), ('model.model.layers.13.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  54,  57,  50,  49,  54,  49,  48,  53,  51,  52,  49,\n",
      "         57,  49,  49, 125], dtype=torch.uint8)), ('model.model.layers.13.mlp.down_proj.weight', tensor([[ 86],\n",
      "        [104],\n",
      "        [158],\n",
      "        ...,\n",
      "        [ 85],\n",
      "        [152],\n",
      "        [142]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.13.mlp.down_proj.weight.absmax', tensor([ 64, 204, 209,  ..., 224,  56, 198], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.13.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.13.mlp.down_proj.weight.nested_absmax', tensor([0.0273, 0.0254, 0.0288,  ..., 0.0263, 0.0259, 0.0254], device='cuda:0')), ('model.model.layers.13.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  51,  56,  56,  52,  48,  56,  51,  51,  51,  48,  54,\n",
      "         51,  49,  50,  54, 125], dtype=torch.uint8)), ('model.model.layers.13.input_layernorm.weight', tensor([0.4102, 0.3984, 0.3672,  ..., 0.3809, 0.3770, 0.3867], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.13.post_attention_layernorm.weight', tensor([0.2598, 0.2500, 0.2432,  ..., 0.2598, 0.2637, 0.2559], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.14.self_attn.q_proj.base_layer.weight', tensor([[103],\n",
      "        [198],\n",
      "        [ 82],\n",
      "        ...,\n",
      "        [125],\n",
      "        [185],\n",
      "        [145]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.14.self_attn.q_proj.base_layer.weight.absmax', tensor([ 33,  37, 193,  ..., 189,  56, 202], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.14.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0338, 0.0309, 0.0504,  ..., 0.1617, 0.0973, 0.0421], device='cuda:0')), ('model.model.layers.14.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  57,  57,  52,  51,  55,  56,  52,  48,  50,  56,  50,  57,  49,\n",
      "         55, 125], dtype=torch.uint8)), ('model.model.layers.14.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.14.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.14.self_attn.k_proj.weight', tensor([[ 70],\n",
      "        [168],\n",
      "        [ 74],\n",
      "        ...,\n",
      "        [ 87],\n",
      "        [175],\n",
      "        [163]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.14.self_attn.k_proj.weight.absmax', tensor([ 52,  49,  62,  ...,  59,  48, 204], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.14.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.14.self_attn.k_proj.weight.nested_absmax', tensor([0.0727, 0.0361, 0.0336,  ..., 0.1567, 0.0591, 0.0439], device='cuda:0')), ('model.model.layers.14.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.14.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  49,  48,  52,  50,  50,  55,  49,  53,  53,  52,  52,  55,  48,\n",
      "         48,  54, 125], dtype=torch.uint8)), ('model.model.layers.14.self_attn.v_proj.base_layer.weight', tensor([[169],\n",
      "        [  5],\n",
      "        [ 88],\n",
      "        ...,\n",
      "        [166],\n",
      "        [118],\n",
      "        [ 92]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.14.self_attn.v_proj.base_layer.weight.absmax', tensor([167,  49,  48,  ..., 193, 161, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.14.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0152, 0.0276, 0.0286,  ..., 0.0330, 0.0496, 0.0315], device='cuda:0')), ('model.model.layers.14.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  56,  55,  57,  51,  57,  49,  52,  48,  50,  48,  48,  54,  49,\n",
      "         52,  57, 125], dtype=torch.uint8)), ('model.model.layers.14.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.14.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.14.self_attn.o_proj.weight', tensor([[ 74],\n",
      "        [ 20],\n",
      "        [140],\n",
      "        ...,\n",
      "        [ 45],\n",
      "        [198],\n",
      "        [152]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.14.self_attn.o_proj.weight.absmax', tensor([ 50, 190, 194,  ...,  46, 194, 209], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.14.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.14.self_attn.o_proj.weight.nested_absmax', tensor([0.0222, 0.0288, 0.0346,  ..., 0.0288, 0.0288, 0.0242], device='cuda:0')), ('model.model.layers.14.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.14.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  56,  49,  48,  53,  49,  50,  54,  52,  55,  48,  51,  50,  55,\n",
      "         51,  56, 125], dtype=torch.uint8)), ('model.model.layers.14.mlp.gate_proj.weight', tensor([[58],\n",
      "        [73],\n",
      "        [35],\n",
      "        ...,\n",
      "        [70],\n",
      "        [38],\n",
      "        [29]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.14.mlp.gate_proj.weight.absmax', tensor([ 51,  65, 184,  ...,  48, 207,  42], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.14.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.14.mlp.gate_proj.weight.nested_absmax', tensor([0.0443, 0.0414, 0.0224,  ..., 0.0409, 0.0370, 0.0199], device='cuda:0')), ('model.model.layers.14.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  52,  50,  52,  52,  53,  54,  50,  54,  56,  53,  52,\n",
      "         56,  57,  54,  53, 125], dtype=torch.uint8)), ('model.model.layers.14.mlp.up_proj.weight', tensor([[121],\n",
      "        [168],\n",
      "        [104],\n",
      "        ...,\n",
      "        [ 35],\n",
      "        [ 90],\n",
      "        [ 73]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.14.mlp.up_proj.weight.absmax', tensor([242, 192, 149,  ...,  48,  51,  38], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.14.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.14.mlp.up_proj.weight.nested_absmax', tensor([0.0260, 0.0333, 0.0280,  ..., 0.0319, 0.0529, 0.0240], device='cuda:0')), ('model.model.layers.14.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  55,  51,  50,  53,  53,  56,  49,  48,  49,  52,  49,\n",
      "         53,  54,  51,  52, 125], dtype=torch.uint8)), ('model.model.layers.14.mlp.down_proj.weight', tensor([[155],\n",
      "        [133],\n",
      "        [201],\n",
      "        ...,\n",
      "        [  2],\n",
      "        [ 65],\n",
      "        [172]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.14.mlp.down_proj.weight.absmax', tensor([219,  52,  58,  ..., 194,  47,  61], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.14.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.14.mlp.down_proj.weight.nested_absmax', tensor([0.0298, 0.0302, 0.0224,  ..., 0.0302, 0.0302, 0.0298], device='cuda:0')), ('model.model.layers.14.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  51,  56,  56,  57,  54,  51,  52,  48,  49,  51,  49,\n",
      "         55,  53,  57,  54, 125], dtype=torch.uint8)), ('model.model.layers.14.input_layernorm.weight', tensor([0.4102, 0.4199, 0.3711,  ..., 0.4023, 0.3906, 0.3867], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.14.post_attention_layernorm.weight', tensor([0.2715, 0.2598, 0.2598,  ..., 0.2754, 0.2734, 0.2676], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.15.self_attn.q_proj.base_layer.weight', tensor([[ 66],\n",
      "        [104],\n",
      "        [210],\n",
      "        ...,\n",
      "        [ 93],\n",
      "        [167],\n",
      "        [ 69]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.self_attn.q_proj.base_layer.weight.absmax', tensor([ 18,  35,  38,  ..., 215, 192, 198], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.15.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0349, 0.0331, 0.0346,  ..., 0.2058, 0.0788, 0.0740], device='cuda:0')), ('model.model.layers.15.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  57,  56,  50,  50,  53,  57,  54,  54,  48,  57,  53,  57,  50,\n",
      "         52,  52, 125], dtype=torch.uint8)), ('model.model.layers.15.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.15.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.15.self_attn.k_proj.weight', tensor([[197],\n",
      "        [ 95],\n",
      "        [245],\n",
      "        ...,\n",
      "        [105],\n",
      "        [ 55],\n",
      "        [ 87]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.self_attn.k_proj.weight.absmax', tensor([ 35,  25,  10,  ..., 190, 198, 228], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.15.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.15.self_attn.k_proj.weight.nested_absmax', tensor([0.0371, 0.0343, 0.0353,  ..., 0.1485, 0.1026, 0.0958], device='cuda:0')), ('model.model.layers.15.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.15.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  49,  52,  52,  49,  57,  50,  48,  54,  57,  55,  54,  56,  57,\n",
      "         48,  53,  54, 125], dtype=torch.uint8)), ('model.model.layers.15.self_attn.v_proj.base_layer.weight', tensor([[167],\n",
      "        [134],\n",
      "        [158],\n",
      "        ...,\n",
      "        [199],\n",
      "        [178],\n",
      "        [137]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.self_attn.v_proj.base_layer.weight.absmax', tensor([193, 178,  58,  ..., 152, 203, 201], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.15.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0214, 0.0231, 0.0241,  ..., 0.0194, 0.0329, 0.0241], device='cuda:0')), ('model.model.layers.15.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  48,  51,  54,  55,  55,  57,  51,  50,  57,  49,  56,  48,  55,\n",
      "         49,  55,  53, 125], dtype=torch.uint8)), ('model.model.layers.15.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.15.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.15.self_attn.o_proj.weight', tensor([[138],\n",
      "        [ 48],\n",
      "        [ 29],\n",
      "        ...,\n",
      "        [135],\n",
      "        [150],\n",
      "        [ 18]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.self_attn.o_proj.weight.absmax', tensor([ 56, 232, 205,  ..., 209, 178, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.15.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.15.self_attn.o_proj.weight.nested_absmax', tensor([0.0254, 0.0184, 0.0332,  ..., 0.0298, 0.0284, 0.0303], device='cuda:0')), ('model.model.layers.15.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.15.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         51,  57,  53,  48,  54,  49,  50,  57,  57,  50,  48,  52,  56,  50,\n",
      "         54,  51,  53, 125], dtype=torch.uint8)), ('model.model.layers.15.mlp.gate_proj.weight', tensor([[ 94],\n",
      "        [ 52],\n",
      "        [106],\n",
      "        ...,\n",
      "        [  8],\n",
      "        [103],\n",
      "        [105]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.mlp.gate_proj.weight.absmax', tensor([48, 88, 62,  ..., 47, 68, 56], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.15.mlp.gate_proj.weight.nested_absmax', tensor([0.0260, 0.0229, 0.0324,  ..., 0.0387, 0.0495, 0.0373], device='cuda:0')), ('model.model.layers.15.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  54,  55,  55,  53,  51,  51,  56,  54,  52,  57,  55,\n",
      "         52,  57,  55,  54, 125], dtype=torch.uint8)), ('model.model.layers.15.mlp.up_proj.weight', tensor([[ 85],\n",
      "        [199],\n",
      "        [ 34],\n",
      "        ...,\n",
      "        [ 53],\n",
      "        [217],\n",
      "        [ 55]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.mlp.up_proj.weight.absmax', tensor([204, 191,  62,  ...,  60,  47,  59], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.15.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.15.mlp.up_proj.weight.nested_absmax', tensor([0.0316, 0.0209, 0.0243,  ..., 0.0282, 0.0326, 0.0267], device='cuda:0')), ('model.model.layers.15.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  57,  57,  54,  50,  56,  54,  51,  48,  50,  56,  48,\n",
      "         52,  57,  53, 125], dtype=torch.uint8)), ('model.model.layers.15.mlp.down_proj.weight', tensor([[ 83],\n",
      "        [ 20],\n",
      "        [105],\n",
      "        ...,\n",
      "        [196],\n",
      "        [132],\n",
      "        [ 34]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.15.mlp.down_proj.weight.absmax', tensor([ 55, 162, 100,  ..., 204,  52, 178], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.15.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.15.mlp.down_proj.weight.nested_absmax', tensor([0.0359, 0.0241, 0.0266,  ..., 0.0407, 0.0334, 0.0329], device='cuda:0')), ('model.model.layers.15.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  54,  50,  57,  53,  56,  51,  54,  50,  54,  57,  56,\n",
      "         53,  53,  53, 125], dtype=torch.uint8)), ('model.model.layers.15.input_layernorm.weight', tensor([0.4023, 0.3965, 0.3730,  ..., 0.3789, 0.3809, 0.3828], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.15.post_attention_layernorm.weight', tensor([0.2852, 0.2715, 0.2695,  ..., 0.2832, 0.2832, 0.2793], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.16.self_attn.q_proj.base_layer.weight', tensor([[169],\n",
      "        [ 51],\n",
      "        [208],\n",
      "        ...,\n",
      "        [138],\n",
      "        [ 54],\n",
      "        [ 53]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.16.self_attn.q_proj.base_layer.weight.absmax', tensor([ 37,  23,  64,  ...,  62, 195, 205], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.16.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.16.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0268, 0.0276, 0.0259,  ..., 0.0474, 0.2144, 0.0528], device='cuda:0')), ('model.model.layers.16.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.16.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  57,  48,  53,  51,  51,  55,  54,  51,  49,  55,  48,  50,  52,\n",
      "         50,  51, 125], dtype=torch.uint8)), ('model.model.layers.16.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.16.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.16.self_attn.k_proj.weight', tensor([[ 60],\n",
      "        [150],\n",
      "        [124],\n",
      "        ...,\n",
      "        [ 71],\n",
      "        [119],\n",
      "        [ 92]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.16.self_attn.k_proj.weight.absmax', tensor([ 38,  22,  64,  ..., 202, 171, 184], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.16.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.16.self_attn.k_proj.weight.nested_absmax', tensor([0.0290, 0.0334, 0.0254,  ..., 0.0803, 0.1487, 0.1203], device='cuda:0')), ('model.model.layers.16.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.16.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  48,  51,  50,  56,  49,  57,  51,  48,  48,  56,  56,  57,  57,\n",
      "         54,  57, 125], dtype=torch.uint8)), ('model.model.layers.16.self_attn.v_proj.base_layer.weight', tensor([[ 87],\n",
      "        [100],\n",
      "        [ 76],\n",
      "        ...,\n",
      "        [126],\n",
      "        [181],\n",
      "        [129]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.16.self_attn.v_proj.base_layer.weight.absmax', tensor([211, 211, 198,  ..., 201, 201,  94], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.16.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.16.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0404, 0.0395, 0.0688,  ..., 0.0287, 0.0351, 0.0229], device='cuda:0')), ('model.model.layers.16.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.16.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  50,  48,  55,  54,  51,  57,  48,  50,  51,  54,  54,  49,  54,\n",
      "         49,  51,  53, 125], dtype=torch.uint8)), ('model.model.layers.16.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.16.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.16.self_attn.o_proj.weight', tensor([[231],\n",
      "        [ 70],\n",
      "        [ 73],\n",
      "        ...,\n",
      "        [ 87],\n",
      "        [198],\n",
      "        [103]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.16.self_attn.o_proj.weight.absmax', tensor([241, 192, 186,  ..., 210,  59,  56], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.16.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.16.self_attn.o_proj.weight.nested_absmax', tensor([0.0246, 0.0339, 0.0300,  ..., 0.0170, 0.0334, 0.0246], device='cuda:0')), ('model.model.layers.16.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.16.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  49,  51,  48,  49,  51,  52,  55,  51,  49,  53,  51,  49,  49,\n",
      "         52,  51, 125], dtype=torch.uint8)), ('model.model.layers.16.mlp.gate_proj.weight', tensor([[ 51],\n",
      "        [ 45],\n",
      "        [ 62],\n",
      "        ...,\n",
      "        [120],\n",
      "        [113],\n",
      "        [197]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.16.mlp.gate_proj.weight.absmax', tensor([ 61,  37, 165,  ..., 209, 199, 193], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.16.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.16.mlp.gate_proj.weight.nested_absmax', tensor([0.0208, 0.0750, 0.0267,  ..., 0.0301, 0.0238, 0.0506], device='cuda:0')), ('model.model.layers.16.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.16.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  57,  56,  55,  53,  53,  57,  57,  55,  52,  49,  57,\n",
      "         51,  53,  55, 125], dtype=torch.uint8)), ('model.model.layers.16.mlp.up_proj.weight', tensor([[ 26],\n",
      "        [141],\n",
      "        [111],\n",
      "        ...,\n",
      "        [125],\n",
      "        [184],\n",
      "        [ 51]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.16.mlp.up_proj.weight.absmax', tensor([189,  56, 194,  ..., 202,  63, 209], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.16.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.16.mlp.up_proj.weight.nested_absmax', tensor([0.0346, 0.0561, 0.0287,  ..., 0.0321, 0.0258, 0.0336], device='cuda:0')), ('model.model.layers.16.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.16.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  57,  51,  53,  56,  57,  53,  54,  50,  49,  55,  55,\n",
      "         54,  53,  56, 125], dtype=torch.uint8)), ('model.model.layers.16.mlp.down_proj.weight', tensor([[ 70],\n",
      "        [188],\n",
      "        [ 53],\n",
      "        ...,\n",
      "        [135],\n",
      "        [125],\n",
      "        [132]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.16.mlp.down_proj.weight.absmax', tensor([211,  31,  43,  ...,  48,  20,  59], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.16.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.16.mlp.down_proj.weight.nested_absmax', tensor([0.0233, 0.0316, 0.0291,  ..., 0.0291, 0.0306, 0.0213], device='cuda:0')), ('model.model.layers.16.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.16.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  53,  49,  56,  52,  51,  50,  49,  52,  48,  51,  53,\n",
      "         48,  51,  52, 125], dtype=torch.uint8)), ('model.model.layers.16.input_layernorm.weight', tensor([0.4062, 0.4102, 0.3828,  ..., 0.3828, 0.3984, 0.3965], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.16.post_attention_layernorm.weight', tensor([0.3008, 0.2871, 0.2910,  ..., 0.3008, 0.3047, 0.2969], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.17.self_attn.q_proj.base_layer.weight', tensor([[ 18],\n",
      "        [215],\n",
      "        [150],\n",
      "        ...,\n",
      "        [200],\n",
      "        [219],\n",
      "        [126]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.17.self_attn.q_proj.base_layer.weight.absmax', tensor([ 13,  14,  16,  ..., 186, 192, 195], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.17.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.17.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0398, 0.0409, 0.0393,  ..., 0.0730, 0.3650, 0.1765], device='cuda:0')), ('model.model.layers.17.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.17.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  56,  56,  53,  53,  55,  49,  57,  56,  54,  52,  51,  54,  56,\n",
      "         52,  52, 125], dtype=torch.uint8)), ('model.model.layers.17.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.17.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.17.self_attn.k_proj.weight', tensor([[ 40],\n",
      "        [169],\n",
      "        [194],\n",
      "        ...,\n",
      "        [182],\n",
      "        [ 58],\n",
      "        [166]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.17.self_attn.k_proj.weight.absmax', tensor([ 14,  13,   1,  ..., 196, 201, 211], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.17.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.17.self_attn.k_proj.weight.nested_absmax', tensor([0.0404, 0.0392, 0.0433,  ..., 0.0738, 0.1616, 0.1529], device='cuda:0')), ('model.model.layers.17.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.17.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         54,  48,  48,  51,  53,  48,  49,  50,  54,  54,  50,  52,  49,  48,\n",
      "         55,  51,  54, 125], dtype=torch.uint8)), ('model.model.layers.17.self_attn.v_proj.base_layer.weight', tensor([[ 73],\n",
      "        [156],\n",
      "        [ 86],\n",
      "        ...,\n",
      "        [157],\n",
      "        [185],\n",
      "        [169]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.17.self_attn.v_proj.base_layer.weight.absmax', tensor([162, 192,  62,  ...,  97,  41,  63], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.17.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.17.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0825, 0.0205, 0.0288,  ..., 0.0239, 0.0288, 0.0259], device='cuda:0')), ('model.model.layers.17.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.17.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  49,  57,  56,  48,  50,  53,  49,  54,  54,  57,  56,  56,  51,\n",
      "         55,  51, 125], dtype=torch.uint8)), ('model.model.layers.17.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.17.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.17.self_attn.o_proj.weight', tensor([[101],\n",
      "        [148],\n",
      "        [ 71],\n",
      "        ...,\n",
      "        [ 39],\n",
      "        [ 47],\n",
      "        [ 44]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.17.self_attn.o_proj.weight.absmax', tensor([ 35,  42, 228,  ...,  60,  88,  41], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.17.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.17.self_attn.o_proj.weight.nested_absmax', tensor([0.0194, 0.0348, 0.0348,  ..., 0.0304, 0.0289, 0.0265], device='cuda:0')), ('model.model.layers.17.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.17.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  49,  51,  55,  52,  55,  48,  50,  48,  48,  54,  53,  55,  56,\n",
      "         52,  52,  53, 125], dtype=torch.uint8)), ('model.model.layers.17.mlp.gate_proj.weight', tensor([[ 28],\n",
      "        [187],\n",
      "        [170],\n",
      "        ...,\n",
      "        [102],\n",
      "        [ 68],\n",
      "        [151]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.17.mlp.gate_proj.weight.absmax', tensor([203,  59,  56,  ...,  76,  59, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.17.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.17.mlp.gate_proj.weight.nested_absmax', tensor([0.0233, 0.0262, 0.0369,  ..., 0.0194, 0.0282, 0.0399], device='cuda:0')), ('model.model.layers.17.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.17.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  52,  56,  50,  52,  51,  56,  55,  49,  51,  51,  49,\n",
      "         50,  49,  53, 125], dtype=torch.uint8)), ('model.model.layers.17.mlp.up_proj.weight', tensor([[ 82],\n",
      "        [ 83],\n",
      "        [ 54],\n",
      "        ...,\n",
      "        [ 97],\n",
      "        [184],\n",
      "        [ 19]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.17.mlp.up_proj.weight.absmax', tensor([ 63,  55, 225,  ..., 193, 213,  51], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.17.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.17.mlp.up_proj.weight.nested_absmax', tensor([0.0294, 0.0274, 0.0245,  ..., 0.0245, 0.0245, 0.0230], device='cuda:0')), ('model.model.layers.17.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.17.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  55,  53,  50,  55,  53,  54,  54,  50,  53,  52,  49,\n",
      "         51,  56,  57,  53, 125], dtype=torch.uint8)), ('model.model.layers.17.mlp.down_proj.weight', tensor([[195],\n",
      "        [157],\n",
      "        [245],\n",
      "        ...,\n",
      "        [104],\n",
      "        [ 81],\n",
      "        [ 88]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.17.mlp.down_proj.weight.absmax', tensor([197, 170, 205,  ..., 166,  54, 181], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.17.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.17.mlp.down_proj.weight.nested_absmax', tensor([0.0293, 0.0249, 0.0239,  ..., 0.0288, 0.0269, 0.0332], device='cuda:0')), ('model.model.layers.17.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.17.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  51,  52,  49,  48,  55,  49,  48,  54,  57,  50,  52,\n",
      "         48,  53,  55, 125], dtype=torch.uint8)), ('model.model.layers.17.input_layernorm.weight', tensor([0.4180, 0.4238, 0.3984,  ..., 0.4160, 0.4199, 0.3984], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.17.post_attention_layernorm.weight', tensor([0.3184, 0.3086, 0.3086,  ..., 0.3184, 0.3203, 0.3125], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.18.self_attn.q_proj.base_layer.weight', tensor([[152],\n",
      "        [125],\n",
      "        [  7],\n",
      "        ...,\n",
      "        [ 67],\n",
      "        [203],\n",
      "        [209]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.18.self_attn.q_proj.base_layer.weight.absmax', tensor([ 60,  31,  12,  ..., 170, 178, 171], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.18.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.18.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0277, 0.0274, 0.0317,  ..., 0.0612, 0.0798, 0.3542], device='cuda:0')), ('model.model.layers.18.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.18.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  56,  57,  55,  48,  49,  54,  52,  48,  54,  48,  53,  57,\n",
      "         50,  54,  53, 125], dtype=torch.uint8)), ('model.model.layers.18.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.18.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.18.self_attn.k_proj.weight', tensor([[ 62],\n",
      "        [ 82],\n",
      "        [ 54],\n",
      "        ...,\n",
      "        [161],\n",
      "        [156],\n",
      "        [ 83]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.18.self_attn.k_proj.weight.absmax', tensor([ 13, 200,  28,  ..., 192, 174, 184], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.18.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.18.self_attn.k_proj.weight.nested_absmax', tensor([0.0262, 0.0269, 0.0334,  ..., 0.1198, 0.1198, 0.1950], device='cuda:0')), ('model.model.layers.18.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.18.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  56,  57,  48,  50,  54,  57,  53,  55,  55,  53,  48,  51,  50,\n",
      "         48,  52, 125], dtype=torch.uint8)), ('model.model.layers.18.self_attn.v_proj.base_layer.weight', tensor([[ 44],\n",
      "        [ 53],\n",
      "        [120],\n",
      "        ...,\n",
      "        [ 44],\n",
      "        [149],\n",
      "        [ 71]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.18.self_attn.v_proj.base_layer.weight.absmax', tensor([163, 163, 201,  ...,  55, 175,  33], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.18.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.18.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0489, 0.0426, 0.0426,  ..., 0.0201, 0.0177, 0.0191], device='cuda:0')), ('model.model.layers.18.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.18.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  51,  56,  54,  56,  54,  57,  56,  49,  55,  57,  55,  50,  49,\n",
      "         56,  51, 125], dtype=torch.uint8)), ('model.model.layers.18.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.18.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.18.self_attn.o_proj.weight', tensor([[ 58],\n",
      "        [ 84],\n",
      "        [206],\n",
      "        ...,\n",
      "        [ 64],\n",
      "        [211],\n",
      "        [178]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.18.self_attn.o_proj.weight.absmax', tensor([195, 197,  60,  ...,  43,  54,  38], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.18.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.18.self_attn.o_proj.weight.nested_absmax', tensor([0.0200, 0.0381, 0.0327,  ..., 0.0332, 0.0259, 0.0268], device='cuda:0')), ('model.model.layers.18.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.18.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  50,  57,  56,  57,  48,  49,  53,  53,  55,  57,  50,  50,  51,\n",
      "         54,  51, 125], dtype=torch.uint8)), ('model.model.layers.18.mlp.gate_proj.weight', tensor([[166],\n",
      "        [166],\n",
      "        [131],\n",
      "        ...,\n",
      "        [ 23],\n",
      "        [105],\n",
      "        [104]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.18.mlp.gate_proj.weight.absmax', tensor([164,  47,  65,  ..., 197, 207,  87], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.18.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.18.mlp.gate_proj.weight.nested_absmax', tensor([0.0326, 0.0389, 0.0326,  ..., 0.0243, 0.0282, 0.0462], device='cuda:0')), ('model.model.layers.18.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.18.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  57,  54,  55,  50,  49,  48,  55,  51,  57,  56,  53,\n",
      "         49, 125], dtype=torch.uint8)), ('model.model.layers.18.mlp.up_proj.weight', tensor([[170],\n",
      "        [235],\n",
      "        [179],\n",
      "        ...,\n",
      "        [ 98],\n",
      "        [164],\n",
      "        [187]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.18.mlp.up_proj.weight.absmax', tensor([181,  57, 205,  ...,  63,  43, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.18.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.18.mlp.up_proj.weight.nested_absmax', tensor([0.0325, 0.0300, 0.0354,  ..., 0.0305, 0.0305, 0.0276], device='cuda:0')), ('model.model.layers.18.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.18.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  54,  50,  55,  53,  52,  57,  54,  49,  56,  52,  56,\n",
      "         50,  53,  57, 125], dtype=torch.uint8)), ('model.model.layers.18.mlp.down_proj.weight', tensor([[233],\n",
      "        [ 31],\n",
      "        [111],\n",
      "        ...,\n",
      "        [106],\n",
      "        [214],\n",
      "        [187]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.18.mlp.down_proj.weight.absmax', tensor([ 39,  58, 194,  ...,  60, 181, 205], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.18.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.18.mlp.down_proj.weight.nested_absmax', tensor([0.0211, 0.0294, 0.0206,  ..., 0.0343, 0.0362, 0.0406], device='cuda:0')), ('model.model.layers.18.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.18.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  50,  54,  48,  53,  57,  51,  54,  50,  50,  57,  50,\n",
      "         50,  57, 125], dtype=torch.uint8)), ('model.model.layers.18.input_layernorm.weight', tensor([0.4453, 0.4414, 0.4219,  ..., 0.4258, 0.4375, 0.4219], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.18.post_attention_layernorm.weight', tensor([0.3379, 0.3262, 0.3262,  ..., 0.3340, 0.3379, 0.3320], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.19.self_attn.q_proj.base_layer.weight', tensor([[122],\n",
      "        [ 82],\n",
      "        [154],\n",
      "        ...,\n",
      "        [200],\n",
      "        [214],\n",
      "        [ 21]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.self_attn.q_proj.base_layer.weight.absmax', tensor([ 44,  58,  44,  ..., 210, 183, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.19.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.19.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0568, 0.0273, 0.0314,  ..., 0.0489, 0.2496, 0.0675], device='cuda:0')), ('model.model.layers.19.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.19.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  48,  49,  50,  54,  49,  48,  49,  51,  55,  52,  54,  50,\n",
      "         54,  49,  54, 125], dtype=torch.uint8)), ('model.model.layers.19.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.19.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.19.self_attn.k_proj.weight', tensor([[219],\n",
      "        [117],\n",
      "        [ 53],\n",
      "        ...,\n",
      "        [201],\n",
      "        [100],\n",
      "        [155]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.self_attn.k_proj.weight.absmax', tensor([ 36,  35,  31,  ..., 183,  69, 205], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.19.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.19.self_attn.k_proj.weight.nested_absmax', tensor([0.0509, 0.0286, 0.0367,  ..., 0.0758, 0.1794, 0.1149], device='cuda:0')), ('model.model.layers.19.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.19.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  57,  52,  51,  48,  54,  56,  52,  52,  52,  55,  50,  56,\n",
      "         56,  53, 125], dtype=torch.uint8)), ('model.model.layers.19.self_attn.v_proj.base_layer.weight', tensor([[134],\n",
      "        [118],\n",
      "        [153],\n",
      "        ...,\n",
      "        [105],\n",
      "        [146],\n",
      "        [ 75]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.self_attn.v_proj.base_layer.weight.absmax', tensor([221, 211, 167,  ...,  54,  74, 118], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.19.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.19.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0295, 0.0290, 0.0251,  ..., 0.0256, 0.0261, 0.0315], device='cuda:0')), ('model.model.layers.19.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.19.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  52,  49,  57,  56,  49,  56,  56,  57,  51,  48,  55,  52,  57,\n",
      "         56,  57, 125], dtype=torch.uint8)), ('model.model.layers.19.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.19.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.19.self_attn.o_proj.weight', tensor([[176],\n",
      "        [195],\n",
      "        [109],\n",
      "        ...,\n",
      "        [196],\n",
      "        [195],\n",
      "        [ 90]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.self_attn.o_proj.weight.absmax', tensor([204, 197,  61,  ..., 226, 208, 161], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.19.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.19.self_attn.o_proj.weight.nested_absmax', tensor([0.0434, 0.0273, 0.0283,  ..., 0.0332, 0.0219, 0.0278], device='cuda:0')), ('model.model.layers.19.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.19.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  51,  52,  57,  51,  53,  50,  52,  49,  57,  51,  55,  54,  51,\n",
      "         55,  51, 125], dtype=torch.uint8)), ('model.model.layers.19.mlp.gate_proj.weight', tensor([[ 72],\n",
      "        [169],\n",
      "        [163],\n",
      "        ...,\n",
      "        [151],\n",
      "        [ 87],\n",
      "        [152]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.mlp.gate_proj.weight.absmax', tensor([ 84, 195,  75,  ...,  53,  53,  74], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.19.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.19.mlp.gate_proj.weight.nested_absmax', tensor([0.1549, 0.0206, 0.2603,  ..., 0.0597, 0.0284, 0.0333], device='cuda:0')), ('model.model.layers.19.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.19.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  48,  50,  48,  55,  50,  57,  52,  53,  50,  51,  55,  49,\n",
      "         53,  57,  55, 125], dtype=torch.uint8)), ('model.model.layers.19.mlp.up_proj.weight', tensor([[131],\n",
      "        [186],\n",
      "        [ 57],\n",
      "        ...,\n",
      "        [  6],\n",
      "        [103],\n",
      "        [121]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.mlp.up_proj.weight.absmax', tensor([ 75,  75,  71,  ..., 209,  63, 191], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.19.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.19.mlp.up_proj.weight.nested_absmax', tensor([0.4348, 0.0295, 0.0393,  ..., 0.0315, 0.0241, 0.0261], device='cuda:0')), ('model.model.layers.19.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.19.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  54,  50,  56,  56,  49,  50,  52,  57,  49,  56,  57,\n",
      "         51,  55,  55, 125], dtype=torch.uint8)), ('model.model.layers.19.mlp.down_proj.weight', tensor([[ 99],\n",
      "        [151],\n",
      "        [118],\n",
      "        ...,\n",
      "        [236],\n",
      "        [103],\n",
      "        [153]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.mlp.down_proj.weight.absmax', tensor([52, 48, 59,  ..., 55, 87, 69], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.19.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.19.mlp.down_proj.weight.nested_absmax', tensor([0.0293, 0.0376, 0.0239,  ..., 0.0254, 0.0332, 0.0288], device='cuda:0')), ('model.model.layers.19.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.19.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  51,  52,  52,  49,  49,  56,  51,  53,  54,  55,  48,\n",
      "         52,  55,  49, 125], dtype=torch.uint8)), ('model.model.layers.19.input_layernorm.weight', tensor([0.4473, 0.4531, 0.4316,  ..., 0.4219, 0.4277, 0.4316], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.19.post_attention_layernorm.weight', tensor([0.3516, 0.3379, 0.3398,  ..., 0.3477, 0.3477, 0.3438], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.20.self_attn.q_proj.base_layer.weight', tensor([[133],\n",
      "        [226],\n",
      "        [231],\n",
      "        ...,\n",
      "        [159],\n",
      "        [181],\n",
      "        [103]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.self_attn.q_proj.base_layer.weight.absmax', tensor([ 16,  13,   6,  ...,  81, 167, 169], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.20.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.20.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0435, 0.0401, 0.0425,  ..., 0.0576, 0.2280, 0.2260], device='cuda:0')), ('model.model.layers.20.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.20.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  49,  57,  51,  57,  56,  55,  48,  55,  49,  53,  49,  52,\n",
      "         49,  51, 125], dtype=torch.uint8)), ('model.model.layers.20.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.20.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.20.self_attn.k_proj.weight', tensor([[ 91],\n",
      "        [178],\n",
      "        [ 69],\n",
      "        ...,\n",
      "        [135],\n",
      "        [ 27],\n",
      "        [ 61]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.self_attn.k_proj.weight.absmax', tensor([  8,  11,   9,  ..., 176, 213, 196], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.20.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.20.self_attn.k_proj.weight.nested_absmax', tensor([0.0444, 0.0426, 0.0435,  ..., 0.0527, 0.0727, 0.1001], device='cuda:0')), ('model.model.layers.20.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.20.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  56,  49,  52,  50,  54,  54,  50,  48,  52,  56,  51,  51,  57,\n",
      "         56,  52,  52, 125], dtype=torch.uint8)), ('model.model.layers.20.self_attn.v_proj.base_layer.weight', tensor([[100],\n",
      "        [184],\n",
      "        [ 39],\n",
      "        ...,\n",
      "        [172],\n",
      "        [ 85],\n",
      "        [ 64]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.self_attn.v_proj.base_layer.weight.absmax', tensor([ 63, 196, 220,  ..., 192,  36,  52], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.20.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.20.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0336, 0.0224, 0.0249,  ..., 0.0173, 0.0244, 0.0214], device='cuda:0')), ('model.model.layers.20.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.20.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  52,  57,  54,  51,  55,  49,  48,  48,  49,  48,  48,  53,  49,\n",
      "         55,  51, 125], dtype=torch.uint8)), ('model.model.layers.20.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.20.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.20.self_attn.o_proj.weight', tensor([[152],\n",
      "        [214],\n",
      "        [153],\n",
      "        ...,\n",
      "        [133],\n",
      "        [ 43],\n",
      "        [ 42]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.self_attn.o_proj.weight.absmax', tensor([209,  58, 170,  ...,  77,  71,  80], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.20.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.20.self_attn.o_proj.weight.nested_absmax', tensor([0.0255, 0.0216, 0.0313,  ..., 0.0230, 0.0299, 0.0299], device='cuda:0')), ('model.model.layers.20.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.20.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  52,  51,  51,  54,  52,  49,  53,  56,  50,  55,  50,  55,  52,\n",
      "         51,  50, 125], dtype=torch.uint8)), ('model.model.layers.20.mlp.gate_proj.weight', tensor([[118],\n",
      "        [ 57],\n",
      "        [123],\n",
      "        ...,\n",
      "        [  1],\n",
      "        [136],\n",
      "        [ 38]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.mlp.gate_proj.weight.absmax', tensor([118, 248,  59,  ..., 175,  57,  87], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.20.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.20.mlp.gate_proj.weight.nested_absmax', tensor([0.0222, 0.0398, 0.0505,  ..., 0.0247, 0.0559, 0.0295], device='cuda:0')), ('model.model.layers.20.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.20.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  48,  53,  52,  51,  57,  56,  57,  57,  56,  54,  49,  56,\n",
      "         49,  50,  54, 125], dtype=torch.uint8)), ('model.model.layers.20.mlp.up_proj.weight', tensor([[  7],\n",
      "        [120],\n",
      "        [103],\n",
      "        ...,\n",
      "        [ 66],\n",
      "        [229],\n",
      "        [ 53]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.mlp.up_proj.weight.absmax', tensor([216,  55, 205,  ..., 159, 224, 193], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.20.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.20.mlp.up_proj.weight.nested_absmax', tensor([0.0227, 0.0271, 0.0286,  ..., 0.0286, 0.0623, 0.0208], device='cuda:0')), ('model.model.layers.20.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.20.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  53,  56,  57,  54,  54,  51,  52,  49,  54,  49,  52,\n",
      "         55,  50,  51, 125], dtype=torch.uint8)), ('model.model.layers.20.mlp.down_proj.weight', tensor([[109],\n",
      "        [101],\n",
      "        [150],\n",
      "        ...,\n",
      "        [149],\n",
      "        [247],\n",
      "        [ 86]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.mlp.down_proj.weight.absmax', tensor([89, 59, 49,  ..., 49, 48, 52], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.20.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.20.mlp.down_proj.weight.nested_absmax', tensor([0.0318, 0.0216, 0.0279,  ..., 0.0264, 0.0255, 0.0201], device='cuda:0')), ('model.model.layers.20.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.20.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  50,  56,  48,  54,  54,  49,  55,  54,  49,  55,  54,\n",
      "         48,  55,  49, 125], dtype=torch.uint8)), ('model.model.layers.20.input_layernorm.weight', tensor([0.4473, 0.4609, 0.4316,  ..., 0.4297, 0.4316, 0.4434], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.20.post_attention_layernorm.weight', tensor([0.3613, 0.3535, 0.3496,  ..., 0.3633, 0.3574, 0.3535], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.21.self_attn.q_proj.base_layer.weight', tensor([[ 37],\n",
      "        [219],\n",
      "        [ 81],\n",
      "        ...,\n",
      "        [ 74],\n",
      "        [ 86],\n",
      "        [146]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.21.self_attn.q_proj.base_layer.weight.absmax', tensor([ 21,  20,  29,  ..., 193, 192, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.21.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.21.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0383, 0.0373, 0.0303,  ..., 0.0720, 0.4030, 0.1247], device='cuda:0')), ('model.model.layers.21.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.21.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  53,  57,  54,  57,  54,  56,  57,  48,  52,  49,  51,  55,  54,\n",
      "         49,  49,  52, 125], dtype=torch.uint8)), ('model.model.layers.21.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.21.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.21.self_attn.k_proj.weight', tensor([[172],\n",
      "        [157],\n",
      "        [230],\n",
      "        ...,\n",
      "        [149],\n",
      "        [199],\n",
      "        [ 79]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.21.self_attn.k_proj.weight.absmax', tensor([ 37,  42,  39,  ..., 192, 175, 179], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.21.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.21.self_attn.k_proj.weight.nested_absmax', tensor([0.0622, 0.0288, 0.0322,  ..., 0.1056, 0.1886, 0.1837], device='cuda:0')), ('model.model.layers.21.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.21.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  54,  52,  57,  48,  49,  51,  56,  49,  55,  51,  49,  48,  51,\n",
      "         51,  51, 125], dtype=torch.uint8)), ('model.model.layers.21.self_attn.v_proj.base_layer.weight', tensor([[122],\n",
      "        [101],\n",
      "        [ 83],\n",
      "        ...,\n",
      "        [ 36],\n",
      "        [136],\n",
      "        [ 71]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.21.self_attn.v_proj.base_layer.weight.absmax', tensor([ 52,  43, 204,  ...,  81,  65, 208], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.21.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.21.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0201, 0.0182, 0.0182,  ..., 0.0265, 0.0211, 0.0216], device='cuda:0')), ('model.model.layers.21.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.21.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  54,  50,  56,  51,  48,  48,  54,  54,  54,  56,  48,  57,  48,\n",
      "         56,  50, 125], dtype=torch.uint8)), ('model.model.layers.21.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.21.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.21.self_attn.o_proj.weight', tensor([[ 71],\n",
      "        [166],\n",
      "        [124],\n",
      "        ...,\n",
      "        [ 36],\n",
      "        [190],\n",
      "        [ 56]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.21.self_attn.o_proj.weight.absmax', tensor([191,  48,  62,  ...,  87,  56, 169], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.21.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.21.self_attn.o_proj.weight.nested_absmax', tensor([0.0234, 0.0293, 0.0244,  ..., 0.0303, 0.0215, 0.0430], device='cuda:0')), ('model.model.layers.21.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.21.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  53,  52,  48,  54,  55,  55,  55,  52,  49,  49,  54,  57,  57,\n",
      "         50,  57,  53, 125], dtype=torch.uint8)), ('model.model.layers.21.mlp.gate_proj.weight', tensor([[ 98],\n",
      "        [178],\n",
      "        [165],\n",
      "        ...,\n",
      "        [  7],\n",
      "        [ 35],\n",
      "        [121]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.21.mlp.gate_proj.weight.absmax', tensor([ 51, 155,  61,  ...,  79, 229,  49], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.21.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.21.mlp.gate_proj.weight.nested_absmax', tensor([0.0404, 0.1492, 0.0316,  ..., 0.0350, 0.0272, 0.0369], device='cuda:0')), ('model.model.layers.21.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.21.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  48,  57,  52,  55,  50,  48,  48,  53,  48,  54,  57,  50,\n",
      "         53,  53,  56, 125], dtype=torch.uint8)), ('model.model.layers.21.mlp.up_proj.weight', tensor([[209],\n",
      "        [ 55],\n",
      "        [201],\n",
      "        ...,\n",
      "        [ 21],\n",
      "        [133],\n",
      "        [ 81]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.21.mlp.up_proj.weight.absmax', tensor([ 60, 192, 177,  ...,  54,  40,  63], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.21.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.21.mlp.up_proj.weight.nested_absmax', tensor([0.0247, 0.0486, 0.0281,  ..., 0.0218, 0.0242, 0.0267], device='cuda:0')), ('model.model.layers.21.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.21.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  53,  52,  52,  54,  57,  57,  49,  54,  50,  50,  52,\n",
      "         52,  56, 125], dtype=torch.uint8)), ('model.model.layers.21.mlp.down_proj.weight', tensor([[171],\n",
      "        [104],\n",
      "        [157],\n",
      "        ...,\n",
      "        [ 59],\n",
      "        [116],\n",
      "        [179]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.21.mlp.down_proj.weight.absmax', tensor([ 62,  47,  42,  ...,  46, 200, 178], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.21.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.21.mlp.down_proj.weight.nested_absmax', tensor([0.0201, 0.0309, 0.0275,  ..., 0.0265, 0.0221, 0.0284], device='cuda:0')), ('model.model.layers.21.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.21.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  50,  53,  49,  50,  51,  53,  54,  57,  51,  54,  57,\n",
      "         51,  49,  54, 125], dtype=torch.uint8)), ('model.model.layers.21.input_layernorm.weight', tensor([0.4727, 0.4805, 0.4629,  ..., 0.4512, 0.4668, 0.4727], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.21.post_attention_layernorm.weight', tensor([0.3711, 0.3652, 0.3613,  ..., 0.3770, 0.3691, 0.3711], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.22.self_attn.q_proj.base_layer.weight', tensor([[ 52],\n",
      "        [ 84],\n",
      "        [ 26],\n",
      "        ...,\n",
      "        [152],\n",
      "        [162],\n",
      "        [154]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.22.self_attn.q_proj.base_layer.weight.absmax', tensor([184,  54, 195,  ..., 190, 165, 186], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.22.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.22.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0319, 0.0383, 0.0305,  ..., 0.2004, 0.0725, 0.4016], device='cuda:0')), ('model.model.layers.22.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.22.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  52,  50,  54,  55,  48,  50,  50,  51,  49,  49,  54,  56,\n",
      "         55,  53, 125], dtype=torch.uint8)), ('model.model.layers.22.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.22.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.22.self_attn.k_proj.weight', tensor([[ 55],\n",
      "        [ 86],\n",
      "        [103],\n",
      "        ...,\n",
      "        [241],\n",
      "        [107],\n",
      "        [ 57]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.22.self_attn.k_proj.weight.absmax', tensor([ 57,  48, 216,  ..., 176,  93, 195], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.22.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.22.self_attn.k_proj.weight.nested_absmax', tensor([0.0445, 0.0318, 0.0352,  ..., 0.1167, 0.0777, 0.1480], device='cuda:0')), ('model.model.layers.22.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.22.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  56,  48,  55,  56,  50,  54,  54,  54,  56,  48,  50,  52,  48,\n",
      "         54,  51, 125], dtype=torch.uint8)), ('model.model.layers.22.self_attn.v_proj.base_layer.weight', tensor([[ 56],\n",
      "        [106],\n",
      "        [251],\n",
      "        ...,\n",
      "        [137],\n",
      "        [153],\n",
      "        [125]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.22.self_attn.v_proj.base_layer.weight.absmax', tensor([100, 225,  62,  ...,  87, 198,  62], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.22.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.22.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0342, 0.0356, 0.0376,  ..., 0.0307, 0.0400, 0.0366], device='cuda:0')), ('model.model.layers.22.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.22.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  54,  51,  57,  57,  55,  54,  48,  57,  57,  49,  51,  51,  52,\n",
      "         57,  49,  53, 125], dtype=torch.uint8)), ('model.model.layers.22.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.22.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.22.self_attn.o_proj.weight', tensor([[202],\n",
      "        [ 53],\n",
      "        [185],\n",
      "        ...,\n",
      "        [136],\n",
      "        [169],\n",
      "        [170]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.22.self_attn.o_proj.weight.absmax', tensor([209, 194, 190,  ..., 229,  59, 240], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.22.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.22.self_attn.o_proj.weight.nested_absmax', tensor([0.0318, 0.0313, 0.0240,  ..., 0.0348, 0.0323, 0.0284], device='cuda:0')), ('model.model.layers.22.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.22.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  53,  51,  50,  54,  49,  57,  57,  51,  56,  50,  53,  52,  51,\n",
      "         53,  54,  52, 125], dtype=torch.uint8)), ('model.model.layers.22.mlp.gate_proj.weight', tensor([[ 68],\n",
      "        [196],\n",
      "        [ 67],\n",
      "        ...,\n",
      "        [102],\n",
      "        [117],\n",
      "        [203]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.22.mlp.gate_proj.weight.absmax', tensor([220, 191, 255,  ..., 197, 206, 188], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.22.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.22.mlp.gate_proj.weight.nested_absmax', tensor([0.0249, 0.0313, 0.0483,  ..., 0.0405, 0.0391, 0.0234], device='cuda:0')), ('model.model.layers.22.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.22.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  50,  54,  54,  50,  51,  56,  48,  57,  51,  51,  55,\n",
      "         54,  49,  54, 125], dtype=torch.uint8)), ('model.model.layers.22.mlp.up_proj.weight', tensor([[237],\n",
      "        [132],\n",
      "        [222],\n",
      "        ...,\n",
      "        [ 83],\n",
      "        [ 49],\n",
      "        [ 10]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.22.mlp.up_proj.weight.absmax', tensor([ 43, 165,  62,  ..., 205, 205, 207], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.22.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.22.mlp.up_proj.weight.nested_absmax', tensor([0.0209, 0.0267, 0.0316,  ..., 0.0218, 0.0252, 0.0248], device='cuda:0')), ('model.model.layers.22.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.22.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  53,  48,  53,  51,  56,  50,  52,  52,  56,  52,  51,\n",
      "         52,  56,  51, 125], dtype=torch.uint8)), ('model.model.layers.22.mlp.down_proj.weight', tensor([[165],\n",
      "        [169],\n",
      "        [ 60],\n",
      "        ...,\n",
      "        [218],\n",
      "        [ 54],\n",
      "        [ 58]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.22.mlp.down_proj.weight.absmax', tensor([214,  65,  55,  ...,  70,  56,  55], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.22.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.22.mlp.down_proj.weight.nested_absmax', tensor([0.0265, 0.0324, 0.0343,  ..., 0.0348, 0.0353, 0.0372], device='cuda:0')), ('model.model.layers.22.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.22.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  50,  51,  57,  51,  49,  52,  55,  54,  52,  55,  51,\n",
      "         56,  48,  56, 125], dtype=torch.uint8)), ('model.model.layers.22.input_layernorm.weight', tensor([0.4805, 0.4805, 0.4707,  ..., 0.4629, 0.4824, 0.4824], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.22.post_attention_layernorm.weight', tensor([0.3809, 0.3789, 0.3809,  ..., 0.3887, 0.3828, 0.3867], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.23.self_attn.q_proj.base_layer.weight', tensor([[131],\n",
      "        [126],\n",
      "        [155],\n",
      "        ...,\n",
      "        [165],\n",
      "        [ 73],\n",
      "        [214]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.23.self_attn.q_proj.base_layer.weight.absmax', tensor([ 26,  31,  15,  ..., 166, 180, 178], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.23.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.23.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0390, 0.0371, 0.0382,  ..., 0.1546, 0.0970, 0.2025], device='cuda:0')), ('model.model.layers.23.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.23.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  50,  54,  55,  53,  54,  57,  48,  48,  53,  52,  56,  57,\n",
      "         51,  53, 125], dtype=torch.uint8)), ('model.model.layers.23.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.23.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.23.self_attn.k_proj.weight', tensor([[105],\n",
      "        [138],\n",
      "        [189],\n",
      "        ...,\n",
      "        [208],\n",
      "        [232],\n",
      "        [117]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.23.self_attn.k_proj.weight.absmax', tensor([  7,  22,  13,  ..., 166,  98, 177], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.23.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.23.self_attn.k_proj.weight.nested_absmax', tensor([0.0363, 0.0366, 0.0377,  ..., 0.2119, 0.1367, 0.2275], device='cuda:0')), ('model.model.layers.23.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.23.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  54,  51,  52,  48,  52,  56,  49,  54,  51,  56,  57,  48,\n",
      "         56,  52, 125], dtype=torch.uint8)), ('model.model.layers.23.self_attn.v_proj.base_layer.weight', tensor([[141],\n",
      "        [103],\n",
      "        [138],\n",
      "        ...,\n",
      "        [151],\n",
      "        [ 26],\n",
      "        [161]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.23.self_attn.v_proj.base_layer.weight.absmax', tensor([221,  37,  30,  ...,  34, 198,  60], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.23.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.23.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0252, 0.0200, 0.0223,  ..., 0.0208, 0.0223, 0.0262], device='cuda:0')), ('model.model.layers.23.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.23.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  56,  53,  52,  57,  54,  53,  53,  56,  50,  52,  56,  57,  57,\n",
      "         54,  55, 125], dtype=torch.uint8)), ('model.model.layers.23.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.23.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.23.self_attn.o_proj.weight', tensor([[102],\n",
      "        [ 58],\n",
      "        [144],\n",
      "        ...,\n",
      "        [132],\n",
      "        [150],\n",
      "        [114]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.23.self_attn.o_proj.weight.absmax', tensor([190, 102,  40,  ..., 168,  49, 221], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.23.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.23.self_attn.o_proj.weight.nested_absmax', tensor([0.0250, 0.0230, 0.0279,  ..., 0.0279, 0.0250, 0.0279], device='cuda:0')), ('model.model.layers.23.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.23.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  55,  55,  54,  56,  55,  56,  54,  53,  52,  57,  53,  54,  56,\n",
      "         49,  55,  54, 125], dtype=torch.uint8)), ('model.model.layers.23.mlp.gate_proj.weight', tensor([[ 93],\n",
      "        [117],\n",
      "        [119],\n",
      "        ...,\n",
      "        [107],\n",
      "        [ 86],\n",
      "        [133]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.23.mlp.gate_proj.weight.absmax', tensor([ 93, 172,  55,  ...,  36, 176, 223], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.23.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.23.mlp.gate_proj.weight.nested_absmax', tensor([0.0293, 0.0302, 0.0346,  ..., 0.0297, 0.0278, 0.0283], device='cuda:0')), ('model.model.layers.23.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.23.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  51,  48,  53,  52,  51,  57,  51,  50,  51,  49,  56,\n",
      "         54,  56,  55,  52, 125], dtype=torch.uint8)), ('model.model.layers.23.mlp.up_proj.weight', tensor([[218],\n",
      "        [211],\n",
      "        [101],\n",
      "        ...,\n",
      "        [199],\n",
      "        [ 23],\n",
      "        [ 60]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.23.mlp.up_proj.weight.absmax', tensor([199, 192, 166,  ..., 189,  70,  63], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.23.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.23.mlp.up_proj.weight.nested_absmax', tensor([0.0362, 0.0270, 0.0314,  ..., 0.0309, 0.0289, 0.0314], device='cuda:0')), ('model.model.layers.23.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.23.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  55,  52,  57,  56,  48,  54,  49,  57,  53,  52,  57,\n",
      "         55,  53,  49, 125], dtype=torch.uint8)), ('model.model.layers.23.mlp.down_proj.weight', tensor([[122],\n",
      "        [ 45],\n",
      "        [ 89],\n",
      "        ...,\n",
      "        [102],\n",
      "        [ 37],\n",
      "        [165]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.23.mlp.down_proj.weight.absmax', tensor([ 42,  61, 204,  ...,  46,  75,  49], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.23.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.23.mlp.down_proj.weight.nested_absmax', tensor([0.0204, 0.0248, 0.0296,  ..., 0.0345, 0.0243, 0.0277], device='cuda:0')), ('model.model.layers.23.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.23.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  53,  49,  48,  52,  49,  57,  48,  52,  48,  57,  49,\n",
      "         56,  51,  53, 125], dtype=torch.uint8)), ('model.model.layers.23.input_layernorm.weight', tensor([0.5039, 0.5156, 0.5039,  ..., 0.5000, 0.5156, 0.5195], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.23.post_attention_layernorm.weight', tensor([0.3984, 0.3906, 0.3906,  ..., 0.3945, 0.3984, 0.4023], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.24.self_attn.q_proj.base_layer.weight', tensor([[181],\n",
      "        [154],\n",
      "        [ 37],\n",
      "        ...,\n",
      "        [138],\n",
      "        [101],\n",
      "        [207]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.24.self_attn.q_proj.base_layer.weight.absmax', tensor([203,  63,  68,  ..., 194, 214, 198], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.24.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.24.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0389, 0.0359, 0.0335,  ..., 0.2400, 0.1424, 0.0501], device='cuda:0')), ('model.model.layers.24.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.24.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  52,  56,  57,  50,  48,  48,  51,  53,  51,  54,  50,  50,  52,\n",
      "         51,  54,  53, 125], dtype=torch.uint8)), ('model.model.layers.24.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.24.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.24.self_attn.k_proj.weight', tensor([[199],\n",
      "        [ 88],\n",
      "        [ 52],\n",
      "        ...,\n",
      "        [180],\n",
      "        [184],\n",
      "        [111]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.24.self_attn.k_proj.weight.absmax', tensor([108,  49,  51,  ..., 203, 214, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.24.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.24.self_attn.k_proj.weight.nested_absmax', tensor([0.0312, 0.0409, 0.0287,  ..., 0.1361, 0.0526, 0.0678], device='cuda:0')), ('model.model.layers.24.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.24.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  53,  50,  55,  50,  55,  53,  48,  53,  53,  54,  52,  54,  56,\n",
      "         57,  54,  52, 125], dtype=torch.uint8)), ('model.model.layers.24.self_attn.v_proj.base_layer.weight', tensor([[195],\n",
      "        [103],\n",
      "        [ 34],\n",
      "        ...,\n",
      "        [231],\n",
      "        [107],\n",
      "        [ 69]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.24.self_attn.v_proj.base_layer.weight.absmax', tensor([196, 230,  92,  ..., 149,  95, 215], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.24.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.24.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0299, 0.0260, 0.0499,  ..., 0.0289, 0.0206, 0.0294], device='cuda:0')), ('model.model.layers.24.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.24.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  56,  50,  48,  53,  52,  57,  52,  56,  56,  48,  54,  55,  54,\n",
      "         50,  55, 125], dtype=torch.uint8)), ('model.model.layers.24.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.24.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.24.self_attn.o_proj.weight', tensor([[ 91],\n",
      "        [209],\n",
      "        [158],\n",
      "        ...,\n",
      "        [136],\n",
      "        [120],\n",
      "        [ 58]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.24.self_attn.o_proj.weight.absmax', tensor([ 90, 234, 200,  ..., 194, 192, 208], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.24.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.24.self_attn.o_proj.weight.nested_absmax', tensor([0.0257, 0.0306, 0.0335,  ..., 0.0360, 0.0257, 0.0286], device='cuda:0')), ('model.model.layers.24.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.24.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  55,  48,  51,  56,  53,  49,  48,  52,  52,  49,  55,  56,  48,\n",
      "         48,  57, 125], dtype=torch.uint8)), ('model.model.layers.24.mlp.gate_proj.weight', tensor([[ 36],\n",
      "        [164],\n",
      "        [134],\n",
      "        ...,\n",
      "        [ 74],\n",
      "        [174],\n",
      "        [ 48]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.24.mlp.gate_proj.weight.absmax', tensor([167, 192,  23,  ...,  66, 200,  59], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.24.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.24.mlp.gate_proj.weight.nested_absmax', tensor([0.0252, 0.0267, 0.0272,  ..., 0.0438, 0.0287, 0.0389], device='cuda:0')), ('model.model.layers.24.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.24.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  52,  50,  51,  56,  50,  53,  51,  50,  51,  53,  56,\n",
      "         49,  54,  57,  54, 125], dtype=torch.uint8)), ('model.model.layers.24.mlp.up_proj.weight', tensor([[134],\n",
      "        [199],\n",
      "        [ 94],\n",
      "        ...,\n",
      "        [ 67],\n",
      "        [140],\n",
      "        [219]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.24.mlp.up_proj.weight.absmax', tensor([171, 194,  94,  ...,  66, 192, 204], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.24.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.24.mlp.up_proj.weight.nested_absmax', tensor([0.0210, 0.0278, 0.0200,  ..., 0.0297, 0.0351, 0.0478], device='cuda:0')), ('model.model.layers.24.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.24.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  56,  57,  53,  55,  56,  49,  54,  57,  53,  56,  52,\n",
      "         50,  55,  52, 125], dtype=torch.uint8)), ('model.model.layers.24.mlp.down_proj.weight', tensor([[101],\n",
      "        [ 44],\n",
      "        [140],\n",
      "        ...,\n",
      "        [143],\n",
      "        [106],\n",
      "        [ 81]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.24.mlp.down_proj.weight.absmax', tensor([204, 193, 162,  ..., 190,  64,  93], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.24.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.24.mlp.down_proj.weight.nested_absmax', tensor([0.0368, 0.0280, 0.0251,  ..., 0.0236, 0.0319, 0.0314], device='cuda:0')), ('model.model.layers.24.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.24.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  54,  53,  50,  55,  50,  53,  49,  51,  48,  51,  49,\n",
      "         57,  53,  57,  53, 125], dtype=torch.uint8)), ('model.model.layers.24.input_layernorm.weight', tensor([0.4902, 0.5156, 0.5039,  ..., 0.4824, 0.5078, 0.5000], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.24.post_attention_layernorm.weight', tensor([0.4082, 0.4043, 0.4043,  ..., 0.4082, 0.4121, 0.4043], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.25.self_attn.q_proj.base_layer.weight', tensor([[146],\n",
      "        [ 21],\n",
      "        [194],\n",
      "        ...,\n",
      "        [163],\n",
      "        [135],\n",
      "        [162]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.25.self_attn.q_proj.base_layer.weight.absmax', tensor([ 19,  14,   2,  ..., 200, 212, 191], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.25.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.25.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0404, 0.0422, 0.0415,  ..., 0.4030, 0.1364, 0.1003], device='cuda:0')), ('model.model.layers.25.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.25.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  53,  57,  55,  56,  51,  51,  49,  55,  49,  52,  56,  54,  56,\n",
      "         53,  52,  54, 125], dtype=torch.uint8)), ('model.model.layers.25.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.25.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.25.self_attn.k_proj.weight', tensor([[ 50],\n",
      "        [101],\n",
      "        [187],\n",
      "        ...,\n",
      "        [192],\n",
      "        [ 76],\n",
      "        [132]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.25.self_attn.k_proj.weight.absmax', tensor([ 26,   7,  17,  ..., 161, 192, 220], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.25.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.25.self_attn.k_proj.weight.nested_absmax', tensor([0.0411, 0.0394, 0.0387,  ..., 0.1362, 0.0645, 0.0537], device='cuda:0')), ('model.model.layers.25.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.25.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  54,  49,  52,  57,  55,  55,  51,  50,  57,  57,  54,  57,  52,\n",
      "         48,  54, 125], dtype=torch.uint8)), ('model.model.layers.25.self_attn.v_proj.base_layer.weight', tensor([[170],\n",
      "        [133],\n",
      "        [117],\n",
      "        ...,\n",
      "        [200],\n",
      "        [ 24],\n",
      "        [188]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.25.self_attn.v_proj.base_layer.weight.absmax', tensor([ 61,  60, 198,  ...,  53, 214, 191], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.25.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.25.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0369, 0.0296, 0.0389,  ..., 0.0291, 0.0252, 0.0335], device='cuda:0')), ('model.model.layers.25.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.25.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  57,  57,  54,  53,  53,  49,  57,  52,  53,  56,  48,  53,  53,\n",
      "         52,  57,  54, 125], dtype=torch.uint8)), ('model.model.layers.25.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.25.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.25.self_attn.o_proj.weight', tensor([[170],\n",
      "        [133],\n",
      "        [104],\n",
      "        ...,\n",
      "        [ 45],\n",
      "        [225],\n",
      "        [ 52]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.25.self_attn.o_proj.weight.absmax', tensor([217,  49,  83,  ..., 182,  56, 207], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.25.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.25.self_attn.o_proj.weight.nested_absmax', tensor([0.0324, 0.0510, 0.0329,  ..., 0.0315, 0.0271, 0.0320], device='cuda:0')), ('model.model.layers.25.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.25.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  57,  48,  57,  57,  48,  48,  57,  52,  56,  52,  48,  53,  50,\n",
      "         54,  54, 125], dtype=torch.uint8)), ('model.model.layers.25.mlp.gate_proj.weight', tensor([[ 96],\n",
      "        [241],\n",
      "        [193],\n",
      "        ...,\n",
      "        [180],\n",
      "        [174],\n",
      "        [134]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.25.mlp.gate_proj.weight.absmax', tensor([ 57,  84, 190,  ..., 189,  52, 205], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.25.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.25.mlp.gate_proj.weight.nested_absmax', tensor([0.0457, 0.0330, 0.0296,  ..., 0.0232, 0.0286, 0.0305], device='cuda:0')), ('model.model.layers.25.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.25.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  52,  57,  50,  48,  57,  49,  50,  54,  56,  51,  48,\n",
      "         49,  48,  49, 125], dtype=torch.uint8)), ('model.model.layers.25.mlp.up_proj.weight', tensor([[ 38],\n",
      "        [172],\n",
      "        [176],\n",
      "        ...,\n",
      "        [185],\n",
      "        [125],\n",
      "        [157]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.25.mlp.up_proj.weight.absmax', tensor([226, 159, 186,  ..., 222, 137,  43], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.25.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.25.mlp.up_proj.weight.nested_absmax', tensor([0.0227, 0.0237, 0.0320,  ..., 0.0266, 0.0339, 0.0222], device='cuda:0')), ('model.model.layers.25.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.25.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  48,  56,  55,  53,  57,  54,  56,  57,  51,  51,  49,\n",
      "         48,  53,  53, 125], dtype=torch.uint8)), ('model.model.layers.25.mlp.down_proj.weight', tensor([[229],\n",
      "        [178],\n",
      "        [154],\n",
      "        ...,\n",
      "        [218],\n",
      "        [ 97],\n",
      "        [102]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.25.mlp.down_proj.weight.absmax', tensor([ 61,  59,  61,  ...,  41, 212, 174], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.25.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.25.mlp.down_proj.weight.nested_absmax', tensor([0.0322, 0.0292, 0.0312,  ..., 0.0385, 0.0253, 0.0214], device='cuda:0')), ('model.model.layers.25.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.25.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  55,  57,  49,  57,  52,  52,  49,  48,  49,  52,  53,  50,\n",
      "         56,  50,  55,  53, 125], dtype=torch.uint8)), ('model.model.layers.25.input_layernorm.weight', tensor([0.5391, 0.5508, 0.5391,  ..., 0.5508, 0.5547, 0.5430], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.25.post_attention_layernorm.weight', tensor([0.4121, 0.4121, 0.4141,  ..., 0.4219, 0.4219, 0.4199], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.26.self_attn.q_proj.base_layer.weight', tensor([[230],\n",
      "        [ 85],\n",
      "        [180],\n",
      "        ...,\n",
      "        [168],\n",
      "        [ 38],\n",
      "        [ 17]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.26.self_attn.q_proj.base_layer.weight.absmax', tensor([ 37, 188,  25,  ..., 162, 101, 179], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.26.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.26.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0251, 0.0250, 0.0240,  ..., 0.0665, 0.1413, 0.2672], device='cuda:0')), ('model.model.layers.26.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.26.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  53,  48,  51,  52,  53,  54,  54,  54,  55,  48,  54,  53,  54,\n",
      "         50,  48,  52, 125], dtype=torch.uint8)), ('model.model.layers.26.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.26.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.26.self_attn.k_proj.weight', tensor([[203],\n",
      "        [ 86],\n",
      "        [164],\n",
      "        ...,\n",
      "        [151],\n",
      "        [ 23],\n",
      "        [ 70]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.26.self_attn.k_proj.weight.absmax', tensor([ 62,  51,  44,  ..., 202, 175, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.26.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.26.self_attn.k_proj.weight.nested_absmax', tensor([0.0232, 0.0266, 0.0246,  ..., 0.0647, 0.0681, 0.1643], device='cuda:0')), ('model.model.layers.26.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.26.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  53,  51,  57,  54,  55,  54,  53,  52,  55,  48,  53,  48,  52,\n",
      "         55,  54, 125], dtype=torch.uint8)), ('model.model.layers.26.self_attn.v_proj.base_layer.weight', tensor([[243],\n",
      "        [ 54],\n",
      "        [180],\n",
      "        ...,\n",
      "        [ 77],\n",
      "        [ 89],\n",
      "        [123]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.26.self_attn.v_proj.base_layer.weight.absmax', tensor([200, 166, 197,  ...,  54, 214, 108], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.26.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.26.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0290, 0.0353, 0.0324,  ..., 0.0299, 0.0275, 0.0348], device='cuda:0')), ('model.model.layers.26.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.26.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  48,  54,  51,  51,  48,  52,  54,  55,  55,  54,  48,  53,  54,\n",
      "         50,  57, 125], dtype=torch.uint8)), ('model.model.layers.26.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.26.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.26.self_attn.o_proj.weight', tensor([[168],\n",
      "        [ 21],\n",
      "        [ 93],\n",
      "        ...,\n",
      "        [ 40],\n",
      "        [210],\n",
      "        [210]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.26.self_attn.o_proj.weight.absmax', tensor([ 62,  55, 190,  ...,  73,  62,  61], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.26.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.26.self_attn.o_proj.weight.nested_absmax', tensor([0.0356, 0.0615, 0.0385,  ..., 0.0405, 0.0585, 0.0600], device='cuda:0')), ('model.model.layers.26.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.26.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  57,  56,  54,  49,  49,  56,  56,  57,  55,  55,  57,  53,  54,\n",
      "         55,  55, 125], dtype=torch.uint8)), ('model.model.layers.26.mlp.gate_proj.weight', tensor([[166],\n",
      "        [183],\n",
      "        [ 44],\n",
      "        ...,\n",
      "        [ 75],\n",
      "        [130],\n",
      "        [102]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.26.mlp.gate_proj.weight.absmax', tensor([ 60, 206,  49,  ...,  41,  45,  49], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.26.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.26.mlp.gate_proj.weight.nested_absmax', tensor([0.0246, 0.0539, 0.0242,  ..., 0.0310, 0.0286, 0.0256], device='cuda:0')), ('model.model.layers.26.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.26.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  53,  50,  55,  49,  50,  51,  56,  57,  56,  50,  54,\n",
      "         55,  55,  52,  54, 125], dtype=torch.uint8)), ('model.model.layers.26.mlp.up_proj.weight', tensor([[ 78],\n",
      "        [105],\n",
      "        [156],\n",
      "        ...,\n",
      "        [ 89],\n",
      "        [ 52],\n",
      "        [165]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.26.mlp.up_proj.weight.absmax', tensor([198,  67, 209,  ..., 170, 191,  54], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.26.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.26.mlp.up_proj.weight.nested_absmax', tensor([0.0318, 0.0274, 0.0269,  ..., 0.0240, 0.0269, 0.0352], device='cuda:0')), ('model.model.layers.26.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.26.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  50,  54,  54,  49,  49,  50,  56,  48,  52,  52,  49,\n",
      "         50,  56,  52, 125], dtype=torch.uint8)), ('model.model.layers.26.mlp.down_proj.weight', tensor([[ 84],\n",
      "        [ 72],\n",
      "        [125],\n",
      "        ...,\n",
      "        [ 44],\n",
      "        [219],\n",
      "        [203]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.26.mlp.down_proj.weight.absmax', tensor([ 50,  91,  37,  ...,  63, 161,  40], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.26.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.26.mlp.down_proj.weight.nested_absmax', tensor([0.0222, 0.0290, 0.0227,  ..., 0.0325, 0.0212, 0.0290], device='cuda:0')), ('model.model.layers.26.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.26.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  49,  49,  49,  49,  56,  49,  55,  48,  54,  49,  57,\n",
      "         48,  49,  49, 125], dtype=torch.uint8)), ('model.model.layers.26.input_layernorm.weight', tensor([0.5117, 0.5312, 0.5273,  ..., 0.5117, 0.5352, 0.5273], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.26.post_attention_layernorm.weight', tensor([0.4316, 0.4258, 0.4297,  ..., 0.4395, 0.4375, 0.4355], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.27.self_attn.q_proj.base_layer.weight', tensor([[ 44],\n",
      "        [137],\n",
      "        [ 19],\n",
      "        ...,\n",
      "        [ 22],\n",
      "        [ 76],\n",
      "        [181]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.self_attn.q_proj.base_layer.weight.absmax', tensor([ 49,  46, 218,  ...,  72, 150,  77], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.27.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.27.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0237, 0.0358, 0.0310,  ..., 0.0935, 0.1208, 0.2604], device='cuda:0')), ('model.model.layers.27.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.27.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  57,  49,  48,  53,  54,  53,  50,  56,  54,  56,  55,  52,\n",
      "         55,  55, 125], dtype=torch.uint8)), ('model.model.layers.27.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.27.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.27.self_attn.k_proj.weight', tensor([[102],\n",
      "        [154],\n",
      "        [  5],\n",
      "        ...,\n",
      "        [220],\n",
      "        [115],\n",
      "        [ 41]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.self_attn.k_proj.weight.absmax', tensor([ 52,   7,  47,  ...,  72, 198, 197], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.27.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.27.self_attn.k_proj.weight.nested_absmax', tensor([0.0233, 0.0366, 0.0317,  ..., 0.0375, 0.0327, 0.0737], device='cuda:0')), ('model.model.layers.27.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.27.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  56,  49,  54,  57,  55,  51,  55,  52,  53,  56,  50,  50,  57,\n",
      "         48,  54,  53, 125], dtype=torch.uint8)), ('model.model.layers.27.self_attn.v_proj.base_layer.weight', tensor([[198],\n",
      "        [ 22],\n",
      "        [ 45],\n",
      "        ...,\n",
      "        [ 56],\n",
      "        [ 83],\n",
      "        [ 58]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.self_attn.v_proj.base_layer.weight.absmax', tensor([186,  57,  76,  ..., 235, 195, 212], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.27.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.27.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0415, 0.0244, 0.0244,  ..., 0.0556, 0.0459, 0.0405], device='cuda:0')), ('model.model.layers.27.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.27.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  48,  56,  50,  50,  49,  55,  54,  48,  51,  57,  50,  49,  56,\n",
      "         57, 125], dtype=torch.uint8)), ('model.model.layers.27.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.27.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.27.self_attn.o_proj.weight', tensor([[133],\n",
      "        [130],\n",
      "        [ 84],\n",
      "        ...,\n",
      "        [137],\n",
      "        [117],\n",
      "        [115]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.self_attn.o_proj.weight.absmax', tensor([ 51, 196,  63,  ..., 194, 207, 192], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.27.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.27.self_attn.o_proj.weight.nested_absmax', tensor([0.0369, 0.0261, 0.0418,  ..., 0.0242, 0.0354, 0.0467], device='cuda:0')), ('model.model.layers.27.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.27.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  48,  53,  49,  55,  52,  53,  52,  55,  52,  51,  51,  56,  53,\n",
      "         51,  49,  53, 125], dtype=torch.uint8)), ('model.model.layers.27.mlp.gate_proj.weight', tensor([[235],\n",
      "        [ 67],\n",
      "        [140],\n",
      "        ...,\n",
      "        [200],\n",
      "        [149],\n",
      "        [ 54]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.mlp.gate_proj.weight.absmax', tensor([75, 16, 55,  ..., 39, 97, 51], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.27.mlp.gate_proj.weight.nested_absmax', tensor([0.0217, 0.0354, 0.0256,  ..., 0.0335, 0.0422, 0.0252], device='cuda:0')), ('model.model.layers.27.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.27.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  53,  48,  51,  55,  50,  57,  48,  55,  53,  49,  57,\n",
      "         51,  52,  48,  53, 125], dtype=torch.uint8)), ('model.model.layers.27.mlp.up_proj.weight', tensor([[108],\n",
      "        [231],\n",
      "        [ 38],\n",
      "        ...,\n",
      "        [ 35],\n",
      "        [149],\n",
      "        [ 39]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.mlp.up_proj.weight.absmax', tensor([ 54, 206, 193,  ...,  63,  51,  24], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.27.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.27.mlp.up_proj.weight.nested_absmax', tensor([0.0252, 0.0252, 0.0394,  ..., 0.0228, 0.0267, 0.0242], device='cuda:0')), ('model.model.layers.27.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.27.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  53,  50,  48,  55,  51,  50,  54,  55,  49,  48,  50,\n",
      "         50,  52,  49,  53, 125], dtype=torch.uint8)), ('model.model.layers.27.mlp.down_proj.weight', tensor([[146],\n",
      "        [224],\n",
      "        [161],\n",
      "        ...,\n",
      "        [147],\n",
      "        [ 93],\n",
      "        [104]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.27.mlp.down_proj.weight.absmax', tensor([ 54,  53,  64,  ...,  41,  41, 191], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.27.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.27.mlp.down_proj.weight.nested_absmax', tensor([0.0326, 0.0287, 0.0277,  ..., 0.0400, 0.0360, 0.0282], device='cuda:0')), ('model.model.layers.27.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.27.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  52,  50,  50,  55,  57,  49,  48,  54,  51,  55,  56,\n",
      "         53,  53,  53, 125], dtype=torch.uint8)), ('model.model.layers.27.input_layernorm.weight', tensor([0.5391, 0.5430, 0.5430,  ..., 0.5430, 0.5469, 0.5508], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.27.post_attention_layernorm.weight', tensor([0.4492, 0.4414, 0.4395,  ..., 0.4473, 0.4531, 0.4434], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.28.self_attn.q_proj.base_layer.weight', tensor([[114],\n",
      "        [169],\n",
      "        [ 52],\n",
      "        ...,\n",
      "        [ 40],\n",
      "        [158],\n",
      "        [178]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.28.self_attn.q_proj.base_layer.weight.absmax', tensor([ 75,  76,  66,  ..., 174, 159, 176], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.28.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.28.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.3736, 0.2701, 0.2662,  ..., 0.1197, 0.0645, 0.1793], device='cuda:0')), ('model.model.layers.28.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.28.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  54,  48,  56,  52,  50,  48,  56,  49,  57,  48,  52,  52,  49,\n",
      "         49,  51, 125], dtype=torch.uint8)), ('model.model.layers.28.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.28.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.28.self_attn.k_proj.weight', tensor([[103],\n",
      "        [184],\n",
      "        [218],\n",
      "        ...,\n",
      "        [124],\n",
      "        [135],\n",
      "        [ 69]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.28.self_attn.k_proj.weight.absmax', tensor([ 29,   9,  22,  ..., 204, 237, 223], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.28.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.28.self_attn.k_proj.weight.nested_absmax', tensor([0.0354, 0.0359, 0.0346,  ..., 0.1301, 0.0501, 0.0525], device='cuda:0')), ('model.model.layers.28.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.28.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  54,  51,  55,  56,  50,  56,  50,  54,  48,  54,  54,  48,  49,\n",
      "         55,  49,  53, 125], dtype=torch.uint8)), ('model.model.layers.28.self_attn.v_proj.base_layer.weight', tensor([[ 57],\n",
      "        [205],\n",
      "        [113],\n",
      "        ...,\n",
      "        [144],\n",
      "        [ 29],\n",
      "        [136]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.28.self_attn.v_proj.base_layer.weight.absmax', tensor([207, 219, 191,  ..., 165, 204, 160], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.28.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.28.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0346, 0.0404, 0.0390,  ..., 0.0306, 0.0438, 0.0624], device='cuda:0')), ('model.model.layers.28.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.28.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  50,  51,  53,  55,  57,  56,  54,  53,  54,  57,  52,  48,  52,\n",
      "         54, 125], dtype=torch.uint8)), ('model.model.layers.28.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.28.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.28.self_attn.o_proj.weight', tensor([[155],\n",
      "        [ 19],\n",
      "        [ 76],\n",
      "        ...,\n",
      "        [103],\n",
      "        [195],\n",
      "        [120]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.28.self_attn.o_proj.weight.absmax', tensor([206, 191, 160,  ...,  79, 218, 195], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.28.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.28.self_attn.o_proj.weight.nested_absmax', tensor([0.0448, 0.0297, 0.0292,  ..., 0.0345, 0.0262, 0.0414], device='cuda:0')), ('model.model.layers.28.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.28.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  49,  56,  56,  50,  48,  48,  50,  53,  48,  50,  54,  55,  57,\n",
      "         56,  50,  53, 125], dtype=torch.uint8)), ('model.model.layers.28.mlp.gate_proj.weight', tensor([[105],\n",
      "        [111],\n",
      "        [ 98],\n",
      "        ...,\n",
      "        [149],\n",
      "        [149],\n",
      "        [ 75]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.28.mlp.gate_proj.weight.absmax', tensor([ 21, 180,  37,  ...,  49,  62, 194], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.28.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.28.mlp.gate_proj.weight.nested_absmax', tensor([0.0250, 0.0718, 0.0411,  ..., 0.0679, 0.0259, 0.0342], device='cuda:0')), ('model.model.layers.28.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.28.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  50,  49,  48,  53,  57,  55,  49,  53,  55,  52,  55,\n",
      "         56,  51,  51, 125], dtype=torch.uint8)), ('model.model.layers.28.mlp.up_proj.weight', tensor([[217],\n",
      "        [231],\n",
      "        [ 46],\n",
      "        ...,\n",
      "        [131],\n",
      "        [230],\n",
      "        [134]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.28.mlp.up_proj.weight.absmax', tensor([ 37, 195,  45,  ...,  45,  18, 213], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.28.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.28.mlp.up_proj.weight.nested_absmax', tensor([0.0291, 0.0320, 0.0311,  ..., 0.0276, 0.0237, 0.0247], device='cuda:0')), ('model.model.layers.28.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.28.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  48,  49,  51,  56,  52,  53,  54,  50,  50,  53,  51,\n",
      "         57,  53,  50, 125], dtype=torch.uint8)), ('model.model.layers.28.mlp.down_proj.weight', tensor([[158],\n",
      "        [ 91],\n",
      "        [136],\n",
      "        ...,\n",
      "        [165],\n",
      "        [108],\n",
      "        [212]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.28.mlp.down_proj.weight.absmax', tensor([202,  47,  52,  ...,  52, 190, 205], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.28.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.28.mlp.down_proj.weight.nested_absmax', tensor([0.0303, 0.0273, 0.0220,  ..., 0.0254, 0.0269, 0.0249], device='cuda:0')), ('model.model.layers.28.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.28.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  56,  56,  50,  57,  54,  53,  54,  48,  57,  52,  51,  49,\n",
      "         50,  54,  55, 125], dtype=torch.uint8)), ('model.model.layers.28.input_layernorm.weight', tensor([0.5547, 0.5625, 0.5547,  ..., 0.5391, 0.5664, 0.5547], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.28.post_attention_layernorm.weight', tensor([0.4570, 0.4590, 0.4512,  ..., 0.4648, 0.4570, 0.4531], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.29.self_attn.q_proj.base_layer.weight', tensor([[154],\n",
      "        [ 71],\n",
      "        [ 40],\n",
      "        ...,\n",
      "        [101],\n",
      "        [202],\n",
      "        [145]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.self_attn.q_proj.base_layer.weight.absmax', tensor([34, 46, 39,  ..., 80, 85, 94], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.29.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.0427, 0.0259, 0.0335,  ..., 0.0754, 0.1828, 0.2873], device='cuda:0')), ('model.model.layers.29.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.29.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  52,  52,  55,  53,  50,  56,  56,  56,  51,  56,  49,  52,  56,\n",
      "         49,  50, 125], dtype=torch.uint8)), ('model.model.layers.29.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.29.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.29.self_attn.k_proj.weight', tensor([[245],\n",
      "        [ 59],\n",
      "        [155],\n",
      "        ...,\n",
      "        [198],\n",
      "        [ 48],\n",
      "        [ 56]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.self_attn.k_proj.weight.absmax', tensor([ 26,  44,  23,  ..., 161, 195, 168], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.29.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.29.self_attn.k_proj.weight.nested_absmax', tensor([0.0265, 0.0274, 0.0322,  ..., 0.0465, 0.0499, 0.1456], device='cuda:0')), ('model.model.layers.29.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.29.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  52,  53,  55,  54,  52,  52,  57,  48,  57,  54,  50,  48,  50,\n",
      "         56,  53, 125], dtype=torch.uint8)), ('model.model.layers.29.self_attn.v_proj.base_layer.weight', tensor([[168],\n",
      "        [213],\n",
      "        [  3],\n",
      "        ...,\n",
      "        [154],\n",
      "        [104],\n",
      "        [ 76]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.self_attn.v_proj.base_layer.weight.absmax', tensor([197, 142, 101,  ...,  63,  30, 212], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.29.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.29.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0303, 0.0220, 0.0293,  ..., 0.0240, 0.0264, 0.0235], device='cuda:0')), ('model.model.layers.29.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.29.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  50,  55,  48,  50,  49,  52,  55,  53,  49,  51,  54,  50,  56,\n",
      "         48,  48,  54, 125], dtype=torch.uint8)), ('model.model.layers.29.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.29.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.29.self_attn.o_proj.weight', tensor([[110],\n",
      "        [230],\n",
      "        [ 92],\n",
      "        ...,\n",
      "        [ 87],\n",
      "        [137],\n",
      "        [178]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.self_attn.o_proj.weight.absmax', tensor([ 62,  69, 194,  ..., 190,  64,  62], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.29.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.29.self_attn.o_proj.weight.nested_absmax', tensor([0.0392, 0.0372, 0.0338,  ..., 0.0304, 0.0285, 0.0441], device='cuda:0')), ('model.model.layers.29.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.29.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  50,  53,  57,  53,  51,  53,  52,  54,  49,  54,  54,  52,  50,\n",
      "        125], dtype=torch.uint8)), ('model.model.layers.29.mlp.gate_proj.weight', tensor([[173],\n",
      "        [ 72],\n",
      "        [184],\n",
      "        ...,\n",
      "        [ 41],\n",
      "        [ 75],\n",
      "        [ 69]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.mlp.gate_proj.weight.absmax', tensor([ 50, 189,  51,  ..., 192,  63, 211], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.29.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.29.mlp.gate_proj.weight.nested_absmax', tensor([0.0332, 0.0298, 0.0225,  ..., 0.0288, 0.0193, 0.0274], device='cuda:0')), ('model.model.layers.29.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.29.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  49,  50,  54,  49,  49,  54,  48,  53,  50,  50,  54,  57,\n",
      "         57,  51,  53,  54, 125], dtype=torch.uint8)), ('model.model.layers.29.mlp.up_proj.weight', tensor([[119],\n",
      "        [130],\n",
      "        [102],\n",
      "        ...,\n",
      "        [ 23],\n",
      "        [ 70],\n",
      "        [184]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.mlp.up_proj.weight.absmax', tensor([196, 149,  74,  ...,  57,  43, 169], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.29.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.29.mlp.up_proj.weight.nested_absmax', tensor([0.0292, 0.0297, 0.0331,  ..., 0.0258, 0.0336, 0.0350], device='cuda:0')), ('model.model.layers.29.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.29.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  52,  51,  50,  53,  51,  48,  57,  57,  57,  49,  56,\n",
      "         51,  54,  53,  53, 125], dtype=torch.uint8)), ('model.model.layers.29.mlp.down_proj.weight', tensor([[233],\n",
      "        [115],\n",
      "        [197],\n",
      "        ...,\n",
      "        [215],\n",
      "        [148],\n",
      "        [ 87]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.29.mlp.down_proj.weight.absmax', tensor([194, 201,  84,  ...,  93,  56,  59], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.29.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.29.mlp.down_proj.weight.nested_absmax', tensor([0.0328, 0.0362, 0.0279,  ..., 0.0338, 0.0260, 0.0226], device='cuda:0')), ('model.model.layers.29.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.29.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  50,  49,  55,  53,  56,  57,  49,  57,  57,  53,  52,\n",
      "         51, 125], dtype=torch.uint8)), ('model.model.layers.29.input_layernorm.weight', tensor([0.5234, 0.5352, 0.5234,  ..., 0.5195, 0.5312, 0.5469], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.29.post_attention_layernorm.weight', tensor([0.4668, 0.4668, 0.4629,  ..., 0.4688, 0.4688, 0.4668], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.30.self_attn.q_proj.base_layer.weight', tensor([[114],\n",
      "        [119],\n",
      "        [176],\n",
      "        ...,\n",
      "        [137],\n",
      "        [ 70],\n",
      "        [118]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.self_attn.q_proj.base_layer.weight.absmax', tensor([ 65,  68,  81,  ..., 174,  81, 101], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.30.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.30.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.2334, 0.2920, 0.1670,  ..., 0.1778, 0.1504, 0.2022], device='cuda:0')), ('model.model.layers.30.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.30.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  53,  54,  49,  57,  55,  55,  54,  50,  52,  56,  57,  51,  49,\n",
      "         56,  56,  53, 125], dtype=torch.uint8)), ('model.model.layers.30.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.30.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.30.self_attn.k_proj.weight', tensor([[194],\n",
      "        [116],\n",
      "        [197],\n",
      "        ...,\n",
      "        [217],\n",
      "        [184],\n",
      "        [163]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.self_attn.k_proj.weight.absmax', tensor([54, 44, 53,  ..., 63, 90, 63], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.30.self_attn.k_proj.weight.nested_absmax', tensor([0.0296, 0.0259, 0.0291,  ..., 0.0820, 0.0341, 0.0800], device='cuda:0')), ('model.model.layers.30.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.30.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  53,  54,  57,  55,  52,  55,  48,  57,  48,  51,  51,  57,  54,\n",
      "         54,  48,  54, 125], dtype=torch.uint8)), ('model.model.layers.30.self_attn.v_proj.base_layer.weight', tensor([[ 97],\n",
      "        [221],\n",
      "        [142],\n",
      "        ...,\n",
      "        [ 71],\n",
      "        [ 86],\n",
      "        [ 20]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.self_attn.v_proj.base_layer.weight.absmax', tensor([ 60,  78, 191,  ...,  98,  93, 203], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.30.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.30.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0256, 0.0271, 0.0359,  ..., 0.0661, 0.2234, 0.0866], device='cuda:0')), ('model.model.layers.30.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.30.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  51,  57,  55,  53,  48,  49,  50,  49,  53,  51,  51,  56,  55,\n",
      "         48,  55, 125], dtype=torch.uint8)), ('model.model.layers.30.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.30.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.30.self_attn.o_proj.weight', tensor([[105],\n",
      "        [100],\n",
      "        [135],\n",
      "        ...,\n",
      "        [213],\n",
      "        [199],\n",
      "        [164]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.self_attn.o_proj.weight.absmax', tensor([195,  63,  57,  ..., 209,  53, 201], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.30.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.30.self_attn.o_proj.weight.nested_absmax', tensor([0.0364, 0.0442, 0.0301,  ..., 0.0242, 0.0354, 0.0306], device='cuda:0')), ('model.model.layers.30.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.30.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  51,  57,  49,  54,  53,  56,  56,  52,  50,  53,  54,  51,  54,\n",
      "         50,  57, 125], dtype=torch.uint8)), ('model.model.layers.30.mlp.gate_proj.weight', tensor([[ 17],\n",
      "        [ 82],\n",
      "        [105],\n",
      "        ...,\n",
      "        [193],\n",
      "        [119],\n",
      "        [147]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.mlp.gate_proj.weight.absmax', tensor([ 54, 209, 109,  ...,  88,  40,  59], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.30.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.30.mlp.gate_proj.weight.nested_absmax', tensor([0.0184, 0.0471, 0.0300,  ..., 0.0225, 0.0305, 0.0344], device='cuda:0')), ('model.model.layers.30.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.30.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  50,  48,  53,  48,  56,  48,  54,  53,  56,  49,  57,  55,\n",
      "         52,  48,  51, 125], dtype=torch.uint8)), ('model.model.layers.30.mlp.up_proj.weight', tensor([[121],\n",
      "        [165],\n",
      "        [201],\n",
      "        ...,\n",
      "        [196],\n",
      "        [116],\n",
      "        [139]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.mlp.up_proj.weight.absmax', tensor([211,  46, 206,  ..., 201,  57, 184], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.30.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.30.mlp.up_proj.weight.nested_absmax', tensor([0.0292, 0.0414, 0.0370,  ..., 0.0341, 0.0292, 0.0385], device='cuda:0')), ('model.model.layers.30.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.30.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  56,  55,  48,  51,  52,  50,  48,  49,  54,  50,  50,\n",
      "         48,  48,  57, 125], dtype=torch.uint8)), ('model.model.layers.30.mlp.down_proj.weight', tensor([[195],\n",
      "        [ 30],\n",
      "        [ 89],\n",
      "        ...,\n",
      "        [198],\n",
      "        [154],\n",
      "        [100]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.30.mlp.down_proj.weight.absmax', tensor([ 45,  57, 181,  ..., 149,  70, 187], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.30.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.30.mlp.down_proj.weight.nested_absmax', tensor([0.0292, 0.0341, 0.0229,  ..., 0.0297, 0.0414, 0.0370], device='cuda:0')), ('model.model.layers.30.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.30.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  52,  57,  51,  57,  56,  49,  48,  49,  56,  54,  54,  50,  52,\n",
      "         53,  50,  55, 125], dtype=torch.uint8)), ('model.model.layers.30.input_layernorm.weight', tensor([0.5664, 0.5742, 0.5625,  ..., 0.5469, 0.5547, 0.5781], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.30.post_attention_layernorm.weight', tensor([0.4746, 0.4824, 0.4707,  ..., 0.4746, 0.4766, 0.4707], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.31.self_attn.q_proj.base_layer.weight', tensor([[ 11],\n",
      "        [222],\n",
      "        [169],\n",
      "        ...,\n",
      "        [180],\n",
      "        [ 29],\n",
      "        [156]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.31.self_attn.q_proj.base_layer.weight.absmax', tensor([ 60,  63,  63,  ..., 198,  93, 196], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.31.self_attn.q_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.31.self_attn.q_proj.base_layer.weight.nested_absmax', tensor([0.1546, 0.0302, 0.0448,  ..., 0.0682, 0.0501, 0.0526], device='cuda:0')), ('model.model.layers.31.self_attn.q_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.31.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  54,  50,  57,  56,  54,  53,  48,  56,  48,  49,  49,  56,  49,\n",
      "         55,  57, 125], dtype=torch.uint8)), ('model.model.layers.31.self_attn.q_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.31.self_attn.q_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.31.self_attn.k_proj.weight', tensor([[ 48],\n",
      "        [239],\n",
      "        [166],\n",
      "        ...,\n",
      "        [148],\n",
      "        [166],\n",
      "        [120]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.31.self_attn.k_proj.weight.absmax', tensor([ 39,  52,  55,  ...,  72, 173, 206], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.31.self_attn.k_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.31.self_attn.k_proj.weight.nested_absmax', tensor([0.0742, 0.0310, 0.0377,  ..., 0.1708, 0.0512, 0.0868], device='cuda:0')), ('model.model.layers.31.self_attn.k_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.31.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         53,  55,  54,  56,  51,  57,  56,  53,  54,  56,  48,  51,  52,  49,\n",
      "         55,  50, 125], dtype=torch.uint8)), ('model.model.layers.31.self_attn.v_proj.base_layer.weight', tensor([[177],\n",
      "        [ 98],\n",
      "        [ 83],\n",
      "        ...,\n",
      "        [ 99],\n",
      "        [253],\n",
      "        [ 95]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.31.self_attn.v_proj.base_layer.weight.absmax', tensor([ 28, 205, 158,  ...,  92,  46,  45], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.31.self_attn.v_proj.base_layer.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.31.self_attn.v_proj.base_layer.weight.nested_absmax', tensor([0.0229, 0.0264, 0.0214,  ..., 0.0308, 0.0366, 0.0249], device='cuda:0')), ('model.model.layers.31.self_attn.v_proj.base_layer.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.31.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  57,  51,  50,  49,  49,  52,  52,  56,  49,  57,  50,  53,  57,\n",
      "         54,  52,  52, 125], dtype=torch.uint8)), ('model.model.layers.31.self_attn.v_proj.lora_A.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.31.self_attn.v_proj.lora_B.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')), ('model.model.layers.31.self_attn.o_proj.weight', tensor([[141],\n",
      "        [102],\n",
      "        [116],\n",
      "        ...,\n",
      "        [ 67],\n",
      "        [189],\n",
      "        [ 56]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.31.self_attn.o_proj.weight.absmax', tensor([194,  56, 219,  ...,  90, 202, 195], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.31.self_attn.o_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.31.self_attn.o_proj.weight.nested_absmax', tensor([0.0347, 0.0586, 0.0259,  ..., 0.0230, 0.0352, 0.0274], device='cuda:0')), ('model.model.layers.31.self_attn.o_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.31.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
      "        101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
      "        101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
      "        111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
      "        100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
      "         52,  56,  55,  55,  54,  51,  49,  51,  54,  54,  50,  53,  50,  56,\n",
      "         57,  57, 125], dtype=torch.uint8)), ('model.model.layers.31.mlp.gate_proj.weight', tensor([[132],\n",
      "        [225],\n",
      "        [108],\n",
      "        ...,\n",
      "        [188],\n",
      "        [ 68],\n",
      "        [169]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.31.mlp.gate_proj.weight.absmax', tensor([ 33,  59,  62,  ..., 175, 134,  91], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.31.mlp.gate_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.31.mlp.gate_proj.weight.nested_absmax', tensor([0.0325, 0.2927, 0.0457,  ..., 0.0325, 0.0227, 0.0711], device='cuda:0')), ('model.model.layers.31.mlp.gate_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.31.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  52,  57,  50,  52,  51,  51,  57,  48,  53,  54,  48,  49,\n",
      "         53,  48,  49,  53, 125], dtype=torch.uint8)), ('model.model.layers.31.mlp.up_proj.weight', tensor([[ 20],\n",
      "        [210],\n",
      "        [149],\n",
      "        ...,\n",
      "        [ 86],\n",
      "        [134],\n",
      "        [139]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.31.mlp.up_proj.weight.absmax', tensor([193, 196, 156,  ...,  68,  61, 197], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.31.mlp.up_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.31.mlp.up_proj.weight.nested_absmax', tensor([0.0332, 0.1001, 0.0786,  ..., 0.0327, 0.0318, 0.1011], device='cuda:0')), ('model.model.layers.31.mlp.up_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.31.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
      "         49,  48,  48,  56,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  50,  50,  49,  49,  49,  54,  57,  49,  53,  51,  52,  53,\n",
      "         49,  57,  50, 125], dtype=torch.uint8)), ('model.model.layers.31.mlp.down_proj.weight', tensor([[ 98],\n",
      "        [ 83],\n",
      "        [232],\n",
      "        ...,\n",
      "        [ 86],\n",
      "        [ 57],\n",
      "        [125]], device='cuda:0', dtype=torch.uint8)), ('model.model.layers.31.mlp.down_proj.weight.absmax', tensor([ 61, 204, 158,  ...,  61,  97,  89], device='cuda:0',\n",
      "       dtype=torch.uint8)), ('model.model.layers.31.mlp.down_proj.weight.quant_map', tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:0')), ('model.model.layers.31.mlp.down_proj.weight.nested_absmax', tensor([0.0257, 0.0359, 0.0286,  ..., 0.0266, 0.0237, 0.0242], device='cuda:0')), ('model.model.layers.31.mlp.down_proj.weight.nested_quant_map', tensor([-9.9297e-01, -9.7891e-01, -9.6484e-01, -9.5078e-01, -9.3672e-01,\n",
      "        -9.2266e-01, -9.0859e-01, -8.9453e-01, -8.8047e-01, -8.6641e-01,\n",
      "        -8.5234e-01, -8.3828e-01, -8.2422e-01, -8.1016e-01, -7.9609e-01,\n",
      "        -7.8203e-01, -7.6797e-01, -7.5391e-01, -7.3984e-01, -7.2578e-01,\n",
      "        -7.1172e-01, -6.9766e-01, -6.8359e-01, -6.6953e-01, -6.5547e-01,\n",
      "        -6.4141e-01, -6.2734e-01, -6.1328e-01, -5.9922e-01, -5.8516e-01,\n",
      "        -5.7109e-01, -5.5703e-01, -5.4297e-01, -5.2891e-01, -5.1484e-01,\n",
      "        -5.0078e-01, -4.8672e-01, -4.7266e-01, -4.5859e-01, -4.4453e-01,\n",
      "        -4.3047e-01, -4.1641e-01, -4.0234e-01, -3.8828e-01, -3.7422e-01,\n",
      "        -3.6016e-01, -3.4609e-01, -3.3203e-01, -3.1797e-01, -3.0391e-01,\n",
      "        -2.8984e-01, -2.7578e-01, -2.6172e-01, -2.4766e-01, -2.3359e-01,\n",
      "        -2.1953e-01, -2.0547e-01, -1.9141e-01, -1.7734e-01, -1.6328e-01,\n",
      "        -1.4922e-01, -1.3516e-01, -1.2109e-01, -1.0703e-01, -9.8594e-02,\n",
      "        -9.5781e-02, -9.2969e-02, -9.0156e-02, -8.7344e-02, -8.4531e-02,\n",
      "        -8.1719e-02, -7.8906e-02, -7.6094e-02, -7.3281e-02, -7.0469e-02,\n",
      "        -6.7656e-02, -6.4844e-02, -6.2031e-02, -5.9219e-02, -5.6406e-02,\n",
      "        -5.3594e-02, -5.0781e-02, -4.7969e-02, -4.5156e-02, -4.2344e-02,\n",
      "        -3.9531e-02, -3.6719e-02, -3.3906e-02, -3.1094e-02, -2.8281e-02,\n",
      "        -2.5469e-02, -2.2656e-02, -1.9844e-02, -1.7031e-02, -1.4219e-02,\n",
      "        -1.1406e-02, -9.7187e-03, -9.1562e-03, -8.5938e-03, -8.0312e-03,\n",
      "        -7.4687e-03, -6.9063e-03, -6.3437e-03, -5.7813e-03, -5.2188e-03,\n",
      "        -4.6562e-03, -4.0937e-03, -3.5312e-03, -2.9687e-03, -2.4062e-03,\n",
      "        -1.8438e-03, -1.2812e-03, -9.4375e-04, -8.3125e-04, -7.1875e-04,\n",
      "        -6.0625e-04, -4.9375e-04, -3.8125e-04, -2.6875e-04, -1.5625e-04,\n",
      "        -8.8750e-05, -6.6250e-05, -4.3750e-05, -2.1250e-05, -7.7500e-06,\n",
      "        -3.2500e-06, -5.5000e-07,  0.0000e+00,  5.5000e-07,  3.2500e-06,\n",
      "         7.7500e-06,  2.1250e-05,  4.3750e-05,  6.6250e-05,  8.8750e-05,\n",
      "         1.5625e-04,  2.6875e-04,  3.8125e-04,  4.9375e-04,  6.0625e-04,\n",
      "         7.1875e-04,  8.3125e-04,  9.4375e-04,  1.2812e-03,  1.8438e-03,\n",
      "         2.4062e-03,  2.9687e-03,  3.5312e-03,  4.0937e-03,  4.6562e-03,\n",
      "         5.2188e-03,  5.7813e-03,  6.3437e-03,  6.9063e-03,  7.4687e-03,\n",
      "         8.0312e-03,  8.5938e-03,  9.1562e-03,  9.7187e-03,  1.1406e-02,\n",
      "         1.4219e-02,  1.7031e-02,  1.9844e-02,  2.2656e-02,  2.5469e-02,\n",
      "         2.8281e-02,  3.1094e-02,  3.3906e-02,  3.6719e-02,  3.9531e-02,\n",
      "         4.2344e-02,  4.5156e-02,  4.7969e-02,  5.0781e-02,  5.3594e-02,\n",
      "         5.6406e-02,  5.9219e-02,  6.2031e-02,  6.4844e-02,  6.7656e-02,\n",
      "         7.0469e-02,  7.3281e-02,  7.6094e-02,  7.8906e-02,  8.1719e-02,\n",
      "         8.4531e-02,  8.7344e-02,  9.0156e-02,  9.2969e-02,  9.5781e-02,\n",
      "         9.8594e-02,  1.0703e-01,  1.2109e-01,  1.3516e-01,  1.4922e-01,\n",
      "         1.6328e-01,  1.7734e-01,  1.9141e-01,  2.0547e-01,  2.1953e-01,\n",
      "         2.3359e-01,  2.4766e-01,  2.6172e-01,  2.7578e-01,  2.8984e-01,\n",
      "         3.0391e-01,  3.1797e-01,  3.3203e-01,  3.4609e-01,  3.6016e-01,\n",
      "         3.7422e-01,  3.8828e-01,  4.0234e-01,  4.1641e-01,  4.3047e-01,\n",
      "         4.4453e-01,  4.5859e-01,  4.7266e-01,  4.8672e-01,  5.0078e-01,\n",
      "         5.1484e-01,  5.2891e-01,  5.4297e-01,  5.5703e-01,  5.7109e-01,\n",
      "         5.8516e-01,  5.9922e-01,  6.1328e-01,  6.2734e-01,  6.4141e-01,\n",
      "         6.5547e-01,  6.6953e-01,  6.8359e-01,  6.9766e-01,  7.1172e-01,\n",
      "         7.2578e-01,  7.3984e-01,  7.5391e-01,  7.6797e-01,  7.8203e-01,\n",
      "         7.9609e-01,  8.1016e-01,  8.2422e-01,  8.3828e-01,  8.5234e-01,\n",
      "         8.6641e-01,  8.8047e-01,  8.9453e-01,  9.0859e-01,  9.2266e-01,\n",
      "         9.3672e-01,  9.5078e-01,  9.6484e-01,  9.7891e-01,  9.9297e-01,\n",
      "         1.0000e+00], device='cuda:0')), ('model.model.layers.31.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
      "         32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
      "        115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
      "        121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
      "         34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
      "         48,  57,  54,  44,  32,  49,  49,  48,  48,  56,  93,  44,  32,  34,\n",
      "        110, 101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105,\n",
      "        122, 101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115,\n",
      "        116, 101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102,\n",
      "        108, 111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116,\n",
      "        101, 100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,\n",
      "         48,  53,  48,  48,  50,  53,  54,  50,  51,  50,  57,  49,  55,  51,\n",
      "         48,  56,  56, 125], dtype=torch.uint8)), ('model.model.layers.31.input_layernorm.weight', tensor([0.4824, 0.4805, 0.4336,  ..., 0.4258, 0.4531, 0.4785], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.layers.31.post_attention_layernorm.weight', tensor([0.4297, 0.4297, 0.4355,  ..., 0.4180, 0.4043, 0.4238], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.model.norm.weight', tensor([1.8594, 1.8516, 1.7969,  ..., 1.7109, 1.8125, 1.5938], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.classifier.original_module.weight', tensor([[ 0.0197, -0.0231, -0.0250,  ..., -0.0155,  0.0297,  0.0414],\n",
      "        [ 0.0314,  0.0206,  0.0217,  ..., -0.0244, -0.0290, -0.0303],\n",
      "        [ 0.0220, -0.0053, -0.0186,  ...,  0.0176,  0.0140, -0.0065]],\n",
      "       device='cuda:0', dtype=torch.float16)), ('model.classifier.original_module.bias', tensor([0., 0., 0.], device='cuda:0', dtype=torch.float16)), ('model.classifier.modules_to_save.default.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16)), ('model.classifier.modules_to_save.default.bias', tensor([nan, nan, nan], device='cuda:0', dtype=torch.float16))])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "for layer in trainer.model.children():\n",
    "    #if isinstance(layer, nn.Linear):\n",
    "    print(layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss={'logits': tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)}, logits=tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = ['hello', 'its me']\n",
    "tokenized = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "trainer.model(**tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USO IL LORO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling task wnut_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n",
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,344,717 || all params: 6,613,741,594 || trainable%: 0.09593233890111372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1009/1009 [00:00<00:00, 10585.56 examples/s]\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mferrazzipietro\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pferrazzi/LS-LLaMA-E3C/wandb/run-20240429_103438-bt08out5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ferrazzipietro/huggingface/runs/bt08out5' target=\"_blank\">clear-breeze-11</a></strong> to <a href='https://wandb.ai/ferrazzipietro/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ferrazzipietro/huggingface' target=\"_blank\">https://wandb.ai/ferrazzipietro/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ferrazzipietro/huggingface/runs/bt08out5' target=\"_blank\">https://wandb.ai/ferrazzipietro/huggingface/runs/bt08out5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 02:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5bb0-39a7bc50710f4364275d2b0f;1f163fc5-3ea5-4109-ab6a-2c56095d29ca)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5bc0-3fe2ed1b27be37d52ec067cf;202a9e28-421b-49ec-b8e6-8542fbfa7cc1)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5bcf-5e7dfe0c135e33044cd4681c;23b28223-9e50-41de-a131-421078e48efa)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5be0-4bd3003714b52f425871c24c;8305a202-0368-4d29-a8ee-ca769741670d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5bf0-0a44536350a9f6421ac1d6a1;f3c5a24a-e97c-4571-bfd3-0a0c419f0952)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5bff-702d002426af4d3d1a73d562;0bddf715-793d-44b1-8ed9-8ccf38a5121c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5c0f-26faef7626d01f5b1aa0cf8c;c9cdc2a2-da95-470f-92da-88bc6cff0bef)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5c1f-3b1f1ef618a008b12622cbe0;c40b1db7-37e8-4fdc-9755-96832e84e9b8)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5c2f-393be78e3d9a879679a274a2;5a1c3b1f-61e4-48f9-b339-66f8f334066b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5c3f-60c139645ff2ee986fb20199;2faac3b0-3a45-438b-9b7b-726b8d6564f9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n",
      "  warnings.warn(\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=392.2526611328125, metrics={'train_runtime': 161.8758, 'train_samples_per_second': 0.988, 'train_steps_per_second': 0.062, 'total_flos': 199352334704640.0, 'train_loss': 392.2526611328125, 'epoch': 10.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from modeling_llama import LlamaForTokenClassification\n",
    "\n",
    "\n",
    "def load_ontonotesv5():\n",
    "    ret = {}\n",
    "    for split_name in ['train', 'dev', 'test']:\n",
    "        data = []\n",
    "        with open(f'./data/NER/ontonotesv5/{split_name}.jsonl', 'r') as reader:\n",
    "            for line in reader:\n",
    "                data.append(json.loads(line))\n",
    "        ret[split_name] = Dataset.from_list(data)\n",
    "    return DatasetDict(ret)\n",
    "\n",
    "\n",
    "# if len(sys.argv) != 3:\n",
    "#     print('usage python %.py task model_size')\n",
    "#     sys.exit()\n",
    "# task, model_size = sys.argv[1], sys.argv[2].lower()\n",
    "task, model_size = 'wnut_17', '7b'\n",
    "print(f'handling task {task}')\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "learning_rate = 1e-4\n",
    "max_length = 64\n",
    "if model_size == '7b':\n",
    "    model_id = 'meta-llama/Llama-2-7b-chat-hf'#'NousResearch/Llama-2-7b-hf'\n",
    "    lora_r = 12\n",
    "elif model_size == '13b':\n",
    "    model_id = 'NousResearch/Llama-2-13b-hf'\n",
    "    lora_r = 12\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,token = dotenv_values(\".env.base\")['LLAMA_TOKEN'],\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# seqeval = evaluate.load(\"seqeval\")\n",
    "if task == 'wnut_17':\n",
    "    ds = load_dataset(\"wnut_17\")\n",
    "    label2id = { \"O\": 0, \"B-corporation\": 1, \"I-corporation\": 2, \"B-creative-work\": 3, \"I-creative-work\": 4, \"B-group\": 5, \"I-group\": 6, \"B-location\": 7, \"I-location\": 8, \"B-person\": 9, \"I-person\": 10, \"B-product\": 11, \"I-product\": 12, }\n",
    "elif task == 'conll2003':\n",
    "    ds = load_dataset(\"conll2003\")\n",
    "    label2id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "elif task == 'ontonotesv5':\n",
    "    ds = load_ontonotesv5()\n",
    "    label2id = {'O': 0, 'B-NORP': 1, 'B-PERSON': 2, 'B-WORK_OF_ART': 3, 'B-QUANTITY': 4, 'B-EVENT': 5, 'B-DATE': 6, 'B-TIME': 7, 'B-PERCENT': 8, 'B-LANGUAGE': 9, 'B-ORG': 10, 'B-CARDINAL': 11, 'B-LAW': 12, 'B-GPE': 13, 'B-PRODUCT': 14, 'B-LOC': 15, 'B-MONEY': 16, 'B-ORDINAL': 17, 'B-FAC': 18}\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "label_list = list(label2id.keys()) # ds[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "model = LlamaForTokenClassification.from_pretrained(\n",
    "    model_id, num_labels=len(label2id), id2label=id2label, label2id=label2id,\n",
    "    load_in_4bit=True,\n",
    "    token = dotenv_values(\".env.base\")['LLAMA_TOKEN'],\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache'\n",
    "\n",
    ")# .bfloat16()\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, inference_mode=False, r=lora_r, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, padding='longest', max_length=max_length, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     predictions, labels = p\n",
    "#     predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "#     true_predictions = [\n",
    "#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     true_labels = [\n",
    "#         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "\n",
    "#     results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": results[\"overall_precision\"],\n",
    "#         \"recall\": results[\"overall_recall\"],\n",
    "#         \"f1\": results[\"overall_f1\"],\n",
    "#         \"accuracy\": results[\"overall_accuracy\"],\n",
    "#     }\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_ds_model\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"].select(range(16)),\n",
    "    eval_dataset=tokenized_ds[\"test\"].select(range(16)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss={'logits': tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)}, logits=tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = ['hello', 'its me']\n",
    "tokenized = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "trainer.model(**tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
