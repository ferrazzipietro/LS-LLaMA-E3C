{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from peft import PeftModel, PeftConfig\n",
    "from dotenv import dotenv_values\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "from utils import DataPreprocessor, DatasetFormatConverter\n",
    "from utils import DataPreprocessor\n",
    "\n",
    "from src.billm import LlamaForTokenClassification\n",
    "\n",
    "WANDB_KEY = dotenv_values(\".env.base\")['WANDB_KEY']\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "adapters = \"ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.layer1_NoQuant_64_64_0.01_2_0.0002\"\n",
    "peft_config = PeftConfig.from_pretrained(adapters)\n",
    "BASE_MODEL_CHECKPOINT = peft_config.base_model_name_or_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT,token =HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# seqeval = evaluate.load(\"seqeval\")\n",
    "DATASET_CHEKPOINT=\"ferrazzipietro/e3c-sentences\" \n",
    "TRAIN_LAYER=\"en.layer1\"\n",
    "preprocessor = DataPreprocessor(BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer)\n",
    "dataset = load_dataset(DATASET_CHEKPOINT) #download_mode=\"force_redownload\"\n",
    "dataset = dataset[TRAIN_LAYER]\n",
    "dataset = dataset.shuffle(seed=1234)  \n",
    "dataset_format_converter = DatasetFormatConverter(dataset)\n",
    "dataset_format_converter.apply()\n",
    "ds = dataset_format_converter.dataset\n",
    "label2id = dataset_format_converter.label2id\n",
    "id2label = dataset_format_converter.get_id2label()\n",
    "label_list = dataset_format_converter.get_label_list()\n",
    "dataset_format_converter.set_tokenizer(tokenizer)\n",
    "dataset_format_converter.set_max_seq_length(256)\n",
    "tokenized_ds = ds.map(lambda x: dataset_format_converter.tokenize_and_align_labels(x), batched=True)# dataset_format_converter.dataset.map(tokenize_and_align_labels, batched=True)\n",
    "train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(tokenized_ds, TRAIN_LAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'entities', 'original_text', 'original_id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 669\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the lengths of the strings\n",
    "lengths = [len(sentence) for sentence in train_data['sentence']]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(lengths, bins=10)\n",
    "plt.xlabel('Length of Strings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of String Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "/home/pferrazzi/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                )\n",
    "\n",
    "model = LlamaForTokenClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    num_labels=len(label2id), id2label=id2label, label2id=label2id,\n",
    "    token = HF_TOKEN,\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache',\n",
    "    device_map='auto',\n",
    "    quantization_config = bnb_config)\n",
    "model = PeftModel.from_pretrained(model, adapters, token = HF_TOKEN)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_output_logits \u001b[38;5;241m=\u001b[39m generated_ids\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()    \n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[1;32m      4\u001b[0m [[label_list[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      6\u001b[0m     ]\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "model_output_logits = generated_ids.logits[0].cpu().detach().float().numpy()    \n",
    "predictions = np.argmax(model_output_logits, axis=2)\n",
    "print(predictions)\n",
    "[[label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip([0,1,1,1,2,2,0,2], [0,1,1,0,0,0,0,2])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    print(results)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( ['O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'I', 'I', 'O', 'I', 'I', 'I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 4\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(p):\n\u001b[0;32m----> 4\u001b[0m     predictions, labels \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m     true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m         [label_list[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[1;32m     10\u001b[0m     ]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "examples = [train_data['sentence'][0] , train_data['sentence'][5]]\n",
    "input_sentences = examples\n",
    "encodeds = tokenizer(input_sentences, return_tensors=\"pt\", add_special_tokens=True, padding=True)\n",
    "model_inputs = encodeds.to('cuda')\n",
    "generated_ids = model(**model_inputs)\n",
    "compute_metrics(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = OutputGeneration(model, tokenizer, id2label)\n",
    "pl = gen._create_prediction_list(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their child acquired walking at the age of 14 months.\n",
      "n words in sentence: 10\n",
      "len_ sentence: 53\n",
      "n tokens with labels: 14\n",
      "n  created labels: 78\n",
      "Pertinent laboratory studies included a hemoglobin level of 10 g/dL, platelet count was normal, blood urea of 1,2 g/l (0,18-0,45 g/L), and a creatinine level of 68 mg/L (7-13 mg/L).\n",
      "n words in sentence: 30\n",
      "len_ sentence: 181\n",
      "n tokens with labels: 78\n",
      "n  created labels: 78\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate([0,5]):\n",
    "    print((train_data[s]['sentence']))\n",
    "    print(f\"n words in sentence: {len(train_data[s]['sentence'].split())}\")\n",
    "    print(f\"len_ sentence: {len(train_data[s]['sentence'])}\")\n",
    "    print(f\"n tokens with labels: {len(train_data[s]['labels'])}\")\n",
    "    print(f\"n  created labels: {len(generated_ids['logits'][i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'MT5ForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PhiForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'T5ForTokenClassification', 'UMT5ForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (14) must match the existing size (4096) at non-singleton dimension 2.  Target sizes: [1, 32, 14, 14].  Tensor sizes: [1, 1, 4096, 14]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m token_classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      5\u001b[0m                             tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[1;32m      6\u001b[0m                             aggregation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtoken_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m l \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m tqdm(token_classifier(KeyDataset(train_data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m24\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m))):\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:248\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    246\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1234\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1149\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1148\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1149\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:285\u001b[0m, in \u001b[0;36mTokenClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits,\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m    295\u001b[0m }\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/billm/modeling_llama.py:269\u001b[0m, in \u001b[0;36mLlamaForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 269\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    282\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/billm/modeling_llama.py:132\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    121\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    122\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    123\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m         cache_position,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbi_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_bidirectional\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:741\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:671\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    667\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# In case we are not compiling, we may set `causal_mask` to None, which is required to dispatch to SDPA's Flash Attention 2 backend, rather\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# relying on the `is_causal` argument.\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    681\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (14) must match the existing size (4096) at non-singleton dimension 2.  Target sizes: [1, 32, 14, 14].  Tensor sizes: [1, 1, 4096, 14]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", model=model, \n",
    "                            tokenizer=tokenizer, \n",
    "                            aggregation_strategy=\"simple\", batch_size=12)\n",
    "token_classifier(train_data[0]['sentence'])\n",
    "l = []\n",
    "for out in tqdm(token_classifier(KeyDataset(train_data.select(range(24)), \"sentence\"))):\n",
    "    l.append(out)\n",
    "\n",
    "tmp = train_data.select(range(24)).add_column('model_output', l)\n",
    "tmp[6]['model_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Their', 'child', 'acquired', 'walking', 'at', 'the', 'age', 'of', '14', 'months.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁Their',\n",
       " '▁child',\n",
       " '▁acquired',\n",
       " '▁walking',\n",
       " '▁at',\n",
       " '▁the',\n",
       " '▁age',\n",
       " '▁of',\n",
       " '▁',\n",
       " '1',\n",
       " '4',\n",
       " '▁months',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data[0]['tokens'])\n",
    "example = train_data[0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForTokenClassification' is not supported for ner. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'MT5ForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PhiForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'T5ForTokenClassification', 'UMT5ForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "24it [00:04,  4.83it/s]                      \n",
      "Flattening the indices: 100%|██████████| 24/24 [00:00<00:00, 3808.10 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'end': 24,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.657,\n",
       "  'start': 7,\n",
       "  'word': 'assessment found'},\n",
       " {'end': 38,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.794,\n",
       "  'start': 32,\n",
       "  'word': 'score'},\n",
       " {'end': 44,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.6616,\n",
       "  'start': 40,\n",
       "  'word': 'GCS)'},\n",
       " {'end': 48, 'entity_group': 'B', 'score': 0.6094, 'start': 47, 'word': ''},\n",
       " {'end': 52, 'entity_group': 'I', 'score': 0.585, 'start': 51, 'word': '1'},\n",
       " {'end': 53, 'entity_group': 'B', 'score': 0.572, 'start': 52, 'word': '5'},\n",
       " {'end': 66,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.824,\n",
       "  'start': 55,\n",
       "  'word': 'eye opening'},\n",
       " {'end': 70, 'entity_group': 'B', 'score': 0.708, 'start': 69, 'word': ''},\n",
       " {'end': 72, 'entity_group': 'B', 'score': 0.4004, 'start': 71, 'word': ','},\n",
       " {'end': 88,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.628,\n",
       "  'start': 76,\n",
       "  'word': 'bal response'},\n",
       " {'end': 93, 'entity_group': 'B', 'score': 0.6816, 'start': 91, 'word': '2'},\n",
       " {'end': 100,\n",
       "  'entity_group': 'I',\n",
       "  'score': 0.623,\n",
       "  'start': 94,\n",
       "  'word': 'motor'},\n",
       " {'end': 114, 'entity_group': 'B', 'score': 0.7153, 'start': 112, 'word': '5'},\n",
       " {'end': 135,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.6855,\n",
       "  'start': 132,\n",
       "  'word': 're'},\n",
       " {'end': 148,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.829,\n",
       "  'start': 141,\n",
       "  'word': 'pupils'},\n",
       " {'end': 169,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.8335,\n",
       "  'start': 157,\n",
       "  'word': 'feeling-mot'},\n",
       " {'end': 171,\n",
       "  'entity_group': 'I',\n",
       "  'score': 0.6196,\n",
       "  'start': 169,\n",
       "  'word': 'or'},\n",
       " {'end': 179,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.65,\n",
       "  'start': 171,\n",
       "  'word': 'deficit'},\n",
       " {'end': 192,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.7686,\n",
       "  'start': 187,\n",
       "  'word': 'oste'},\n",
       " {'end': 209,\n",
       "  'entity_group': 'I',\n",
       "  'score': 0.6143,\n",
       "  'start': 194,\n",
       "  'word': 'endinous reflex'},\n",
       " {'end': 211,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.6772,\n",
       "  'start': 209,\n",
       "  'word': 'es'},\n",
       " {'end': 221,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.5474,\n",
       "  'start': 212,\n",
       "  'word': 'slightly'},\n",
       " {'end': 225,\n",
       "  'entity_group': 'I',\n",
       "  'score': 0.6074,\n",
       "  'start': 221,\n",
       "  'word': 'pol'},\n",
       " {'end': 231,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.6665,\n",
       "  'start': 225,\n",
       "  'word': 'ypneic'},\n",
       " {'end': 235, 'entity_group': 'B', 'score': 0.9023, 'start': 234, 'word': ''},\n",
       " {'end': 245, 'entity_group': 'I', 'score': 0.6304, 'start': 244, 'word': '/'},\n",
       " {'end': 248,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.498,\n",
       "  'start': 245,\n",
       "  'word': 'min'},\n",
       " {'end': 251, 'entity_group': 'I', 'score': 0.346, 'start': 249, 'word': 'p'},\n",
       " {'end': 254,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.8604,\n",
       "  'start': 251,\n",
       "  'word': 'uls'},\n",
       " {'end': 258, 'entity_group': 'B', 'score': 0.4985, 'start': 256, 'word': 'o'},\n",
       " {'end': 282,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.8027,\n",
       "  'start': 263,\n",
       "  'word': 'saturation (SpO 2)'},\n",
       " {'end': 286, 'entity_group': 'B', 'score': 0.6333, 'start': 285, 'word': ''},\n",
       " {'end': 304,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.61,\n",
       "  'start': 292,\n",
       "  'word': 'ambient air'},\n",
       " {'end': 308, 'entity_group': 'B', 'score': 0.737, 'start': 305, 'word': 'he'},\n",
       " {'end': 318,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.867,\n",
       "  'start': 312,\n",
       "  'word': 'tachy'},\n",
       " {'end': 324,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.7837,\n",
       "  'start': 322,\n",
       "  'word': 'ic'},\n",
       " {'end': 328, 'entity_group': 'B', 'score': 0.5366, 'start': 327, 'word': ''},\n",
       " {'end': 335, 'entity_group': 'B', 'score': 0.499, 'start': 333, 'word': 'pm'},\n",
       " {'end': 352,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.8047,\n",
       "  'start': 339,\n",
       "  'word': 'hypertensive'},\n",
       " {'end': 356, 'entity_group': 'B', 'score': 0.4436, 'start': 355, 'word': ''},\n",
       " {'end': 360, 'entity_group': 'I', 'score': 0.843, 'start': 359, 'word': '/'},\n",
       " {'end': 366,\n",
       "  'entity_group': 'I',\n",
       "  'score': 0.684,\n",
       "  'start': 362,\n",
       "  'word': 'mmH'},\n",
       " {'end': 367, 'entity_group': 'B', 'score': 0.419, 'start': 366, 'word': 'g'},\n",
       " {'end': 379,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.745,\n",
       "  'start': 372,\n",
       "  'word': 'capill'},\n",
       " {'end': 393,\n",
       "  'entity_group': 'I',\n",
       "  'score': 0.541,\n",
       "  'start': 388,\n",
       "  'word': 'gluc'},\n",
       " {'end': 396,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.617,\n",
       "  'start': 393,\n",
       "  'word': 'ose'},\n",
       " {'end': 400, 'entity_group': 'B', 'score': 0.7466, 'start': 399, 'word': ''},\n",
       " {'end': 402, 'entity_group': 'I', 'score': 0.541, 'start': 401, 'word': '.'},\n",
       " {'end': 407,\n",
       "  'entity_group': 'I',\n",
       "  'score': 0.7344,\n",
       "  'start': 404,\n",
       "  'word': 'g/'},\n",
       " {'end': 408, 'entity_group': 'B', 'score': 0.5786, 'start': 407, 'word': 'L'},\n",
       " {'end': 415, 'entity_group': 'B', 'score': 0.676, 'start': 413, 'word': 'g'},\n",
       " {'end': 424,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.673,\n",
       "  'start': 417,\n",
       "  'word': 'cosuria'},\n",
       " {'end': 434,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.5312,\n",
       "  'start': 431,\n",
       "  'word': 'ur'},\n",
       " {'end': 451,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.7393,\n",
       "  'start': 443,\n",
       "  'word': 'ick test'},\n",
       " {'end': 469,\n",
       "  'entity_group': 'B',\n",
       "  'score': 0.8726,\n",
       "  'start': 459,\n",
       "  'word': 'ketonuria'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "token_classifier = pipeline(\"ner\", model=model, \n",
    "                            tokenizer=tokenizer, \n",
    "                            aggregation_strategy=\"simple\", batch_size=12)\n",
    "\n",
    "\n",
    "#token_classifier(train_data[0]['tokens'])\n",
    "l = []\n",
    "for out in tqdm(token_classifier(KeyDataset(train_data.select(range(24)), \"sentence\"))):\n",
    "    l.append(out)\n",
    "\n",
    "tmp = train_data.select(range(24)).add_column('model_output', l)\n",
    "tmp[6]['model_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import evaluate\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "data = load_dataset(\"csv\", data_files=\"data/evaluation/train_data_LS_Mistral-7B-v0.1_adapters_en.layer1_NoQuant_16_32_0.01_2_0.0002.csv\")\n",
    "def helper(example):\n",
    "    example['model_output'] = ast.literal_eval(example['model_output'].replace('\\n', ','))\n",
    "    return example\n",
    "data = data.map(lambda x: helper(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def _extract_TP_FP_FN(self, model_response: str, ground_truth: str, similar_is_equal:bool=True, similar_is_equal_threshold: int=100, similarity_types:'list[str]'=['case', 'stop_words', 'subset', 'superset', 'leveshtein'], words_level:bool=True) -> [int, int, int]:\n",
    "        \"\"\"\n",
    "        Compute the F1 score, the precision and the recall between the model output and the ground truth\n",
    "\n",
    "        Args:\n",
    "        model_response (str): the model output as it is returned by the model\n",
    "        ground_truth (str): the ground truth in json format.\n",
    "        similar_is_equal (bool): if True, the function will consider similar entities as equal. The default value is False.\n",
    "        similar_is_equal_threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "        words_level (bool): if True, the function will consider as base elements the words. If False, the function will consider as base elements the entity. \n",
    "        E.g., if True, the original sentence is \"Yesterday morning I was so very happy and sad\", the ground truth is [\"yesterday morning\", \"so very happy\"] the model output is [\"morning\", \"happy and\"], the function will consider FP=2 (\"and\"); TP=1 (\"morning\", \"happy\"); FN=2 (\"Yesterday\", \"so\"). \n",
    "        If False, the function will consider FP=1 (\"happy and\"); TP=0; FN=2 (\"Yesterday morning\", \"so very happy\").\n",
    "        similarity_types: the list of similarity types to consider. Must contain elements in ['case', 'stop_words', 'subset', 'superset', 'leveshtein']\n",
    "\n",
    "        \"\"\"\n",
    "        # print('ORIGINAL model_response: ', model_response)\n",
    "        model_response = self._parse_json(model_response)\n",
    "        # print('GROUND TRUTH: ', ground_truth)\n",
    "        ground_truth = self._parse_json(ground_truth.replace('\\n', ''))\n",
    "        model_response = model_response[\"entities\"]\n",
    "        ground_truth = ground_truth[\"entities\"]\n",
    "        # print('PARSED ORIGINAL model_response: ', model_response)\n",
    "        if not similar_is_equal:\n",
    "            similarity_types = []\n",
    "\n",
    "        if words_level:\n",
    "            FP_sum = 0\n",
    "            FN_sum = 0\n",
    "            TP_sum = 0\n",
    "            identified_entities_list = []\n",
    "            for i, response_entity in enumerate(model_response):\n",
    "                entity_identified, FP, FN, TP= self.entity_in_ground_truth_list(response_entity, ground_truth, model_response, similar_is_equal_threshold, similarity_types)\n",
    "                FP_sum += FP\n",
    "                FN_sum += FN\n",
    "                TP_sum += TP\n",
    "                identified_entities_list.append(entity_identified)\n",
    "            FN_entities = set(ground_truth).difference(set(identified_entities_list))\n",
    "            FN_entities = [entity.split() for entity in FN_entities]\n",
    "            FN_entities = [item for row in FN_entities for item in row]\n",
    "            # print('FALSE NEGATIVES: ', FN_entities)\n",
    "            FN_sum += len(FN_entities)\n",
    "            #print('PARSED GROUND TRUTH: ', ground_truth, 'TP_sum:', TP_sum, 'FP_sum:', FP_sum, 'FN_sum:', FN_sum, '\\n\\n')\n",
    "            return [TP_sum, FP_sum, FN_sum]\n",
    "           \n",
    "        elif not words_level:\n",
    "            for i, response_entity in enumerate(model_response):\n",
    "                model_response[i], _, _, _= self.entity_in_ground_truth_list(response_entity, ground_truth, model_response, similar_is_equal_threshold, similarity_types)\n",
    "            #print('PARSED GROUND TRUTH: ', ground_truth)\n",
    "            #print('NEW model_response to calculate TP, FP, FN: ', model_response, '\\n\\n')\n",
    "\n",
    "            TP = len(set(model_response).intersection(set(ground_truth)))\n",
    "            FP = len(set(model_response).difference(set(ground_truth)))\n",
    "            FN = len(set(ground_truth).difference(set(model_response)))\n",
    "            # F1 = 2 * TP / (2 * TP + FN + FP)\n",
    "            return [TP, FP, FN]\n",
    "    \n",
    "    def generate_evaluation_table(self, similar_is_equal_threshold: int, words_level:bool, similarity_types:'list[str]') -> dict:\n",
    "        \"\"\"\n",
    "        Generate the evaluation table for the model output and the ground truth.\n",
    "\n",
    "        Args:\n",
    "        similar_is_equal_threshold (int): the threshold to consider the entities similar by the Leveshtein distance. The default value is 80. 0 is completely\n",
    "        different, 100 is the same. \n",
    "        words_level (bool): if True, the function will consider as base elements the words. If False, the function will consider as base elements the entity. \n",
    "        E.g., if True, the original sentence is \"Yesterday morning I was so very happy and sad\", the ground truth is [\"yesterday morning\", \"so very happy\"] the model output is [\"morning\", \"happy and\"], the function will consider FP=2 (\"and\"); TP=1 (\"morning\", \"happy\"); FN=2 (\"Yesterday\", \"so\"). \n",
    "        If False, the function will consider FP=1 (\"happy and\"); TP=0; FN=2 (\"Yesterday morning\", \"so very happy\").\n",
    "        similarity_types: the list of similarity types to consider. Must contain elements in ['case', 'stop_words', 'subset', 'superset', 'leveshtein']\n",
    "\n",
    "        return:\n",
    "        dict: the evaluation table\n",
    "        \"\"\"\n",
    "        metrics_list = []\n",
    "        for i, res in enumerate(self.data['model_output']):\n",
    "           #  if self.cleaner.verbose: print('res:', res)\n",
    "            metrics_list.append(self._extract_TP_FP_FN(res, self.data['ground_truth'][i], True, similar_is_equal_threshold, similarity_types, words_level))\n",
    "\n",
    "        metrics_dataframe = pd.DataFrame(metrics_list, columns=['TP', 'FP', 'FN'])\n",
    "        summary = metrics_dataframe.sum()\n",
    "        precision = summary['TP'] / (summary['TP'] + summary['FP'])\n",
    "        recall = summary['TP'] / (summary['TP'] + summary['FN'])\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        self.evaluation_table = {'evaluation': metrics_dataframe, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "        return {'evaluation': metrics_dataframe, 'precision':precision, 'recall':recall, 'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(logits, labels):\n",
    "    model_output_logits = logits.cpu().detach().float().numpy()    \n",
    "    predictions = np.argmax(model_output_logits, axis=1)\n",
    "    print(predictions)\n",
    "    print(len(predictions))\n",
    "    print(len(labels))\n",
    "    #labels = [labels*attn_mask for labels, attn_mask in zip(labels, attention_mask[:len(labels)])]\n",
    "    # predicted_labels = []\n",
    "    # for i in range(len(labels)):\n",
    "    #     print('pred: ', predictions[i], 'label: ', labels[i])\n",
    "    #     if labels[i] != -100:\n",
    "    #      predicted_labels.append(id2label[predictions[i]])\n",
    "    # print('labels predictions: ', predicted_labels)\n",
    "\n",
    "    predicted_labels = [id2label[prediction] for i, prediction in enumerate(predictions[:len(labels)]) if labels[i] != -100 ]\n",
    "    labels = [id2label[label] for label in labels if label != -100]\n",
    "    print('labels: ', labels)\n",
    "    print('predictions: ', predicted_labels)\n",
    "\n",
    "    results = seqeval.compute(predictions=predicted_labels, references=labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"soverall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their child acquired walking at the age of 14 months.\n",
      "{'input_ids': [1, 11275, 2278, 16692, 22049, 472, 278, 5046, 310, 29871, 29896, 29946, 7378, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 11275,  2278, 16692, 22049,   472,   278,  5046,   310, 29871,\n",
       "         29896, 29946,  7378, 29889,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2],\n",
       "        [    1,   349,   814,  8946, 10212,  7606, 11898,  5134,   263,  9736,\n",
       "           468,   417,  2109,  3233,   310, 29871, 29896, 29900,   330, 29914,\n",
       "         29881, 29931, 29892, 15284,  1026,  2302,   471,  4226, 29892, 10416,\n",
       "           318,  5638,   310, 29871, 29896, 29892, 29906,   330, 29914, 29880,\n",
       "           313, 29900, 29892, 29896, 29947, 29899, 29900, 29892, 29946, 29945,\n",
       "           330, 29914, 29931,   511,   322,   263,   907, 21203,   457,  3233,\n",
       "           310, 29871, 29953, 29947,   286, 29887, 29914, 29931,   313, 29955,\n",
       "         29899, 29896, 29941,   286, 29887, 29914, 29931,   467]],\n",
       "       device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data[0]['sentence'])\n",
    "print(tokenizer(train_data[0]['sentence']))\n",
    "encodeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 2 2 0 0 0 0 0 0 2 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n",
      "78\n",
      "14\n",
      "labels:  ['B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "predictions:  ['B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'O', 'O']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got a string but expected a list instead: 'B'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 20\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels: \u001b[39m\u001b[38;5;124m'\u001b[39m, labels)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions: \u001b[39m\u001b[38;5;124m'\u001b[39m, predicted_labels)\n\u001b[0;32m---> 20\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mseqeval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoverall_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     26\u001b[0m }\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/evaluate/module.py:450\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/evaluate/module.py:514\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(column) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 514\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enforce_nested_string_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselected_feature_format\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;241m.\u001b[39mencode_batch(batch)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/evaluate/module.py:761\u001b[0m, in \u001b[0;36mEvaluationModule._enforce_nested_string_type\u001b[0;34m(self, schema, obj)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;66;03m# schema.feature is not a dict\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mstr\u001b[39m):  \u001b[38;5;66;03m# don't interpret a string as a list\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot a string but expected a list instead: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Got a string but expected a list instead: 'B'"
     ]
    }
   ],
   "source": [
    "examples = [train_data['sentence'][15], train_data['sentence'][5]]\n",
    "input_sentences = examples\n",
    "encodeds = tokenizer(input_sentences, return_tensors=\"pt\", add_special_tokens=True, padding=True)\n",
    "model_inputs = encodeds.to('cuda')\n",
    "generated_ids = model(**model_inputs)\n",
    "compute_metrics(generated_ids['logits'][0],   train_data[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = [['O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'I'], ['O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'I', 'I', 'O', 'I', 'I', 'I'], ['I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'I', 'I', 'O', 'I', 'O', 'O', 'O', 'O', 'I', 'O', 'I', 'I', 'O', 'I', 'I', 'I', 'I', 'O', 'I', 'I', 'O', 'O', 'O', 'I', 'I', 'I', 'O', 'I', 'O', 'I', 'O', 'O', 'B', 'B', 'I', 'O'], ['I', 'I', 'I', 'I', 'O', 'I', 'I', 'B', 'O', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'O', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'I', 'I'], ['O', 'O', 'O', 'I', 'B', 'I', 'O', 'O', 'O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'O', 'O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O', 'O', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I'], ['I', 'O', 'O', 'I', 'I', 'O', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'I', 'B', 'O', 'O', 'O', 'I', 'I', 'I', 'B', 'B', 'B', 'I', 'O', 'I', 'O', 'O', 'O', 'I', 'O', 'O', 'O', 'I', 'I', 'I', 'I', 'I', 'I'], ['O', 'B', 'I', 'O', 'I', 'I', 'O', 'O', 'I', 'I', 'B', 'O', 'B', 'B', 'O', 'I', 'I', 'I', 'O', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'I', 'O', 'B', 'O', 'B', 'B', 'I', 'O', 'I', 'I'], ['O', 'I', 'O', 'I', 'O', 'O', 'O', 'O', 'I'], ['O', 'I', 'O', 'B', 'O', 'I', 'O', 'I', 'O', 'O', 'I', 'B', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'O', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'I', 'O', 'I', 'B', 'O', 'I', 'B', 'I', 'I', 'I'], ['B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'I', 'B', 'O', 'O', 'O', 'O', 'I', 'O', 'I', 'O', 'O', 'O', 'B', 'B', 'O', 'I', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I'], ['O', 'I', 'O', 'O', 'O', 'O', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'I', 'O', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'I', 'I'], ['B', 'B', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'O', 'I', 'I']]\n",
    "tl = [['O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I'], ['O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I'], ['B', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I'], ['B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I'], ['O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O'], ['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_': {'precision': 0.09701492537313433,\n",
       "  'recall': 0.15294117647058825,\n",
       "  'f1': 0.11872146118721462,\n",
       "  'number': 85},\n",
       " 'overall_precision': 0.09701492537313433,\n",
       " 'overall_recall': 0.15294117647058825,\n",
       " 'overall_f1': 0.11872146118721462,\n",
       " 'overall_accuracy': 0.5340699815837937}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqeval.compute(predictions=tp, references=tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2700,  2.4688,  0.4968],\n",
      "         [ 0.6313, -0.3750, -3.3027],\n",
      "         [-2.2402, -1.0303,  0.9199],\n",
      "         ...,\n",
      "         [ 0.2050,  0.9761, -0.6738],\n",
      "         [ 0.2134,  0.9766, -0.6865],\n",
      "         [ 0.2212,  0.9888, -0.7046]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 3.6992,  2.4766, -0.1819],\n",
      "         [ 2.0234, -0.4658, -0.5229],\n",
      "         ...,\n",
      "         [ 0.5977,  0.6646, -0.5928],\n",
      "         [ 0.6367,  0.6826, -0.5869],\n",
      "         [ 0.6914,  0.6958, -0.5732]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 1.3906,  1.1963, -2.3613],\n",
      "         [ 0.9155,  0.4763, -1.0371],\n",
      "         ...,\n",
      "         [ 0.2817,  2.0430, -0.4761],\n",
      "         [ 0.2883,  2.0684, -0.5220],\n",
      "         [ 0.3015,  2.0840, -0.5698]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 2.6270,  1.6318, -1.8018],\n",
      "         [ 2.4648, -1.9111, -1.4434],\n",
      "         ...,\n",
      "         [ 0.2754,  0.8252, -0.7539],\n",
      "         [ 0.2749,  0.8208, -0.7495],\n",
      "         [ 0.2856,  0.8423, -0.7407]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 0.9810,  0.1714, -1.5791],\n",
      "         [ 2.8066, -1.7266,  1.6250],\n",
      "         ...,\n",
      "         [-0.0678,  1.3525, -0.8960],\n",
      "         [-0.0761,  1.3555, -0.9077],\n",
      "         [-0.0784,  1.3330, -0.9321]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 1.3906,  1.1963, -2.3613],\n",
      "         [-0.4348, -2.4395,  2.3730],\n",
      "         ...,\n",
      "         [ 0.0235,  1.2832, -0.2129],\n",
      "         [ 0.0743,  1.3027, -0.2416],\n",
      "         [ 0.1052,  1.2988, -0.2288]]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "len(tp)\n",
    "\n",
    "batch = train_data.select(range(12))\n",
    "examples = batch['sentence']\n",
    "encodeds = tokenizer(examples, return_tensors=\"pt\", add_special_tokens=True, padding=True)\n",
    "model_inputs = encodeds.to('cuda')\n",
    "generated_ids = model(**model_inputs)\n",
    "print(generated_ids.logits)\n",
    "\n",
    "max_length = 169\n",
    "padded_labels = [sublist + [-100] * (max_length - len(sublist)) for sublist in batch['labels']]\n",
    "padded_labels = np.array(padded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGeneration():\n",
    "    def __init__(self, model, tokenizer, label2id):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "    \n",
    "    # def _create_prediction_list(self, model_output):\n",
    "    #     model_output_logits = model_output.logits.cpu().detach().float().numpy()\n",
    "    #     preds = np.argmax(model_output_logits, axis=2)\n",
    "    #     preds_list = []\n",
    "    #     for pred in preds:\n",
    "    #         preds_list.append([self.label2id[label] for label in pred])\n",
    "    #     return preds_list\n",
    "\n",
    "    def _generate_batch(self, input_sentences, tokenizer):\n",
    "        encodeds = tokenizer(input_sentences, return_tensors=\"pt\", add_special_tokens=True, padding=True)\n",
    "        model_inputs = encodeds.to('cuda')\n",
    "        generated_ids = self.model(**model_inputs)\n",
    "        # preds = self._create_prediction_list(generated_ids)\n",
    "        return generated_ids\n",
    "    \n",
    "    def _pad_labels(self, labels, max_length):\n",
    "        padded_labels = [sublist + [-100] * (max_length - len(sublist)) for sublist in labels]\n",
    "        return np.array(padded_labels)\n",
    "            \n",
    "\n",
    "    def _format_predictions_and_labels(self, generation_output, padded_labels):\n",
    "        predictions=generation_output.logits.cpu().detach().numpy()\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        print('predictions:\\n',predictions)\n",
    "        print('len(predictions[0]):\\n',len(predictions[0]))\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, padded_labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, padded_labels)\n",
    "        ]\n",
    "        return {'predictions': true_predictions, 'labels': true_labels}\n",
    "    \n",
    "    def generate(self, data, batch_size):\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        total_rows = len(data)\n",
    "        indexes = [i for i in range(len(data)) if i % batch_size == 0]\n",
    "        max_index = data.shape[0]\n",
    "\n",
    "        with tqdm(total=total_rows, desc=\"generation\") as pbar:\n",
    "            for i, idx in enumerate(indexes[:-1]):\n",
    "                indici = list(range(idx, indexes[i+1]))\n",
    "                max_length_sequence =   max(len(seq) for seq in data.select(indici)['labels'])\n",
    "                padded_labels = self._pad_labels(data.select(indici)['labels'], max_length_sequence)\n",
    "                generated_output = self._generate_batch(data.select(indici)['sentence'], tokenizer)\n",
    "                formatted_pred_label = self._format_predictions_and_labels(generated_output, padded_labels)\n",
    "                predictions.append(formatted_pred_label['predictions'])\n",
    "                labels.append(formatted_pred_label['labels'])\n",
    "            indici = list(range(indexes[len(indexes[:-1])], max_index))\n",
    "            max_length_sequence =   max(len(seq) for seq in data.select(indici)['labels'])\n",
    "            padded_labels = self._pad_labels(data.select(indici)['labels'], max_length_sequence)\n",
    "            generated_output = self._generate_batch(data.select(indici)['sentence'], tokenizer)\n",
    "            formatted_pred_label = self._format_predictions_and_labels(generated_output, padded_labels)\n",
    "            predictions.append(formatted_pred_label['predictions'])\n",
    "            labels.append(formatted_pred_label['labels'])\n",
    "            pbar.update(batch_size)\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        data = data.add_column('predictions', predictions)\n",
    "        data = data.add_column('ground_truth_labels', labels)\n",
    "\n",
    "\n",
    "    def add_output_column(self, data, model, tokenizer, batch_size:int) -> None:\n",
    "        \"\"\"\n",
    "        Adds a column with the response of the model to the actual query.\n",
    "        \n",
    "        params:\n",
    "        model: the model to use to generate the response\n",
    "        tokenizer: the tokenizer to use to generate the response\n",
    "        batch_size: the batch size to use to process the examples. Increasing this makes it faster but requires more GPU. Default is 8.\n",
    "        \"\"\"\n",
    "        responses_col = []\n",
    "        total_rows = len(data)\n",
    "        indexes = [i for i in range(len(data)) if i % batch_size == 0]\n",
    "        max_index = data.shape[0]\n",
    "\n",
    "        with tqdm(total=total_rows, desc=\"generating responses\") as pbar:\n",
    "            for i, idx in enumerate(indexes[:-1]):\n",
    "                indici = list(range(idx, indexes[i+1]))\n",
    "                tmp = self._generate_batch(data.select(indici)['sentence'], model, tokenizer)\n",
    "                responses_col.extend(tmp)\n",
    "                pbar.update(batch_size)\n",
    "            indici = list(range(indexes[len(indexes[:-1])], max_index))\n",
    "            tmp = self._generate_batch(data.select(indici)['sentence'], model, tokenizer)\n",
    "            responses_col.extend(tmp)\n",
    "            pbar.update(batch_size)\n",
    "        \n",
    "        data = data.add_column('model_responses', responses_col)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generation:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[1 0 2 1 1 0 0 0 0 1 2 0 2 2 1]\n",
      " [1 0 0 0 0 1 0 2 1 2 0 1 0 1 2]]\n",
      "len(predictions[0]):\n",
      " 15\n",
      "predictions:\n",
      " [[1 0 0 1 0 1 2 2 2 2 2 2 2 2 2 0 0 0 0 1 0 1 2 2 2 2 2 2 2 0 0 2 1 0 0 2\n",
      "  2 2 2 0 0 0 1 0 1 2 2 2 2 2 1 1]\n",
      " [1 0 2 1 0 1 0 1 2 2 1 0 0 0 0 0 1 2 1 2 1 0 1 2 2 2 2 2 2 2 2 2 0 0 0 0\n",
      "  0 0 1 1 0 0 0 0 0 0 0 0 0 2 2 2]]\n",
      "len(predictions[0]):\n",
      " 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generation:  33%|███▎      | 2/6 [00:01<00:02,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[1 1 1 0 1 0 0 1 1 2 2 1 2 2 2 2 0 2 2 2 0 0 0 0 1 0 1 0 0 0 2 1 1 0 0 1\n",
      "  1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 1 0 0 1 2 2 2 1 0 1 2 2 2 2 2 2 2 1 2 1 0 0 2 0 2 2 0 1 2 2\n",
      "  2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 2 2 2 2 2 2 0 0 0 0\n",
      "  0 2 2 2 0 0]]\n",
      "len(predictions[0]):\n",
      " 78\n",
      "[[['O', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'I', 'I'], ['O', 'O', 'O', 'O', 'B', 'O', 'I', 'B', 'I', 'O', 'B', 'O', 'B', 'I']], [['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'I', 'B', 'O', 'O', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'I', 'B', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'I']], [['B', 'B', 'O', 'B', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'I'], ['B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'O', 'O', 'I', 'O', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'I', 'O', 'O']]]\n",
      "[[['B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I']], [['O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I']], [['O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to concatenate on axis=1 because tables don't have the same number of rows",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gen \u001b[38;5;241m=\u001b[39m OutputGeneration(model, tokenizer, label2id)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[166], line 68\u001b[0m, in \u001b[0;36mOutputGeneration.generate\u001b[0;34m(self, data, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[0;32m---> 68\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39madd_column(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth_labels\u001b[39m\u001b[38;5;124m'\u001b[39m, labels)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:5635\u001b[0m, in \u001b[0;36mDataset.add_column\u001b[0;34m(self, name, column, new_fingerprint)\u001b[0m\n\u001b[1;32m   5633\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_indices() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   5634\u001b[0m \u001b[38;5;66;03m# Concatenate tables horizontally\u001b[39;00m\n\u001b[0;32m-> 5635\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_table\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5636\u001b[0m \u001b[38;5;66;03m# Update features\u001b[39;00m\n\u001b[1;32m   5637\u001b[0m info \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/table.py:1764\u001b[0m, in \u001b[0;36mconcat_tables\u001b[0;34m(tables, axis)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tables) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tables[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenationTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/table.py:1469\u001b[0m, in \u001b[0;36mConcatenationTable.from_tables\u001b[0;34m(cls, tables, axis)\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m   1468\u001b[0m     table_blocks \u001b[38;5;241m=\u001b[39m to_blocks(table)\n\u001b[0;32m-> 1469\u001b[0m     blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_extend_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_blocks(blocks)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/table.py:1461\u001b[0m, in \u001b[0;36mConcatenationTable.from_tables.<locals>._extend_blocks\u001b[0;34m(result, blocks, axis)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(blocks)\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;66;03m# We make sure each row_block have the same num_rows\u001b[39;00m\n\u001b[0;32m-> 1461\u001b[0m     result, blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_split_both_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(blocks):\n\u001b[1;32m   1463\u001b[0m         result[i]\u001b[38;5;241m.\u001b[39mextend(row_block)\n",
      "File \u001b[0;32m~/LS-LLaMA-E3C/.venv/lib/python3.11/site-packages/datasets/table.py:1451\u001b[0m, in \u001b[0;36mConcatenationTable.from_tables.<locals>._split_both_like\u001b[0;34m(result, blocks)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         new_blocks\u001b[38;5;241m.\u001b[39mappend(blocks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mor\u001b[39;00m blocks:\n\u001b[0;32m-> 1451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to concatenate on axis=1 because tables don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have the same number of rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_result, new_blocks\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to concatenate on axis=1 because tables don't have the same number of rows"
     ]
    }
   ],
   "source": [
    "gen = OutputGeneration(model, tokenizer, label2id)\n",
    "gen.generate(train_data.select(range(6)), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    print('predictions:\\n',predictions)\n",
    "    print('size pred:',predictions.shape)\n",
    "\n",
    "    print('labels:\\n',labels)\n",
    "    print('labels pred:',labels.shape)\n",
    "    predictions=predictions.cpu().detach().numpy()\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    print('predictions:\\n',predictions)\n",
    "    print('len(predictions[0]):\\n',len(predictions[0]))\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    print('true_predictions: ', true_predictions)\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    print('true_labels: ', true_labels)\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    print(results)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " tensor([[[-0.2700,  2.4688,  0.4968],\n",
      "         [ 0.6313, -0.3750, -3.3027],\n",
      "         [-2.2402, -1.0303,  0.9199],\n",
      "         ...,\n",
      "         [ 0.2050,  0.9761, -0.6738],\n",
      "         [ 0.2134,  0.9766, -0.6865],\n",
      "         [ 0.2212,  0.9888, -0.7046]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 3.6992,  2.4766, -0.1819],\n",
      "         [ 2.0234, -0.4658, -0.5229],\n",
      "         ...,\n",
      "         [ 0.5977,  0.6646, -0.5928],\n",
      "         [ 0.6367,  0.6826, -0.5869],\n",
      "         [ 0.6914,  0.6958, -0.5732]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 1.3906,  1.1963, -2.3613],\n",
      "         [ 0.9155,  0.4763, -1.0371],\n",
      "         ...,\n",
      "         [ 0.2817,  2.0430, -0.4761],\n",
      "         [ 0.2883,  2.0684, -0.5220],\n",
      "         [ 0.3015,  2.0840, -0.5698]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 2.6270,  1.6318, -1.8018],\n",
      "         [ 2.4648, -1.9111, -1.4434],\n",
      "         ...,\n",
      "         [ 0.2754,  0.8252, -0.7539],\n",
      "         [ 0.2749,  0.8208, -0.7495],\n",
      "         [ 0.2856,  0.8423, -0.7407]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 0.9810,  0.1714, -1.5791],\n",
      "         [ 2.8066, -1.7266,  1.6250],\n",
      "         ...,\n",
      "         [-0.0678,  1.3525, -0.8960],\n",
      "         [-0.0761,  1.3555, -0.9077],\n",
      "         [-0.0784,  1.3330, -0.9321]],\n",
      "\n",
      "        [[-0.2700,  2.4688,  0.4968],\n",
      "         [ 1.3906,  1.1963, -2.3613],\n",
      "         [-0.4348, -2.4395,  2.3730],\n",
      "         ...,\n",
      "         [ 0.0235,  1.2832, -0.2129],\n",
      "         [ 0.0743,  1.3027, -0.2416],\n",
      "         [ 0.1052,  1.2988, -0.2288]]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "size pred: torch.Size([12, 169, 3])\n",
      "labels:\n",
      " [[-100    1    2 ... -100 -100 -100]\n",
      " [-100    0    0 ... -100 -100 -100]\n",
      " [-100    0    1 ... -100 -100 -100]\n",
      " ...\n",
      " [-100    0    0 ... -100 -100 -100]\n",
      " [-100    0    0 ... -100 -100 -100]\n",
      " [-100    1    2 ... -100 -100 -100]]\n",
      "labels pred: (12, 169)\n",
      "predictions:\n",
      " [[1 0 2 ... 1 1 1]\n",
      " [1 0 0 ... 1 1 1]\n",
      " [1 0 0 ... 1 1 1]\n",
      " ...\n",
      " [1 0 0 ... 1 1 1]\n",
      " [1 0 0 ... 1 1 1]\n",
      " [1 0 2 ... 1 1 1]]\n",
      "len(predictions[0]):\n",
      " 169\n",
      "true_predictions:  [['O', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'I', 'I'], ['O', 'O', 'O', 'O', 'B', 'O', 'I', 'B', 'I', 'O', 'B', 'O', 'B', 'I'], ['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'I', 'B', 'O', 'O', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'I', 'B', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'I'], ['B', 'B', 'O', 'B', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'I'], ['B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'O', 'O', 'I', 'O', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'I', 'I', 'I', 'O', 'O'], ['O', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'I', 'I', 'O', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'I', 'O', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'I', 'O', 'O', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'O'], ['O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I'], ['B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I'], ['O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'I'], ['O', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I']]\n",
      "true_labels:  [['B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I'], ['O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I'], ['O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I'], ['O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I'], ['B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I'], ['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I'], ['O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'I'], ['B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I']]\n",
      "{'_': {'precision': 0.5606060606060606, 'recall': 0.8314606741573034, 'f1': 0.669683257918552, 'number': 89}, 'overall_precision': 0.5606060606060606, 'overall_recall': 0.8314606741573034, 'overall_f1': 0.669683257918552, 'overall_accuracy': 0.8479087452471483}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5606060606060606,\n",
       " 'recall': 0.8314606741573034,\n",
       " 'f1': 0.669683257918552,\n",
       " 'accuracy': 0.8479087452471483}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics((generated_ids.logits, padded_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
