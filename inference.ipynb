{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010af3aafc694a608ca9394433daccd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from dotenv import dotenv_values\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from utils import DataPreprocessor, DatasetFormatConverter\n",
    "from src.billm import LlamaForTokenClassification\n",
    "\n",
    "\n",
    "WANDB_KEY = dotenv_values(\".env.base\")['WANDB_KEY']\n",
    "BASE_MODEL_CHECKPOINT=\"meta-llama/Llama-2-7b-hf\"\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT,\n",
    "                                          token =HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "DATASET_CHEKPOINT=\"ferrazzipietro/e3c-sentences\" \n",
    "TRAIN_LAYER=\"en.layer1\"\n",
    "offset=False\n",
    "instruction_on_response_format='Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}].'# 'Return the result in a json format.'\n",
    "simplest_prompt=False\n",
    "dataset_text_field=\"prompt\"\n",
    "preprocessor = DataPreprocessor()\n",
    "dataset = load_dataset(DATASET_CHEKPOINT) #download_mode=\"force_redownload\"\n",
    "dataset = dataset[TRAIN_LAYER]\n",
    "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "dataset_format_converter_obj = DatasetFormatConverter(dataset)\n",
    "dataset_format_converter_obj.apply()\n",
    "ds = dataset_format_converter_obj.dataset\n",
    "label2id = dataset_format_converter_obj.label2id\n",
    "id2label = dataset_format_converter_obj.get_id2label()\n",
    "label_list = dataset_format_converter_obj.get_label_list()\n",
    "dataset_format_converter_obj.set_tokenizer(tokenizer)\n",
    "dataset_format_converter_obj.set_max_seq_length(280)\n",
    "\n",
    "adapters = \"ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.layer1_NoQuant_16_32_0.01_2_0.0002\"\n",
    "peft_config = PeftConfig.from_pretrained(adapters)\n",
    "base_model = LlamaForTokenClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, \n",
    "    num_labels=len(label2id), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    token = HF_TOKEN,\n",
    "    device_map = 'cuda:0',\n",
    "    load_in_4bit = True,\n",
    "    # torch_dtype = torch.bfloat16,\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache'\n",
    "    )#.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForTokenClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=4096, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3369353219"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in base_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: model.embed_tokens.weight | Number of parameters: 131072000\n",
      "Layer: model.layers.0.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.0.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.1.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.1.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.2.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.2.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.3.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.3.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.4.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.4.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.5.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.5.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.6.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.6.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.7.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.7.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.8.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.8.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.9.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.9.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.10.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.10.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.11.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.11.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.12.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.12.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.13.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.13.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.14.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.14.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.15.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.15.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.16.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.16.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.17.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.17.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.18.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.18.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.19.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.19.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.20.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.20.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.21.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.21.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.22.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.22.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.23.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.23.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.24.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.24.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.25.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.25.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.26.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.26.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.27.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.27.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.28.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.28.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.29.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.29.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.30.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.30.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.31.input_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.layers.31.post_attention_layernorm.weight | Number of parameters: 4096\n",
      "Layer: model.norm.weight | Number of parameters: 4096\n",
      "Layer: classifier.weight | Number of parameters: 12288\n",
      "Layer: classifier.bias | Number of parameters: 3\n"
     ]
    }
   ],
   "source": [
    "def print_layer_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "\n",
    "print_layer_params(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a428afd1c67441bbf35d2028c65ee27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, \n",
    "    device_map = 'cuda:1',\n",
    "    load_in_4bit = True,\n",
    "    # torch_dtype = torch.bfloat16,\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache'\n",
    "    )#.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500412928"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForTokenClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=4096, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
