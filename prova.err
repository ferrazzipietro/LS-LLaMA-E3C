2024-05-06 09:13:42.383487: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-06 09:13:44.554176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-06 09:13:48.648083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
INFO:BiLLM:Here is the Bi-MistralModel! BiLLM_START_INDEX=0
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 29.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:03<00:00, 31.69s/it]
Some weights of MistralForTokenClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The model 'MistralForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'MT5ForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PhiForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'T5ForTokenClassification', 'UMT5ForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].
  0%|          | 0/56 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
  2%|▏         | 1/56 [10:33<9:40:35, 633.37s/it] 23%|██▎       | 13/56 [13:21<34:31, 48.17s/it]   45%|████▍     | 25/56 [16:13<14:50, 28.71s/it] 66%|██████▌   | 37/56 [23:10<09:57, 31.44s/it] 88%|████████▊ | 49/56 [25:51<02:50, 24.37s/it]61it [28:42, 20.75s/it]                        73it [32:17, 19.79s/it]85it [35:08, 17.99s/it]97it [38:25, 17.48s/it]109it [43:06, 19.33s/it]121it [45:57, 17.77s/it]133it [48:23, 16.06s/it]145it [51:00, 15.15s/it]157it [54:24, 15.70s/it]169it [1:03:03, 24.03s/it]181it [1:05:51, 21.00s/it]193it [1:08:42, 18.97s/it]205it [1:13:52, 21.03s/it]217it [1:18:11, 21.20s/it]229it [1:22:45, 21.68s/it]241it [1:24:43, 18.15s/it]253it [1:30:01, 20.63s/it]265it [1:33:06, 19.07s/it]277it [1:36:19, 18.17s/it]289it [1:43:27, 23.42s/it]301it [1:46:58, 21.67s/it]313it [1:50:25, 20.35s/it]325it [1:58:44, 26.73s/it]337it [2:02:34, 24.44s/it]349it [2:05:17, 21.20s/it]361it [2:09:33, 21.23s/it]373it [2:12:46, 19.68s/it]385it [2:16:10, 18.88s/it]397it [2:18:30, 16.73s/it]409it [2:22:03, 17.04s/it]421it [2:24:38, 15.80s/it]433it [2:28:04, 16.19s/it]445it [2:32:47, 18.43s/it]457it [2:36:07, 17.89s/it]469it [2:38:39, 16.31s/it]481it [2:42:26, 17.10s/it]493it [2:46:25, 17.95s/it]505it [2:51:48, 20.65s/it]517it [2:54:33, 18.58s/it]529it [3:08:15, 33.54s/it]541it [3:11:18, 28.06s/it]553it [3:14:44, 24.80s/it]565it [3:17:19, 21.23s/it]577it [3:26:49, 29.10s/it]589it [3:29:47, 24.82s/it]601it [3:33:31, 22.97s/it]613it [3:37:44, 22.41s/it]625it [3:42:10, 22.35s/it]637it [3:46:32, 22.20s/it]649it [3:50:48, 21.93s/it]slurmstepd-vgpu5-0: error: *** JOB 802982 ON vgpu5-0 CANCELLED AT 2024-05-06T13:07:01 ***
