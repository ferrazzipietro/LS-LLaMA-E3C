{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>8850</td>\n",
       "      <td>2675</td>\n",
       "      <td>1526</td>\n",
       "      <td>0.767896</td>\n",
       "      <td>0.852930</td>\n",
       "      <td>0.808182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>7481</td>\n",
       "      <td>4164</td>\n",
       "      <td>2256</td>\n",
       "      <td>0.642422</td>\n",
       "      <td>0.768306</td>\n",
       "      <td>0.699747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             dataset    TP    FP    FN  \\\n",
       "0  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  8850  2675  1526   \n",
       "1  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  7481  4164  2256   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.767896  0.852930  0.808182  \n",
       "1   0.642422  0.768306  0.699747  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.layer1_NoQuant_16_32_0.05_2_0.0002_5Epochs'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/evaluation_table5Epochs.csv')\n",
    "display(data.head(2))\n",
    "data['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"adapters_{config.TRAIN_LAYER}_{nbits}_{r}_{lora_alpha}_{lora_dropout}_{gradient_accumulation_steps}_{learning_rate}_nEpochs{training_params.num_train_epochs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>model_type</th>\n",
       "      <th>training_config</th>\n",
       "      <th>layer</th>\n",
       "      <th>quantization</th>\n",
       "      <th>r</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>lora_dropout</th>\n",
       "      <th>gradient_accumulation_steps</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>run_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>8850</td>\n",
       "      <td>2675</td>\n",
       "      <td>1526</td>\n",
       "      <td>0.767896</td>\n",
       "      <td>0.852930</td>\n",
       "      <td>0.808182</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_32_0.05_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>7481</td>\n",
       "      <td>4164</td>\n",
       "      <td>2256</td>\n",
       "      <td>0.642422</td>\n",
       "      <td>0.768306</td>\n",
       "      <td>0.699747</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_32_0.05_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>6863</td>\n",
       "      <td>5934</td>\n",
       "      <td>1542</td>\n",
       "      <td>0.536298</td>\n",
       "      <td>0.816538</td>\n",
       "      <td>0.647392</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_32_0.05_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>6811</td>\n",
       "      <td>7917</td>\n",
       "      <td>1314</td>\n",
       "      <td>0.462452</td>\n",
       "      <td>0.838277</td>\n",
       "      <td>0.596071</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_32_0.01_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>6941</td>\n",
       "      <td>8447</td>\n",
       "      <td>1066</td>\n",
       "      <td>0.451066</td>\n",
       "      <td>0.866866</td>\n",
       "      <td>0.593375</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_32_0.01_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>231</td>\n",
       "      <td>642</td>\n",
       "      <td>10083</td>\n",
       "      <td>0.264605</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.041298</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_32_0.01_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>5914</td>\n",
       "      <td>7487</td>\n",
       "      <td>2529</td>\n",
       "      <td>0.441310</td>\n",
       "      <td>0.700462</td>\n",
       "      <td>0.541476</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_64_0.05_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>4649</td>\n",
       "      <td>6286</td>\n",
       "      <td>4185</td>\n",
       "      <td>0.425149</td>\n",
       "      <td>0.526262</td>\n",
       "      <td>0.470332</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_64_0.05_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>575</td>\n",
       "      <td>1589</td>\n",
       "      <td>9228</td>\n",
       "      <td>0.265712</td>\n",
       "      <td>0.058656</td>\n",
       "      <td>0.096098</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_64_0.05_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>4445</td>\n",
       "      <td>6222</td>\n",
       "      <td>4333</td>\n",
       "      <td>0.416706</td>\n",
       "      <td>0.506380</td>\n",
       "      <td>0.457187</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_64_0.01_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>4141</td>\n",
       "      <td>5650</td>\n",
       "      <td>4835</td>\n",
       "      <td>0.422939</td>\n",
       "      <td>0.461341</td>\n",
       "      <td>0.441307</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_64_0.01_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>741</td>\n",
       "      <td>2121</td>\n",
       "      <td>8662</td>\n",
       "      <td>0.258910</td>\n",
       "      <td>0.078805</td>\n",
       "      <td>0.120832</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_16_64_0.01_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>3723</td>\n",
       "      <td>5024</td>\n",
       "      <td>5530</td>\n",
       "      <td>0.425632</td>\n",
       "      <td>0.402356</td>\n",
       "      <td>0.413667</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_32_0.05_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>5054</td>\n",
       "      <td>6947</td>\n",
       "      <td>3531</td>\n",
       "      <td>0.421132</td>\n",
       "      <td>0.588701</td>\n",
       "      <td>0.491013</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_32_0.05_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1240</td>\n",
       "      <td>3288</td>\n",
       "      <td>7106</td>\n",
       "      <td>0.273852</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>0.192636</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_32_0.05_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>3619</td>\n",
       "      <td>4737</td>\n",
       "      <td>5708</td>\n",
       "      <td>0.433102</td>\n",
       "      <td>0.388013</td>\n",
       "      <td>0.409320</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_32_0.01_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>5526</td>\n",
       "      <td>7472</td>\n",
       "      <td>2884</td>\n",
       "      <td>0.425142</td>\n",
       "      <td>0.657075</td>\n",
       "      <td>0.516256</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_32_0.01_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1231</td>\n",
       "      <td>3444</td>\n",
       "      <td>6934</td>\n",
       "      <td>0.263316</td>\n",
       "      <td>0.150765</td>\n",
       "      <td>0.191745</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_32_0.01_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>2991</td>\n",
       "      <td>3929</td>\n",
       "      <td>6516</td>\n",
       "      <td>0.432225</td>\n",
       "      <td>0.314610</td>\n",
       "      <td>0.364157</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_64_0.05_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>2727</td>\n",
       "      <td>5597</td>\n",
       "      <td>4721</td>\n",
       "      <td>0.327607</td>\n",
       "      <td>0.366139</td>\n",
       "      <td>0.345803</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_64_0.05_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1369</td>\n",
       "      <td>3894</td>\n",
       "      <td>6532</td>\n",
       "      <td>0.260118</td>\n",
       "      <td>0.173269</td>\n",
       "      <td>0.207991</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_64_0.05_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1879</td>\n",
       "      <td>3862</td>\n",
       "      <td>6563</td>\n",
       "      <td>0.327295</td>\n",
       "      <td>0.222578</td>\n",
       "      <td>0.264965</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_64_0.01_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>3069</td>\n",
       "      <td>5917</td>\n",
       "      <td>4287</td>\n",
       "      <td>0.341531</td>\n",
       "      <td>0.417210</td>\n",
       "      <td>0.375597</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_64_0.01_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1349</td>\n",
       "      <td>3861</td>\n",
       "      <td>6583</td>\n",
       "      <td>0.258925</td>\n",
       "      <td>0.170071</td>\n",
       "      <td>0.205296</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_32_64_0.01_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1423</td>\n",
       "      <td>3843</td>\n",
       "      <td>6589</td>\n",
       "      <td>0.270224</td>\n",
       "      <td>0.177609</td>\n",
       "      <td>0.214340</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_32_0.05_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>2932</td>\n",
       "      <td>5626</td>\n",
       "      <td>4487</td>\n",
       "      <td>0.342603</td>\n",
       "      <td>0.395202</td>\n",
       "      <td>0.367028</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_32_0.05_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1359</td>\n",
       "      <td>3869</td>\n",
       "      <td>6574</td>\n",
       "      <td>0.259946</td>\n",
       "      <td>0.171310</td>\n",
       "      <td>0.206519</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_32_0.05_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1440</td>\n",
       "      <td>3903</td>\n",
       "      <td>6500</td>\n",
       "      <td>0.269512</td>\n",
       "      <td>0.181360</td>\n",
       "      <td>0.216818</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_32_0.01_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>3046</td>\n",
       "      <td>5796</td>\n",
       "      <td>4262</td>\n",
       "      <td>0.344492</td>\n",
       "      <td>0.416804</td>\n",
       "      <td>0.377214</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_32_0.01_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1403</td>\n",
       "      <td>3952</td>\n",
       "      <td>6429</td>\n",
       "      <td>0.261998</td>\n",
       "      <td>0.179137</td>\n",
       "      <td>0.212785</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_32_0.01_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1540</td>\n",
       "      <td>4137</td>\n",
       "      <td>6177</td>\n",
       "      <td>0.271270</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.229954</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_64_0.05_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>2918</td>\n",
       "      <td>5735</td>\n",
       "      <td>4294</td>\n",
       "      <td>0.337224</td>\n",
       "      <td>0.404603</td>\n",
       "      <td>0.367854</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_64_0.05_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1481</td>\n",
       "      <td>4138</td>\n",
       "      <td>6172</td>\n",
       "      <td>0.263570</td>\n",
       "      <td>0.193519</td>\n",
       "      <td>0.223177</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_64_0.05_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1632</td>\n",
       "      <td>4381</td>\n",
       "      <td>5787</td>\n",
       "      <td>0.271412</td>\n",
       "      <td>0.219976</td>\n",
       "      <td>0.243002</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_64_0.01_2_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>2644</td>\n",
       "      <td>5714</td>\n",
       "      <td>4350</td>\n",
       "      <td>0.316344</td>\n",
       "      <td>0.378038</td>\n",
       "      <td>0.344450</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_64_0.01_4_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...</td>\n",
       "      <td>1618</td>\n",
       "      <td>4454</td>\n",
       "      <td>5674</td>\n",
       "      <td>0.266469</td>\n",
       "      <td>0.221887</td>\n",
       "      <td>0.242143</td>\n",
       "      <td>Llama-2-7b-hf</td>\n",
       "      <td>en.layer1_NoQuant_64_64_0.01_8_0.0002_5Epochs</td>\n",
       "      <td>en.layer1</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5Epochs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dataset    TP    FP     FN  \\\n",
       "0   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  8850  2675   1526   \n",
       "1   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  7481  4164   2256   \n",
       "2   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  6863  5934   1542   \n",
       "3   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  6811  7917   1314   \n",
       "4   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  6941  8447   1066   \n",
       "5   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...   231   642  10083   \n",
       "6   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  5914  7487   2529   \n",
       "7   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  4649  6286   4185   \n",
       "8   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...   575  1589   9228   \n",
       "9   ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  4445  6222   4333   \n",
       "10  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  4141  5650   4835   \n",
       "11  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...   741  2121   8662   \n",
       "12  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  3723  5024   5530   \n",
       "13  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  5054  6947   3531   \n",
       "14  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1240  3288   7106   \n",
       "15  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  3619  4737   5708   \n",
       "16  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  5526  7472   2884   \n",
       "17  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1231  3444   6934   \n",
       "18  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  2991  3929   6516   \n",
       "19  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  2727  5597   4721   \n",
       "20  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1369  3894   6532   \n",
       "21  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1879  3862   6563   \n",
       "22  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  3069  5917   4287   \n",
       "23  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1349  3861   6583   \n",
       "24  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1423  3843   6589   \n",
       "25  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  2932  5626   4487   \n",
       "26  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1359  3869   6574   \n",
       "27  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1440  3903   6500   \n",
       "28  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  3046  5796   4262   \n",
       "29  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1403  3952   6429   \n",
       "30  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1540  4137   6177   \n",
       "31  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  2918  5735   4294   \n",
       "32  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1481  4138   6172   \n",
       "33  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1632  4381   5787   \n",
       "34  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  2644  5714   4350   \n",
       "35  ferrazzipietro/LS_Llama-2-7b-hf_adapters_en.la...  1618  4454   5674   \n",
       "\n",
       "    precision    recall        f1     model_type  \\\n",
       "0    0.767896  0.852930  0.808182  Llama-2-7b-hf   \n",
       "1    0.642422  0.768306  0.699747  Llama-2-7b-hf   \n",
       "2    0.536298  0.816538  0.647392  Llama-2-7b-hf   \n",
       "3    0.462452  0.838277  0.596071  Llama-2-7b-hf   \n",
       "4    0.451066  0.866866  0.593375  Llama-2-7b-hf   \n",
       "5    0.264605  0.022397  0.041298  Llama-2-7b-hf   \n",
       "6    0.441310  0.700462  0.541476  Llama-2-7b-hf   \n",
       "7    0.425149  0.526262  0.470332  Llama-2-7b-hf   \n",
       "8    0.265712  0.058656  0.096098  Llama-2-7b-hf   \n",
       "9    0.416706  0.506380  0.457187  Llama-2-7b-hf   \n",
       "10   0.422939  0.461341  0.441307  Llama-2-7b-hf   \n",
       "11   0.258910  0.078805  0.120832  Llama-2-7b-hf   \n",
       "12   0.425632  0.402356  0.413667  Llama-2-7b-hf   \n",
       "13   0.421132  0.588701  0.491013  Llama-2-7b-hf   \n",
       "14   0.273852  0.148574  0.192636  Llama-2-7b-hf   \n",
       "15   0.433102  0.388013  0.409320  Llama-2-7b-hf   \n",
       "16   0.425142  0.657075  0.516256  Llama-2-7b-hf   \n",
       "17   0.263316  0.150765  0.191745  Llama-2-7b-hf   \n",
       "18   0.432225  0.314610  0.364157  Llama-2-7b-hf   \n",
       "19   0.327607  0.366139  0.345803  Llama-2-7b-hf   \n",
       "20   0.260118  0.173269  0.207991  Llama-2-7b-hf   \n",
       "21   0.327295  0.222578  0.264965  Llama-2-7b-hf   \n",
       "22   0.341531  0.417210  0.375597  Llama-2-7b-hf   \n",
       "23   0.258925  0.170071  0.205296  Llama-2-7b-hf   \n",
       "24   0.270224  0.177609  0.214340  Llama-2-7b-hf   \n",
       "25   0.342603  0.395202  0.367028  Llama-2-7b-hf   \n",
       "26   0.259946  0.171310  0.206519  Llama-2-7b-hf   \n",
       "27   0.269512  0.181360  0.216818  Llama-2-7b-hf   \n",
       "28   0.344492  0.416804  0.377214  Llama-2-7b-hf   \n",
       "29   0.261998  0.179137  0.212785  Llama-2-7b-hf   \n",
       "30   0.271270  0.199559  0.229954  Llama-2-7b-hf   \n",
       "31   0.337224  0.404603  0.367854  Llama-2-7b-hf   \n",
       "32   0.263570  0.193519  0.223177  Llama-2-7b-hf   \n",
       "33   0.271412  0.219976  0.243002  Llama-2-7b-hf   \n",
       "34   0.316344  0.378038  0.344450  Llama-2-7b-hf   \n",
       "35   0.266469  0.221887  0.242143  Llama-2-7b-hf   \n",
       "\n",
       "                                  training_config      layer quantization   r  \\\n",
       "0   en.layer1_NoQuant_16_32_0.05_2_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "1   en.layer1_NoQuant_16_32_0.05_4_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "2   en.layer1_NoQuant_16_32_0.05_8_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "3   en.layer1_NoQuant_16_32_0.01_2_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "4   en.layer1_NoQuant_16_32_0.01_4_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "5   en.layer1_NoQuant_16_32_0.01_8_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "6   en.layer1_NoQuant_16_64_0.05_2_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "7   en.layer1_NoQuant_16_64_0.05_4_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "8   en.layer1_NoQuant_16_64_0.05_8_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "9   en.layer1_NoQuant_16_64_0.01_2_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "10  en.layer1_NoQuant_16_64_0.01_4_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "11  en.layer1_NoQuant_16_64_0.01_8_0.0002_5Epochs  en.layer1      NoQuant  16   \n",
       "12  en.layer1_NoQuant_32_32_0.05_2_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "13  en.layer1_NoQuant_32_32_0.05_4_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "14  en.layer1_NoQuant_32_32_0.05_8_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "15  en.layer1_NoQuant_32_32_0.01_2_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "16  en.layer1_NoQuant_32_32_0.01_4_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "17  en.layer1_NoQuant_32_32_0.01_8_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "18  en.layer1_NoQuant_32_64_0.05_2_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "19  en.layer1_NoQuant_32_64_0.05_4_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "20  en.layer1_NoQuant_32_64_0.05_8_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "21  en.layer1_NoQuant_32_64_0.01_2_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "22  en.layer1_NoQuant_32_64_0.01_4_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "23  en.layer1_NoQuant_32_64_0.01_8_0.0002_5Epochs  en.layer1      NoQuant  32   \n",
       "24  en.layer1_NoQuant_64_32_0.05_2_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "25  en.layer1_NoQuant_64_32_0.05_4_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "26  en.layer1_NoQuant_64_32_0.05_8_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "27  en.layer1_NoQuant_64_32_0.01_2_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "28  en.layer1_NoQuant_64_32_0.01_4_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "29  en.layer1_NoQuant_64_32_0.01_8_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "30  en.layer1_NoQuant_64_64_0.05_2_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "31  en.layer1_NoQuant_64_64_0.05_4_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "32  en.layer1_NoQuant_64_64_0.05_8_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "33  en.layer1_NoQuant_64_64_0.01_2_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "34  en.layer1_NoQuant_64_64_0.01_4_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "35  en.layer1_NoQuant_64_64_0.01_8_0.0002_5Epochs  en.layer1      NoQuant  64   \n",
       "\n",
       "   lora_alpha lora_dropout gradient_accumulation_steps learning_rate run_type  \n",
       "0          32         0.05                           2        0.0002  5Epochs  \n",
       "1          32         0.05                           4        0.0002  5Epochs  \n",
       "2          32         0.05                           8        0.0002  5Epochs  \n",
       "3          32         0.01                           2        0.0002  5Epochs  \n",
       "4          32         0.01                           4        0.0002  5Epochs  \n",
       "5          32         0.01                           8        0.0002  5Epochs  \n",
       "6          64         0.05                           2        0.0002  5Epochs  \n",
       "7          64         0.05                           4        0.0002  5Epochs  \n",
       "8          64         0.05                           8        0.0002  5Epochs  \n",
       "9          64         0.01                           2        0.0002  5Epochs  \n",
       "10         64         0.01                           4        0.0002  5Epochs  \n",
       "11         64         0.01                           8        0.0002  5Epochs  \n",
       "12         32         0.05                           2        0.0002  5Epochs  \n",
       "13         32         0.05                           4        0.0002  5Epochs  \n",
       "14         32         0.05                           8        0.0002  5Epochs  \n",
       "15         32         0.01                           2        0.0002  5Epochs  \n",
       "16         32         0.01                           4        0.0002  5Epochs  \n",
       "17         32         0.01                           8        0.0002  5Epochs  \n",
       "18         64         0.05                           2        0.0002  5Epochs  \n",
       "19         64         0.05                           4        0.0002  5Epochs  \n",
       "20         64         0.05                           8        0.0002  5Epochs  \n",
       "21         64         0.01                           2        0.0002  5Epochs  \n",
       "22         64         0.01                           4        0.0002  5Epochs  \n",
       "23         64         0.01                           8        0.0002  5Epochs  \n",
       "24         32         0.05                           2        0.0002  5Epochs  \n",
       "25         32         0.05                           4        0.0002  5Epochs  \n",
       "26         32         0.05                           8        0.0002  5Epochs  \n",
       "27         32         0.01                           2        0.0002  5Epochs  \n",
       "28         32         0.01                           4        0.0002  5Epochs  \n",
       "29         32         0.01                           8        0.0002  5Epochs  \n",
       "30         64         0.05                           2        0.0002  5Epochs  \n",
       "31         64         0.05                           4        0.0002  5Epochs  \n",
       "32         64         0.05                           8        0.0002  5Epochs  \n",
       "33         64         0.01                           2        0.0002  5Epochs  \n",
       "34         64         0.01                           4        0.0002  5Epochs  \n",
       "35         64         0.01                           8        0.0002  5Epochs  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_params_from_file_name(df: pd.DataFrame):\n",
    "    df['model_type'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[1]))\n",
    "    df['training_config'] = df['dataset'].apply(lambda x: str(x.split('adapters_')[1]))\n",
    "    df['layer'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[3]))\n",
    "    df['quantization'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[4]))\n",
    "    df['quantization'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[4]))\n",
    "    df['r'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[5]))\n",
    "    df['lora_alpha'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[6]))\n",
    "    df['lora_dropout'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[7]))\n",
    "    df['gradient_accumulation_steps'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[8]))\n",
    "    df['learning_rate'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[9]))\n",
    "    df['run_type'] = df['dataset'].apply(lambda x: str(x.split('/')[1].split('_')[10]))\n",
    "    return df\n",
    "\n",
    "data = extract_params_from_file_name(data)\n",
    "data.to_csv('data/evaluation_table5Epochs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
