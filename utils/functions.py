import importlib
from dotenv import dotenv_values

HF_TOKEN = dotenv_values(".env.base")['HF_TOKEN']
HF_TOKEN_WRITE = dotenv_values(".env.base")['HF_TOKEN_WRITE']

def generate_adapters_list(log_run_name: str, appendix:str, training_type:str='') -> 'list[str]':
    """
    Given a run of several configurations for the qlora fine-tuning, this function returns the list of the adapters generated by the run.
    """
    adapters_list = []
    module_name = f"log.{log_run_name}"
    models_params = importlib.import_module(module_name)
    
    model_name = models_params.model_name
    quantization = models_params.quantization
    load_in_4bit_list = models_params.load_in_4bit
    bnb_4bit_quant_type_list = models_params.bnb_4bit_quant_type
    bnb_4bit_compute_dtype_list = models_params.bnb_4bit_compute_dtype
    llm_int8_threshold_list = models_params.llm_int8_threshold
    r_list = models_params.r
    lora_alpha_list = models_params.lora_alpha
    lora_dropout_list = models_params.lora_dropout
    gradient_accumulation_steps_list = models_params.gradient_accumulation_steps
    learning_rate_list = models_params.learning_rate
    
    for model_loading_params_idx in range(len(load_in_4bit_list)):
        load_in_4bit = load_in_4bit_list[model_loading_params_idx]
        load_in_8bit = not load_in_4bit
        bnb_4bit_quant_type = bnb_4bit_quant_type_list[model_loading_params_idx]
        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype_list[model_loading_params_idx]
        llm_int8_threshold = llm_int8_threshold_list[model_loading_params_idx]
        for r in r_list:
            for lora_alpha in lora_alpha_list:
                for lora_dropout in lora_dropout_list:
                    for gradient_accumulation_steps in gradient_accumulation_steps_list:
                        for learning_rate in learning_rate_list:
                            nbits = 4
                            if load_in_8bit:
                                nbits = 8   
                            if not quantization:
                                nbits = 'NoQuant'
                            extra_str_cl = ""
                            try:
                                if models_params.clent:
                                    extra_str_cl += "_clent"
                            except:
                                pass
                       
                            if training_type == 'NoLora':
                                ADAPTERS_CHECKPOINT = f"ferrazzipietro/noLoraLS_{model_name}_adapters_{models_params.TRAIN_LAYER}_{nbits}_{gradient_accumulation_steps}_{learning_rate}_{appendix}{extra_str_cl}"
                            elif training_type == 'unmasked':
                                ADAPTERS_CHECKPOINT = f"ferrazzipietro/unmaskedLS_{model_name}_adapters_{models_params.TRAIN_LAYER}_{nbits}_{r}_{lora_alpha}_{lora_dropout}_{gradient_accumulation_steps}_{learning_rate}_{appendix}{extra_str_cl}"
                            else:
                                ADAPTERS_CHECKPOINT = f"ferrazzipietro/LS_{model_name}_adapters_{models_params.TRAIN_LAYER}_{nbits}_{r}_{lora_alpha}_{lora_dropout}_{gradient_accumulation_steps}_{learning_rate}_{appendix}{extra_str_cl}"
                            adapters_list.append(ADAPTERS_CHECKPOINT)
    return list(set(adapters_list))